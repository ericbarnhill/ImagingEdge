{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 4: Mine Unstructured Sources\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 4: In (this notebook) of the app, the graphs \"learns\" by adding unstructured sources.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 1: Scrape PubMed\n",
    "\n",
    "Part 2: Convert PubMed abstracts to Bag of Words\n",
    "\n",
    "Part 3: Build graph connecting search terms and trends\n",
    "\n",
    "Part 5: Validation test suite\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### Part 1: Scraping Arxiv\n",
    "### Part 2: Scraping Twitter\n",
    "### Part 3: Scraping Custom URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up...\n",
      "Python kernel:\n",
      "/home/ericbarnhill/anaconda3/envs/ecb/bin/python\n",
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/imedge/imedge_3_graph.log\n"
     ]
    }
   ],
   "source": [
    "#SETUP\n",
    "%run imedge_3_graph.ipynb\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import twitterscraper\n",
    "import lxml\n",
    "import logging\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arxiv(start_date, n_windows, n_months, search_term):\n",
    "    \"\"\"Pull abstracts within rolling date windows from the ArXiv\n",
    "        \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        node: radiological search term used to probe the ArXiv\n",
    "        \n",
    "    Returns:\n",
    "        List of arxiv abstract sets, one per rolling window\n",
    "    \"\"\"\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        start_string = start.strftime(\"%Y-%m-%d\")\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        end_string = end.strftime(\"%Y-%m-%d\")\n",
    "        #logging.info(\"Query from \" + str(start) + \" to \" + str(end))\n",
    "        # ArXiv API does not appear to handle combined abstract & date searches.\n",
    "        # Consequently ArXiv is scraped through the 'front door'\n",
    "        # Note that this has a hard limit of 200 per rolling window, however\n",
    "        # this seems to be sufficient in preliminary testing\n",
    "        arxiv_request = 'https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term='+ \\\n",
    "        search_term+ \\\n",
    "        '&terms-0-field=abstract&classification-physics_archives=all&' + \\\n",
    "        'date-year=&date-filter_by=date_range&' \\\n",
    "        'date-from_date=' + start_string + '&date-to_date=' + \\\n",
    "        end_string + '&size=200'\n",
    "        with request.urlopen(arxiv_request) as response:\n",
    "            try:\n",
    "                page = response.read()\n",
    "            except (http.client.IncompleteRead) as e:\n",
    "                page = e.partial\n",
    "        soup = BeautifulSoup(page, 'html.parser').get_text()\n",
    "        soup_splits = soup.split('More')\n",
    "        soup_abstracts = []\n",
    "        for n in range(1, len(soup_splits)): # skip first one, it is preparatory text\n",
    "            soup_split = soup_splits[n]\n",
    "            soup_abstracts.append(soup_split.split('Less')[0])\n",
    "        #(\"Acquired\", len(soup_abstracts), \"abstracts across time window\", n)\n",
    "        window_records.append(soup_abstracts)\n",
    "    return window_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_twitter(start_date, n_windows, n_months, search_term, trending_term):\n",
    "    \"\"\"Scrape Twitter within rolling date windows\n",
    "    \n",
    "    To handle Twitter, the methodology changes a bit. I search the dates\n",
    "    for tweets containing search AND trending terms together.\n",
    "        \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        node: radiological search term used to probe the ArXiv\n",
    "        \n",
    "    Returns:\n",
    "        List of twitter mentions, one per rolling window\n",
    "    \"\"\"\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        query = search_term + ' AND ' + trending_term\n",
    "        logger = logging.getLogger('twitterscraper')\n",
    "        logger.disabled = True\n",
    "        twitter_response = twitterscraper.query_tweets(query, limit=200, \n",
    "                            begindate=start, \n",
    "                            enddate=end, poolsize=20, lang='')\n",
    "        logger.disabled = False\n",
    "        window_records.append(len(twitter_response))\n",
    "    return window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_and_trends(path):\n",
    "    with open(os.path.join(path, 'G.pickle'), 'rb') as fp:\n",
    "        G = pickle.load(fp)\n",
    "    with open(os.path.join(path, 'trends_converted.pickle'), 'rb') as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    return G, trends\n",
    "\n",
    "def save_graph_and_trends(path, G, trends):\n",
    "    with open(os.path.join(path, 'G_x.pickle'), 'wb') as fp:\n",
    "        pickle.dump(G, fp)\n",
    "    with open(os.path.join(path, 'trends_converted_x.pickle'), 'wb') as fp:\n",
    "        pickle.dump(trends, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socks\n",
    "import socket\n",
    "import stem.process\n",
    "\n",
    "def develop_graph(year, arxiv = True, twitter = True):\n",
    "    SOCKS_PORT=9050# You can change the port number\n",
    "    tor_process = stem.process.launch_tor_with_config(\n",
    "        config = {\n",
    "            'SocksPort': str(SOCKS_PORT),\n",
    "        },\n",
    "    )\n",
    "    socks.setdefaultproxy(proxy_type=socks.PROXY_TYPE_SOCKS5,\n",
    "                          addr=\"127.0.0.1\", #theres a ',' change it to '.' -- linkedin was being glitchy\n",
    "                          port=SOCKS_PORT)\n",
    "    socket.socket = socks.socksocket\n",
    "    N_WINDOWS = 12\n",
    "    N_MONTHS = 3\n",
    "    START_DATE = datetime.date(year,1,1)\n",
    "    path = os.path.join(IMEDGE_PATH, str(year))\n",
    "    reset_logging()\n",
    "    logfile = os.path.join(IMEDGE_PATH, 'imedge_4_learn.log')\n",
    "    print(\"Logfile path: \", logfile)\n",
    "    logging.basicConfig(filename=logfile,level=logging.DEBUG)\n",
    "    G, trends = load_graph_and_trends(path)\n",
    "    ## loop through nodes\n",
    "    for node,data in G.nodes(data=True):\n",
    "        if data['is_key']:\n",
    "            logging.info(\"Scraping for search term \" + node)\n",
    "            node_neighbors = list(G.neighbors(node))\n",
    "            # scrape the ArXiv\n",
    "            start=time.time()\n",
    "            arxiv_abstracts = scrape_arxiv(START_DATE, N_WINDOWS, N_MONTHS, node)\n",
    "            for i, abstract_set in enumerate(arxiv_abstracts):\n",
    "                for abstract in abstract_set:\n",
    "                    for neighbor in node_neighbors:\n",
    "                        if neighbor in abstract:\n",
    "                            # add a weight to the edge of the graph\n",
    "                            G[node][neighbor]['weight'] = G[node][neighbor]['weight'] + 1\n",
    "                            # add a mention in the trends data\n",
    "                            logging.info(\"Adding one to\" + neighbor + \"new mentions total\" + str(trends[neighbor][i] + 1))\n",
    "                            trends[neighbor][i] = trends[neighbor][i] + 1\n",
    "                    if twitter:\n",
    "                        # here we don't need to create counts, they are given by the API\n",
    "                        logging.debug(\"Scraping twitter for \" + node + \" + \" + neighbor)\n",
    "                        twitter_mentions = scrape_twitter(START_DATE, N_WINDOWS, N_MONTHS, \n",
    "                                                          node, neighbor)\n",
    "                        for i, tweet_count in enumerate(twitter_mentions):\n",
    "                            G[node][neighbor]['weight'] = G[node][neighbor]['weight'] + 1\n",
    "                            trends[neighbor][i] = trends[neighbor][i] + 1\n",
    "    save_graph_and_trends(path, G, trends)\n",
    "    tor_process.kill()\n",
    "    return G, trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/imedge/imedge_4_learn.log\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'http' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-388b230a5c63>\u001b[0m in \u001b[0;36mscrape_arxiv\u001b[0;34m(start_date, n_windows, n_months, search_term)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncompleteRead\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reponse' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-3e09c9a8ce3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mYEAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2017\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevelop_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYEAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-5044de791e46>\u001b[0m in \u001b[0;36mdevelop_graph\u001b[0;34m(year, arxiv, twitter)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# scrape the ArXiv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0marxiv_abstracts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_arxiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTART_DATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_WINDOWS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_MONTHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marxiv_abstracts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabstract_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-388b230a5c63>\u001b[0m in \u001b[0;36mscrape_arxiv\u001b[0;34m(start_date, n_windows, n_months, search_term)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncompleteRead\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'http' is not defined"
     ]
    }
   ],
   "source": [
    "YEAR = 2017\n",
    "G, trends = develop_graph(YEAR, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = os.path.join(IMEDGE_PATH, str(2017))\n",
    "save_graph_and_trends(path, G, trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
