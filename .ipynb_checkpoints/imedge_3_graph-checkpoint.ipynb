{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 3: Convert Trends To Graph\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 3: (this notebook) of the app creates a graph combining search terms and trending terms, and deploys this graph to the web app.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 1: Scrape PubMed\n",
    "\n",
    "Part 2: Convert PubMed abstracts to Bag of Words\n",
    "\n",
    "Part 4: Graph \"learns\" from unstructured sources\n",
    "\n",
    "Part 5: Validation test suite\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Up...\n",
      "Python kernel:\n",
      "/home/ericbarnhill/anaconda3/envs/ecb/bin/python\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "%run imedge_2_trends.ipynb\n",
    "import networkx as nx\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_part_2_data(year):\n",
    "    \"\"\"Recover BOW and trends data.\n",
    "    \"\"\" \n",
    "    DATA_PATH = os.path.join(IMEDGE_PATH, str(year))\n",
    "    with open(os.path.join(DATA_PATH, 'trends.pickle'), 'rb') as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(os.path.join(DATA_PATH, 'records.pickle'), 'rb') as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(os.path.join(DATA_PATH, 'df.pickle'), 'rb') as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_search_terms(G, cent_meas = 'eigenvector_centrality', N=20):\n",
    "    # top nodes in metric of eigencentrality\n",
    "    central_nodes = centrality_measures(G, cent_meas)\n",
    "    hot_search_terms = []\n",
    "    for item in central_nodes:\n",
    "        if G.node[item]['is_key'] == True:\n",
    "            # expected outcome\n",
    "            hot_search_terms.append(item)\n",
    "        else:\n",
    "            # unexpected outcome - log\n",
    "            logging.info(\"Top node is trending term: \"+item)\n",
    "        if len(hot_search_terms) >= N:\n",
    "            break\n",
    "    return hot_search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    \"\"\"Filter list BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = get_top_keywords(n_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            logging.debug(\"matching trend\" + str(trend.trend) + \"and keyword\" + str(keyword))\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    \"\"\" for debugging. \"\"\"\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        #node1 = convert_trend(trend.trend)\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top trending terms for a search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, search_term, n=10):\n",
    "    node_edges = G.edges(search_term.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trend(trend):\n",
    "    if len(trend) > 1:\n",
    "         trend = ' '.join(trend)\n",
    "    return trend\n",
    "\n",
    "def convert_trends(trends):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = convert_trend(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centrality_measures(G, cent_meas):\n",
    "    if cent_meas == 'degree_centrality':\n",
    "        dc = nx.degree_centrality(G)\n",
    "        dc = sorted(dc, key=dc.get, reverse=True)\n",
    "        return dc\n",
    "    elif cent_meas == 'betweenness_centrality':\n",
    "        bc = nx.betweenness_centrality(G)\n",
    "        bc = sorted(bc, key=bc.get, reverse=True)\n",
    "        return bc\n",
    "    elif cent_meas == 'eigenvector_centrality':\n",
    "        ec = nx.eigenvector_centrality_numpy(G)\n",
    "        ec = sorted(ec, key=ec.get, reverse=True)\n",
    "        return ec\n",
    "    else:\n",
    "        print(\"ImagingEdge ERROR: centrality measure not recognized.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_df(df):\n",
    "    \"\"\"Filters and de-dupes trending terms\n",
    "        \n",
    "    Args:\n",
    "        unfiltered data frame\n",
    "        \n",
    "    Returns:\n",
    "        filtered data frame\n",
    "    \"\"\" \n",
    "    df_filt = df.copy(deep=True)\n",
    "    df_filt = df_filt[df_filt.score > 0]\n",
    "    for i in range(df.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            logging.debug(\"Term \" + str(i))\n",
    "        single_let = False\n",
    "        term = df.iloc[i,0]\n",
    "        for element in term:\n",
    "            # DROP TRENDS WITH SINGLE LETTER TERMS\n",
    "            if len(element) == 1:\n",
    "                logging.debug(\"dropping \" + str(term) + \" as it contains a single letter term\")\n",
    "                df_filt.drop(df_filt[df_filt['key'] == term].index, inplace=True)\n",
    "                single_let = True\n",
    "        if not single_let:\n",
    "            # DEDUPE\n",
    "            term_set = set(term)    \n",
    "            for j in range(df.shape[0]):\n",
    "                entry = df.iloc[j,0]\n",
    "                entry_set = set(entry)\n",
    "                if i != j:\n",
    "                    if entry_set.issubset(term_set):\n",
    "                        df_filt.drop(df_filt[df_filt['key'] == entry].index, inplace=True)            \n",
    "    logging.info(\"df length: \" + str(df.shape))\n",
    "    logging.info(\"filtered df length: \" + str(df_filt.shape))\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_scatter_plots():\n",
    "    web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "    dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "    dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "    ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "    ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "    bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "    bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))\n",
    "    import altair as alt\n",
    "    chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "        x = 'listnum:O',\n",
    "        y = 'value:O', \n",
    "        color = 'variable'\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_part_3_data(G, trends_converted, hot_search_terms, path):\n",
    "    with open(os.path.join(path, 'G.pickle'), 'wb') as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(os.path.join(path, 'trends_converted.pickle'), 'wb') as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    with open(os.path.join(path, 'hot_search_terms.pickle'), 'wb') as st_path:\n",
    "        pickle.dump(hot_search_terms, st_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_app(G, trends_converted, hot_search_terms):\n",
    "    GRAPH_PATH = os.path.join(APP_PATH, '/G.pickle')\n",
    "    TRENDS_PATH = os.path.join(APP_PATH, '/trends_converted.pickle')\n",
    "    SEARCH_TERMS_PATH = os.path.join(APP_PATH, '/hot_search_terms.pickle')\n",
    "    with open(GRAPH_PATH, 'wb') as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(TRENDS_PATH, 'wb') as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    with open(SEARCH_TERMS_PATH, 'wb') as st_path:\n",
    "        pickle.dump(hot_search_terms, st_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(year=2017, export=False): \n",
    "    \"\"\"Part 3 of ImagingEdge: Build Labeled Graph.\n",
    " \n",
    "    Args:\n",
    "        year: Year being evaluated for trends. Default is 2017, but can be used for historic periods for validation. \n",
    "        (TODO: allow custom date ranges)\n",
    "        export: export results to app. Method can be called here, or in part 4 after the graph has learned from \n",
    "        unstructured sources.\n",
    "    \"\"\"\n",
    "    # limit number of searched keywords, for computational reasons\n",
    "    N_KEYWORDS = 10000\n",
    "    # choose number of search terms displayed in app\n",
    "    N_APP_SEARCH_TERMS = 20\n",
    "    # Trends close to zero are noise. Set threshold for positive trend scores:\n",
    "    TREND_THRESH = 0.01\n",
    "    GRAPH_PATH = os.path.join(IMEDGE_PATH, str(year))\n",
    "    reset_logging()\n",
    "    logfile = os.path.join(IMEDGE_PATH, 'imedge_3_graph.log')\n",
    "    print(\"Logfile path: \", logfile)\n",
    "    logging.basicConfig(filename=logfile,level=logging.INFO)\n",
    "    trends, records, df = unpickle_part_2_data(year)\n",
    "    df_filt = filt_df(df)\n",
    "    num_above_zero = sum(df_filt.score > TREND_THRESH)\n",
    "    logging.info(\"number of positive trends:\" + str(num_above_zero))\n",
    "    trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                       num_above_zero, N_KEYWORDS, from_sql = True)\n",
    "    G = populate_graph(trends_list)\n",
    "    trends_converted = convert_trends(trends)\n",
    "    hot_search_terms = get_hot_search_terms(G, N=N_APP_SEARCH_TERMS)\n",
    "    pickle_part_3_data(G, trends_converted, hot_search_terms, GRAPH_PATH)\n",
    "    if export:\n",
    "        export_to_app(G, trends_converted, hot_search_terms)\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/imedge/imedge_3_graph.log\n"
     ]
    }
   ],
   "source": [
    "#df_filt = build_graph(year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>score</th>\n",
       "      <th>total_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>(provide, information)</td>\n",
       "      <td>[0.7688005207229552]</td>\n",
       "      <td>441.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>(near, infrared)</td>\n",
       "      <td>[0.7015220347594769]</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>(cell, line)</td>\n",
       "      <td>[0.4902761235578917]</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>(brain, networks)</td>\n",
       "      <td>[0.4058959668338355]</td>\n",
       "      <td>577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2282</th>\n",
       "      <td>(imaging, techniques)</td>\n",
       "      <td>[0.37814831534738]</td>\n",
       "      <td>1306.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         key                 score  total_mentions\n",
       "214   (provide, information)  [0.7688005207229552]           441.0\n",
       "2047        (near, infrared)  [0.7015220347594769]           259.0\n",
       "1708            (cell, line)  [0.4902761235578917]           451.0\n",
       "1946       (brain, networks)  [0.4058959668338355]           577.0\n",
       "2282   (imaging, techniques)    [0.37814831534738]          1306.0"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
