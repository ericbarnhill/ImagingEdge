{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTR Connectivity Graph\n",
    "\n",
    "- Index pruned trends to pruned keywords\n",
    "- Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "- Make Network With Pruned Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "os.chdir(\"/home/ericbarnhill/Documents/code/insight/rtr/\")\n",
    "#%run rtr.ipynb\n",
    "import operator\n",
    "import pickle\n",
    "PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/12_mo_nodedupe/\"\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_data():\n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"df.pickle\", \"rb\") as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = get_top_keywords(n_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    #print(top_keywords_text)\n",
    "    # TRIPLE LOOP - sure to be a bottleneck\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            # direct method only --\n",
    "            # update to include sql compatibility\n",
    "            # when sql is working\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            #print(\"matching trend\", trend.trend, \"and keyword\", keyword)\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ten most heavily weighted edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, keyword, n=10):\n",
    "    node_edges = G.edges(keyword.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find which keywords made the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_keywords(G, N=20):\n",
    "    keywords =  {}\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['is_key']:\n",
    "            total_wt = 0\n",
    "            for edge in G.edges(node[0], data=True):\n",
    "                total_wt += edge[2]['weight']\n",
    "            keywords.update({node[0]: total_wt})\n",
    "    n = 0\n",
    "    hot_keywords = []\n",
    "    for item in sorted(keywords.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            break\n",
    "        else:\n",
    "            hot_keywords.append(item)\n",
    "    return hot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trends(trends):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = ' '.join(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return trends_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle graph\n",
    "def pickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"wb\") as fp:\n",
    "            pickle.dump(G, fp)\n",
    "    except:\n",
    "        print('error')\n",
    "        \n",
    "def unpickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"rb\") as fp:\n",
    "            G = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return G\n",
    "        \n",
    "def unpickle_bow_trends():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/bow_converted.txt\", \"rb\") as fp:\n",
    "            bow_trends = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return bow_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def centrality_measures(G):\n",
    "    dc = nx.degree_centrality(G)\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    ec = nx.eigenvector_centrality_numpy(G)\n",
    "    dc = sorted(dc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bc = sorted(bc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ec = sorted(ec.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dc, bc, ec\n",
    "#    for key, value in sorted(bc.items(), key=operator.itemgetter(1), reverse=True):\n",
    "#        print(key, value)\n",
    "#        n += 1\n",
    "#        if n > 10:\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_app(G, trends_converted):\n",
    "    GRAPH_PATH = \"/home/ericbarnhill/Documents/code/insight_app/G.pickle\"\n",
    "    TRENDS_PATH = \"/home/ericbarnhill/Documents/code/insight_app/trends_converted.pickle\"\n",
    "    with open(GRAPH_PATH, \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(TRENDS_PATH, \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(set_up=False):\n",
    "    if set_up:\n",
    "        setup()\n",
    "    N_KEYWORDS = 50000\n",
    "    L = 5\n",
    "    trends, records, df = unpickle_data()\n",
    "    trends_list = pair_trends_keywords(df, records,\n",
    "                                       len(trends), N_KEYWORDS, from_sql = True)\n",
    "    print(\"Top 20 trends:\")\n",
    "    print_trend_dict(trends_list, 20)\n",
    "    G = populate_graph(trends_list)\n",
    "    print(\"MRI trends:\")\n",
    "    mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "    print(mri_trends)\n",
    "    print(\"Hottest keywords:\")\n",
    "    hot_keywords = get_hot_keywords(G)\n",
    "    print(hot_keywords)\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    print(\"Top degree centrality:\", list(dc)[:L])\n",
    "    print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "    print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "    trends_converted = convert_trends(trends)\n",
    "    export_to_app(G, trends_converted)\n",
    "    return G, trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts', 'rtr_keywords']\n",
      "Top 20 trends:\n",
      "{'cardiac computed tomography': Counter({'tomography': 36, 'image interpretation': 6, 'phantoms': 6})}\n",
      "{'regression analysis': Counter({'tomography': 56, 'image interpretation': 28, 'imaging': 27, 'phantoms': 21, 'ventricular function': 18, 'models': 12, 'insemination': 9, 'heart septal defects': 9, 'chorionic gonadotropin': 9, 'infertility': 9, 'stress': 6, 'heart failure': 6, 'phobia': 6, 'ultrasonography': 5, 'endothelium': 4, 'depressive disorder': 3, 'antineoplastic agents': 3, 'spectroscopy': 1})}\n",
      "{'medline embase': Counter()}\n",
      "{'postoperative day': Counter({'tomography': 32, 'imaging': 12, 'anti-infective agents': 9, 'lens': 9, 'anesthesia': 6, 'fractures': 6, 'anastomosis': 6, 'thoracic surgery': 3, 'liver failure': 3, 'ultrasonography': 2})}\n",
      "{'medical records': Counter({'tomography': 32, 'image interpretation': 9, 'chemotherapy': 9, 'ultrasonography': 6, 'animals': 6, 'injections': 6, 'epilepsy': 6, 'heart septal defects': 3, 'lymphoma': 3, 'heart failure': 2, 'anesthesia': 1})}\n",
      "{'tomography scans': Counter({'tomography': 94, 'phantoms': 21, 'imaging': 12, 'implants': 9, 'adenocarcinoma': 6, 'foramen ovale': 6, 'ear': 6, 'lens': 3, 'fractures': 3, 'image interpretation': 3})}\n",
      "{'pathological findings': Counter({'tomography': 43, 'infertility': 9, 'chemotherapy': 9, 'image interpretation': 6, 'animals': 6, 'microscopy': 4, 'intestine': 3, 'neoplasm recurrence': 2})}\n",
      "{'literature search': Counter({'tomography': 37, 'neoplasm recurrence': 9, 'image interpretation': 6, 'foramen ovale': 6, 'microscopy': 4, 'depression': 3, 'fractures': 3, 'chorionic gonadotropin': 2})}\n",
      "{'receiver operating': Counter({'image interpretation': 143, 'tomography': 94, 'ultrasonography': 15, 'phantoms': 12, 'imaging': 9, 'implants': 9, 'foramen ovale': 6, 'cystadenoma': 3, 'endoscopy': 3, 'ventricular function': 3, 'infertility': 2, 'chorionic gonadotropin': 2})}\n",
      "{'imaging method': Counter({'phantoms': 89, 'tomography': 68, 'image interpretation': 64, 'fractures': 15, 'imaging': 12, 'microscopy': 9, 'simultaneous multislice': 9, 'neoplasm recurrence': 9, 'implants': 6, 'infertility': 6, 'pattern recognition': 6, 'embryo': 3, 'heart failure': 3, 'radiotherapy': 3, 'mice': 3, 'in situ hybridization': 2})}\n",
      "{'high sensitivity': Counter({'tomography': 64, 'image interpretation': 49, 'phantoms': 34, 'microscopy': 30, 'infertility': 9, 'animals': 9, 'chromatography': 9, 'antigens': 6, 'nanotubes': 6, 'fluorescent antibody technique': 4, 'image processing': 3, 'plaque': 2, 'ventricular function': 1})}\n",
      "{'consecutive series': Counter({'tomography': 9})}\n",
      "{'events occurred': Counter()}\n",
      "{'cardiac computed': Counter({'tomography': 36, 'image interpretation': 6, 'phantoms': 6, 'imaging': 3})}\n",
      "{'high level': Counter({'tomography': 33, 'imaging': 21, 'microscopy': 19, 'mice': 15, 'animals': 12, 'phantoms': 12, 'immunity': 9, 'macrophages': 9, 'image interpretation': 8, 'antineoplastic agents': 6, 'receptor': 6, 'faculty': 6, 'fluorescent antibody technique': 6, 'rna': 6, '2': 4, '3': 4, 'carcinoma': 3, 'muscle': 3, 'africa': 3, 'image processing': 3, 'lens': 3, 'statistics': 3, 'stress': 3, 'models': 2, 'embryo': 2})}\n",
      "{'review current literature': Counter()}\n",
      "{'right heart': Counter({'tomography': 43, 'stress': 9, 'heart septal defects': 9, 'heart failure': 3, 'image interpretation': 3, 'ventricular function': 2, 'chemotherapy': 1})}\n",
      "{'ventricular rv': Counter()}\n",
      "{'diffusion tensor': Counter({'image interpretation': 97, 'animals': 27, 'stress': 9, 'dementia': 6, 'injections': 6, 'tomography': 6, 'encephalitis': 6, 'depressive disorder': 5, 'phantoms': 5, 'bipolar disorder': 3, 'imaging': 3, 'mice': 2, 'audiometry': 2})}\n",
      "{'systolic diastolic blood': Counter()}\n",
      "{'john sons': Counter()}\n",
      "MRI trends:\n",
      "['tomography 763', 'image interpretation 429', 'phantoms 239', 'imaging 145', 'ventricular function 45', 'depressive disorder 30', 'immunoglobulins 28', 'magnetic resonance 24', 'animals 23', 'image processing 23']\n",
      "Hottest keywords:\n",
      "[('tomography', 84021), ('image interpretation', 45475), ('phantoms', 35559), ('imaging', 20496), ('microscopy', 20158), ('animals', 10853), ('ventricular function', 9155), ('stress', 5556), ('antineoplastic agents', 5415), ('mice', 5168), ('lens', 3624), ('ultrasonography', 3460), ('calcification', 3180), ('fractures', 3055), ('heart septal defects', 3004), ('image processing', 2796), ('magnetic resonance imaging', 2618), ('embryo', 2580), ('foramen ovale', 2266), ('models', 2120)]\n",
      "Top degree centrality: [('tomography', 0.6015824710894705), ('image interpretation', 0.3616555082166768), ('imaging', 0.26975045648204504), ('phantoms', 0.2533171028606208), ('microscopy', 0.20462568472306755)]\n",
      "Top betweenness centrality: [('tomography', 0.5123600513084944), ('image interpretation', 0.2248210374672075), ('imaging', 0.1016510890963839), ('phantoms', 0.08156439744083382), ('microscopy', 0.07638437377508617)]\n",
      "Top eigencentrality: [('tomography', 0.4479166024389444), ('image interpretation', 0.3145465723222861), ('imaging', 0.2308254326784862), ('phantoms', 0.22224840345351862), ('microscopy', 0.15847836961773834)]\n"
     ]
    }
   ],
   "source": [
    "G, trends_converted = run_nb(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc, bc, ec = centrality_measures(G)\n",
    "L = 10\n",
    "web_list = {'imaging':1, 'tomography':2, 'rats':3, 'ultrasonography':4, \\\n",
    "            'carcinoma':5, 'mice':6, 'echocardiography':7, 'diagnosis':8, \\\n",
    "           'microscopy':9, 'cartilage':10}\n",
    "dc_list = {}\n",
    "bc_list = {}\n",
    "ec_list = {}\n",
    "for n in range(L):\n",
    "    dc_list.update({list(dc)[n][0]:n+1})\n",
    "    bc_list.update({list(bc)[n][0]:n+1})\n",
    "    ec_list.update({list(ec)[n][0]:n+1})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_scatter_plots():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "    dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "    dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "    ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "    ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "    bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "    bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))\n",
    "    import altair as alt\n",
    "    chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "        x = 'listnum:O',\n",
    "        y = 'value:O', \n",
    "        color = 'variable'\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends, records, df = unpickle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends[list(trends)[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ericbarnhill/Documents/code/insight/rtr'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = df.iloc[0,0]\n",
    "sum(df['key']==t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt = df.copy(deep=True)\n",
    "is_subset = np.zeros(df.shape[0])\n",
    "for i in range(df.shape[0]):\n",
    "    single_let = False\n",
    "    term = df.iloc[i,0]\n",
    "    for element in term:\n",
    "        if len(element) == 1:\n",
    "            print(\"dropping \",term,\" as it contains a single letter term\")\n",
    "            df_filt.drop(df_filt[df_filt['key'] == term].index, inplace=True)\n",
    "            single_let = True\n",
    "    if not single_let:\n",
    "        term_set = set(term)    \n",
    "        for j in range(df.shape[0]):\n",
    "            entry = df.iloc[j,0]\n",
    "            entry_set = set(entry)\n",
    "            if i != j:\n",
    "                if entry_set.issubset(term_set):\n",
    "                    df_filt.drop(df_filt[df_filt['key'] == entry].index, inplace=True)\n",
    "            \n",
    "print(\"df length\", df.shape)\n",
    "print(\"df filt length\", df_filt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping\n",
      "Subset sum:  1.0\n",
      "df length (11727, 3)\n",
      "df filt length (11726, 3)\n"
     ]
    }
   ],
   "source": [
    "df_filt = df.copy(deep=True)\n",
    "is_subset = np.zeros(df.shape[0])\n",
    "t1 = 'cardiac'\n",
    "t2 = {t1}\n",
    "for n in range(10):\n",
    "    df_filt = df_filt[t for t in ]\n",
    "print(\"Subset sum: \", np.sum(is_subset))\n",
    "print(\"df length\", df.shape)\n",
    "print(\"df filt length\", df_filt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set({'cardiac', 'troponin'})\n",
    "b = set({'cardiac'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>score</th>\n",
       "      <th>total_mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8805</th>\n",
       "      <td>(cardiac, computed, tomography)</td>\n",
       "      <td>[1.933281138305476]</td>\n",
       "      <td>584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6379</th>\n",
       "      <td>(regression, analysis)</td>\n",
       "      <td>[1.7439089503648177]</td>\n",
       "      <td>10470.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3689</th>\n",
       "      <td>(medline, embase)</td>\n",
       "      <td>[1.6294859359844853]</td>\n",
       "      <td>720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>(postoperative, day)</td>\n",
       "      <td>[1.3618046482029307]</td>\n",
       "      <td>785.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>(medical, records)</td>\n",
       "      <td>[1.3263085272642037]</td>\n",
       "      <td>3728.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2733</th>\n",
       "      <td>(tomography, scans)</td>\n",
       "      <td>[1.3050110594157611]</td>\n",
       "      <td>3480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11533</th>\n",
       "      <td>(pathological, findings)</td>\n",
       "      <td>[1.264084417524167]</td>\n",
       "      <td>821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8683</th>\n",
       "      <td>(literature, search)</td>\n",
       "      <td>[1.2418180923726951]</td>\n",
       "      <td>1037.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>(receiver, operating)</td>\n",
       "      <td>[1.229333882901239]</td>\n",
       "      <td>8134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11256</th>\n",
       "      <td>(imaging, method)</td>\n",
       "      <td>[1.184765666440886]</td>\n",
       "      <td>1360.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   key                 score  total_mentions\n",
       "8805   (cardiac, computed, tomography)   [1.933281138305476]           584.0\n",
       "6379            (regression, analysis)  [1.7439089503648177]         10470.0\n",
       "3689                 (medline, embase)  [1.6294859359844853]           720.0\n",
       "3274              (postoperative, day)  [1.3618046482029307]           785.0\n",
       "3128                (medical, records)  [1.3263085272642037]          3728.0\n",
       "2733               (tomography, scans)  [1.3050110594157611]          3480.0\n",
       "11533         (pathological, findings)   [1.264084417524167]           821.0\n",
       "8683              (literature, search)  [1.2418180923726951]          1037.0\n",
       "5008             (receiver, operating)   [1.229333882901239]          8134.0\n",
       "11256                (imaging, method)   [1.184765666440886]          1360.0"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ecb/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3063\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-a8200046d120>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ecb/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2683\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2685\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecb/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2690\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2692\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecb/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2484\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecb/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecb/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3063\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 6"
     ]
    }
   ],
   "source": [
    "df[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
