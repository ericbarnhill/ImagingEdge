{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 3: Convert Trends To Graph\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 3: (this notebook) of the app creates a graph combining search terms and trending terms, and deploys this graph to the web app.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 1: Scrape PubMed\n",
    "\n",
    "Part 2: Convert PubMed abstracts to Bag of Words\n",
    "\n",
    "Part 4: Graph \"learns\" from unstructured sources\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel:\n",
      "/home/ericbarnhill/anaconda3/envs/ecb/bin/python\n",
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/rtr/scrape.log\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "import os\n",
    "ROOT_PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/\" \n",
    "os.chdir(ROOT_PATH)\n",
    "%run rtr.ipynb\n",
    "import operator\n",
    "import pickle\n",
    "YEAR = 2016\n",
    "DATA_PATH = ROOT_PATH + str(YEAR) + \"/\"\n",
    "os.chdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_data():\n",
    "    \"\"\"Recover BOW and trends data.\n",
    "    \"\"\" \n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"df.pickle\", \"rb\") as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    \"\"\"Filter list BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = get_top_keywords(n_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    #print(top_keywords_text)\n",
    "    # TRIPLE LOOP - sure to be a bottleneck\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            # direct method only --\n",
    "            # update to include sql compatibility\n",
    "            # when sql is working\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            #print(\"matching trend\", trend.trend, \"and keyword\", keyword)\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        #node1 = convert_trend(trend.trend)\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ten most heavily weighted edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, keyword, n=10):\n",
    "    node_edges = G.edges(keyword.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find which keywords made the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_keywords(G, N=20):\n",
    "    keywords =  {}\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['is_key']:\n",
    "            total_wt = 0\n",
    "            for edge in G.edges(node[0], data=True):\n",
    "                total_wt += edge[2]['weight']\n",
    "            keywords.update({node[0]: total_wt})\n",
    "    n = 0\n",
    "    hot_keywords = []\n",
    "    for item in sorted(keywords.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            break\n",
    "        else:\n",
    "            hot_keywords.append(item)\n",
    "    return hot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trend(trend):\n",
    "    if len(trend) > 1:\n",
    "         trend = ' '.join(trend)\n",
    "    return trend\n",
    "\n",
    "def convert_trends(trends):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = convert_trend(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def centrality_measures(G):\n",
    "    dc = nx.degree_centrality(G)\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    ec = nx.eigenvector_centrality_numpy(G)\n",
    "    dc = sorted(dc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bc = sorted(bc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ec = sorted(ec.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dc, bc, ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_df(df):\n",
    "    df_filt = df.copy(deep=True)\n",
    "    df_filt = df_filt[df_filt.score > 0]\n",
    "    for i in range(df.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Term \",i)\n",
    "        single_let = False\n",
    "        term = df.iloc[i,0]\n",
    "        for element in term:\n",
    "            if len(element) == 1:\n",
    "                print(\"dropping \",term,\" as it contains a single letter term\")\n",
    "                df_filt.drop(df_filt[df_filt['key'] == term].index, inplace=True)\n",
    "                single_let = True\n",
    "        if not single_let:\n",
    "            term_set = set(term)    \n",
    "            for j in range(df.shape[0]):\n",
    "                entry = df.iloc[j,0]\n",
    "                entry_set = set(entry)\n",
    "                if i != j:\n",
    "                    if entry_set.issubset(term_set):\n",
    "                        df_filt.drop(df_filt[df_filt['key'] == entry].index, inplace=True)            \n",
    "    print(\"df length\", df.shape)\n",
    "    print(\"df filt length\", df_filt.shape)\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_app(G, trends_converted):\n",
    "    GRAPH_PATH = \"/home/ericbarnhill/Documents/code/insight_app/G.pickle\"\n",
    "    TRENDS_PATH = \"/home/ericbarnhill/Documents/code/insight_app/trends_converted.pickle\"\n",
    "    with open(GRAPH_PATH, \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(TRENDS_PATH, \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_presentation_graphic(): \n",
    "    # todo: clean up\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    L = 10\n",
    "    web_list = {'imaging':1, 'tomography':2, 'rats':3, 'ultrasonography':4, \\\n",
    "                'carcinoma':5, 'mice':6, 'echocardiography':7, 'diagnosis':8, \\\n",
    "               'microscopy':9, 'cartilage':10}\n",
    "    dc_list = {}\n",
    "    bc_list = {}\n",
    "    ec_list = {}\n",
    "    for n in range(L):\n",
    "        dc_list.update({list(dc)[n][0]:n+1})\n",
    "        bc_list.update({list(bc)[n][0]:n+1})\n",
    "        ec_list.update({list(ec)[n][0]:n+1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_scatter_plots():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "    dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "    dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "    ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "    ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "    bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "    bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))\n",
    "    import altair as alt\n",
    "    chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "        x = 'listnum:O',\n",
    "        y = 'value:O', \n",
    "        color = 'variable'\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(): \n",
    "    trends, records, df = unpickle_data()\n",
    "    df_filt = filt_df(df)\n",
    "    N_KEYWORDS = 10000\n",
    "    L = 10\n",
    "    num_above_zero = sum(df_filt.score > 0.1)\n",
    "    print(\"number of positive trends:\", num_above_zero)\n",
    "    trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                       round(num_above_zero*3/4), N_KEYWORDS, from_sql = True)\n",
    "    print(\"Top 20 trends:\")\n",
    "    print_trend_dict(trends_list, 20)\n",
    "    G = populate_graph(trends_list)\n",
    "    print(\"MRI trends:\")\n",
    "    mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "    print(mri_trends)\n",
    "    print(\"Hottest keywords:\")\n",
    "    hot_keywords = get_hot_keywords(G)\n",
    "    print(hot_keywords)\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    print(\"Top degree centrality:\", list(dc)[:L])\n",
    "    print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "    print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "    trends_converted = convert_trends(trends)\n",
    "    with open('G.pickle', \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open('trends_converted.pickle', \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    export_to_app(G, trends_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term  0\n",
      "Term  1000\n",
      "Term  2000\n",
      "Term  3000\n",
      "Term  4000\n",
      "Term  5000\n",
      "Term  6000\n",
      "Term  7000\n",
      "Term  8000\n",
      "Term  9000\n",
      "df length (9611, 3)\n",
      "df filt length (2829, 3)\n",
      "number of positive trends: 606\n",
      "Top 20 trends:\n",
      "{'presenting symptoms': Counter({'tomography': 16, 'child': 9, 'biopsy': 6, 'ultrasonography': 3, 'endocarditis': 2, 'cardiomyopathy': 1, 'embolization': 1})}\n",
      "{'difference observed': Counter({'tomography': 6, 'dental implantation': 3, 'catheters': 2})}\n",
      "{'episodic memory': Counter({'memory': 22, 'image processing': 11, 'polymorphism': 4})}\n",
      "{'internal carotid artery ica': Counter()}\n",
      "{'based functional': Counter({'image processing': 6, 'ventricular dysfunction': 6, 'spectroscopy': 3, 'feedback': 3})}\n",
      "{'aim identify': Counter()}\n",
      "{'blood supply': Counter({'tomography': 25, 'embolization': 12, 'neovascularization': 10, 'hernia': 9, 'mice': 6, 'radiography': 6, 'muscle': 5, 'carcinoma': 5, 'image processing': 4, 'echocardiography': 3, 'remission': 3, 'statistics': 2, 'range of motion': 2})}\n",
      "{'major adverse cardiovascular events': Counter({'ultrasonography': 18, 'ventricular dysfunction': 12, 'tomography': 9, 'echocardiography': 6})}\n",
      "{'intraventricular hemorrhage': Counter({'tomography': 40, 'embolization': 15, 'infant': 10, 'image processing': 7, 'statistics': 6, 'ultrasonography': 2})}\n",
      "{'pressure hg': Counter()}\n",
      "{'multiple organ': Counter({'tomography': 34, 'ultrasonography': 9, 'endocarditis': 6, 'aorta': 3, 'anemia': 3, 'embolization': 2})}\n",
      "{'penetration depth': Counter({'image processing': 25, 'phantoms': 15, 'tomography': 10, 'microscopy': 8, 'burns': 6, 'ultrasonography': 3, 'spectroscopy': 1})}\n",
      "{'performance proposed': Counter()}\n",
      "{'acute pancreatitis': Counter({'tomography': 66, 'carcinoma': 9, 'ultrasonography': 9, 'pancreatitis': 8, 'rats': 6, 'intubation': 5, 'embolization': 4, 'injections': 2, 'radiography': 1, 'anti-inflammatory agents': 1})}\n",
      "{'surgical removal': Counter({'tomography': 32, 'ultrasonography': 18, 'embolization': 11, 'pacemaker': 9, 'image processing': 9, 'catheters': 6, 'aorta': 6, 'endocarditis': 6, 'child': 6, 'mice': 3})}\n",
      "{'improve detection': Counter({'neovascularization': 6, 'embolization': 6, 'scleroderma': 6, 'image processing': 4, 'diagnosis': 3})}\n",
      "{'surgical management': Counter({'tomography': 108, 'image processing': 22, 'ultrasonography': 22, 'biopsy': 21, 'tuberculosis': 15, 'aorta': 12, 'range of motion': 9, 'child': 9, 'muscle': 6, 'twins': 6, 'monitoring': 6, 'carcinoma': 5, 'embolization': 4, 'fractures': 3, 'remission': 3, 'sterilization': 2, 'radiography': 1})}\n",
      "{'prospective randomized controlled': Counter({'range of motion': 12, 'ultrasonography': 9, 'tomography': 3})}\n",
      "{'median interquartile iqr': Counter()}\n",
      "{'proven effective': Counter({'stress': 6, 'ultrasonography': 2})}\n",
      "{'radiofrequency ablation rfa': Counter()}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-61c9216691a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-94-43938a874f02>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top 20 trends:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint_trend_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpopulate_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MRI trends:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmri_trends\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_N_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'magnetic resonance imaging'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-84-fec6fbb28935>\u001b[0m in \u001b[0;36mpopulate_graph\u001b[0;34m(trends_list)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrends_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnod\u001b[0m\u001b[0;31m#e1 = convert_trend(trend.trend)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnode1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nod' is not defined"
     ]
    }
   ],
   "source": [
    "build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
