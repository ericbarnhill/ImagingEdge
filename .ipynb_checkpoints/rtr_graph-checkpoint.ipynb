{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTR Connectivity Graph\n",
    "\n",
    "- Index pruned trends to pruned keywords\n",
    "- Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "- Make Network With Pruned Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "%run rtr.ipynb\n",
    "import operator\n",
    "import pickle\n",
    "PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/12_mo_nodedupe/\"\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_data():\n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"df.pickle\", \"rb\") as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = get_top_keywords(n_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    #print(top_keywords_text)\n",
    "    # TRIPLE LOOP - sure to be a bottleneck\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            # direct method only --\n",
    "            # update to include sql compatibility\n",
    "            # when sql is working\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            #print(\"matching trend\", trend.trend, \"and keyword\", keyword)\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ten most heavily weighted edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, keyword, n=10):\n",
    "    node_edges = G.edges(keyword.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find which keywords made the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_keywords(G, N=20):\n",
    "    keywords =  {}\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['is_key']:\n",
    "            total_wt = 0\n",
    "            for edge in G.edges(node[0], data=True):\n",
    "                total_wt += edge[2]['weight']\n",
    "            keywords.update({node[0]: total_wt})\n",
    "    n = 0\n",
    "    hot_keywords = []\n",
    "    for item in sorted(keywords.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            break\n",
    "        else:\n",
    "            hot_keywords.append(item)\n",
    "    return hot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trends(trends):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = ' '.join(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return trends_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pickle graph\n",
    "def pickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"wb\") as fp:\n",
    "            pickle.dump(G, fp)\n",
    "    except:\n",
    "        print('error')\n",
    "        \n",
    "def unpickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"rb\") as fp:\n",
    "            G = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return G\n",
    "        \n",
    "def unpickle_bow_trends():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/bow_converted.txt\", \"rb\") as fp:\n",
    "            bow_trends = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return bow_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def centrality_measures(G):\n",
    "    dc = nx.degree_centrality(G)\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    ec = nx.eigenvector_centrality_numpy(G)\n",
    "    dc = sorted(dc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bc = sorted(bc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ec = sorted(ec.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dc, bc, ec\n",
    "#    for key, value in sorted(bc.items(), key=operator.itemgetter(1), reverse=True):\n",
    "#        print(key, value)\n",
    "#        n += 1\n",
    "#        if n > 10:\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_app(G, trends_converted):\n",
    "    GRAPH_PATH = \"/home/ericbarnhill/Documents/code/insight_app/G.pickle\"\n",
    "    TRENDS_PATH = \"/home/ericbarnhill/Documents/code/insight_app/trends_converted.pickle\"\n",
    "    with open(GRAPH_PATH, \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(TRENDS_PATH, \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(set_up=False):\n",
    "    if set_up:\n",
    "        setup()\n",
    "    N_KEYWORDS = 10000\n",
    "    L = 5\n",
    "    trends, records, df = unpickle_data()\n",
    "    trends_list = pair_trends_keywords(df, records,\n",
    "                                       len(trends), N_KEYWORDS, from_sql = True)\n",
    "    print(\"Top 20 trends:\")\n",
    "    print_trend_dict(trends_list, 20)\n",
    "    G = populate_graph(trends_list)\n",
    "    print(\"MRI trends:\")\n",
    "    mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "    print(mri_trends)\n",
    "    print(\"Hottest keywords:\")\n",
    "    hot_keywords = get_hot_keywords(G)\n",
    "    print(hot_keywords)\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    print(\"Top degree centrality:\", list(dc)[:L])\n",
    "    print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "    print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "    trends_converted = convert_trends(trends)\n",
    "    export_to_app(G, trends_converted)\n",
    "    return G, trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts', 'rtr_keywords']\n",
      "Top 20 trends:\n",
      "{'eyes normal': Counter()}\n",
      "{'acute pancreatitis': Counter({'tomography': 66, 'carcinoma': 9, 'ultrasonography': 9, 'pancreatitis': 8, 'rats': 6, 'intubation': 5, 'embolization': 4, 'injections': 2, 'radiography': 1, 'anti-inflammatory agents': 1})}\n",
      "{'performance proposed': Counter()}\n",
      "{'chronic total': Counter({'ultrasonography': 47, 'tomography': 7, 'spectroscopy': 3, 'radiography': 3})}\n",
      "{'iqr median': Counter()}\n",
      "{'infarction stroke': Counter()}\n",
      "{'inclusion exclusion': Counter()}\n",
      "{'systemic therapy': Counter({'tomography': 9, 'anti-inflammatory agents': 6, 'microscopy': 6})}\n",
      "{'mesial temporal lobe': Counter({'embolization': 12, 'image processing': 9, 'epilepsy': 9, 'dna': 6, 'ultrasonography': 4})}\n",
      "{'unique identifier': Counter({'ventricular dysfunction': 8, 'tomography': 8, 'transplantation': 6, 'stroke': 6, 'ultrasonography': 3, 'spectroscopy': 3, 'muscle': 2})}\n",
      "{'presented emergency': Counter()}\n",
      "{'images demonstrated': Counter({'tomography': 26, 'microscopy': 3, 'lasers': 3, 'carcinoma': 2})}\n",
      "{'url http unique identifier': Counter()}\n",
      "{'congenital anomalies': Counter({'chromosomes': 12, 'hernia': 9, 'tomography': 6, 'statistics': 6, 'image processing': 5, 'ultrasonography': 4, 'echocardiography': 2})}\n",
      "{'url http unique': Counter()}\n",
      "{'major adverse cardiovascular events': Counter({'ultrasonography': 18, 'ventricular dysfunction': 12, 'tomography': 9, 'echocardiography': 6})}\n",
      "{'man history': Counter()}\n",
      "{'intraventricular hemorrhage': Counter({'tomography': 40, 'embolization': 15, 'infant': 10, 'image processing': 7, 'statistics': 6, 'ultrasonography': 2})}\n",
      "{'pressure hg': Counter()}\n",
      "{'penetration depth': Counter({'image processing': 25, 'phantoms': 15, 'tomography': 10, 'microscopy': 8, 'burns': 6, 'ultrasonography': 3, 'spectroscopy': 1})}\n",
      "{'cardiovascular cv': Counter()}\n",
      "MRI trends:\n",
      "['tomography 1056', 'image processing 1018', 'muscle 268', 'ultrasonography 211', 'child 148', 'range of motion 116', 'embolization 98', 'cartilage 71', 'fractures 70', 'statistics 66']\n",
      "Hottest keywords:\n",
      "[('tomography', 121671), ('image processing', 77839), ('ultrasonography', 32301), ('ventricular dysfunction', 24089), ('microscopy', 23300), ('muscle', 17493), ('embolization', 13293), ('range of motion', 10646), ('spectroscopy', 10368), ('child', 9521), ('carcinoma', 9111), ('echocardiography', 9019), ('rats', 8714), ('aorta', 8650), ('statistics', 6805), ('radiotherapy', 6056), ('diagnosis', 5333), ('biopsy', 5227), ('radiography', 5207), ('magnetic resonance imaging', 4362)]\n",
      "Top degree centrality: [('tomography', 0.6812428078250863), ('image processing', 0.5909090909090909), ('ultrasonography', 0.35083429228998847), ('muscle', 0.24626006904487918), ('microscopy', 0.2008055235903337)]\n",
      "Top betweenness centrality: [('tomography', 0.3861764352929985), ('image processing', 0.2912626045250162), ('ultrasonography', 0.09181098667274493), ('ventricular dysfunction', 0.05033426429909279), ('muscle', 0.048664933920163765)]\n",
      "Top eigencentrality: [('tomography', 0.3881370452889004), ('image processing', 0.3362208619767169), ('ultrasonography', 0.22379749922417852), ('muscle', 0.16298960265268797), ('ventricular dysfunction', 0.12464433541627333)]\n"
     ]
    }
   ],
   "source": [
    "G, trends_converted = run_nb(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc, bc, ec = centrality_measures(G)\n",
    "L = 10\n",
    "web_list = {'imaging':1, 'tomography':2, 'rats':3, 'ultrasonography':4, \\\n",
    "            'carcinoma':5, 'mice':6, 'echocardiography':7, 'diagnosis':8, \\\n",
    "           'microscopy':9, 'cartilage':10}\n",
    "dc_list = {}\n",
    "bc_list = {}\n",
    "ec_list = {}\n",
    "for n in range(L):\n",
    "    dc_list.update({list(dc)[n][0]:n+1})\n",
    "    bc_list.update({list(bc)[n][0]:n+1})\n",
    "    ec_list.update({list(ec)[n][0]:n+1})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_scatter_plots():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "    dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "    dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "    ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "    ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "    bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "    bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))\n",
    "    import altair as alt\n",
    "    chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "        x = 'listnum:O',\n",
    "        y = 'value:O', \n",
    "        color = 'variable'\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends, records, df = unpickle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intercept': 1.4685314685314674,\n",
       " 'slope': -0.08518753973299421,\n",
       " 'resid': array([1.54159289]),\n",
       " 'trend_score': array([-0.05525943]),\n",
       " 'vals': array([26., 39., 58., 46., 48., 17., 30., 14., 29., 23., 22., 11.]),\n",
       " 'total_mentions': 363.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends[list(trends)[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ericbarnhill/Documents/code/insight/rtr/12_mo'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = df.iloc[0,0]\n",
    "sum(df['key']==t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_df(df):\n",
    "    df_filt = df.copy(deep=True)\n",
    "    df_filt = df_filt[df_filt.score > 0]\n",
    "    for i in range(df.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Term \",i)\n",
    "        single_let = False\n",
    "        term = df.iloc[i,0]\n",
    "        for element in term:\n",
    "            if len(element) == 1:\n",
    "                print(\"dropping \",term,\" as it contains a single letter term\")\n",
    "                df_filt.drop(df_filt[df_filt['key'] == term].index, inplace=True)\n",
    "                single_let = True\n",
    "        if not single_let:\n",
    "            term_set = set(term)    \n",
    "            for j in range(df.shape[0]):\n",
    "                entry = df.iloc[j,0]\n",
    "                entry_set = set(entry)\n",
    "                if i != j:\n",
    "                    if entry_set.issubset(term_set):\n",
    "                        df_filt.drop(df_filt[df_filt['key'] == entry].index, inplace=True)            \n",
    "    print(\"df length\", df.shape)\n",
    "    print(\"df filt length\", df_filt.shape)\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term  0\n",
      "Term  1000\n",
      "Term  2000\n",
      "Term  3000\n",
      "Term  4000\n",
      "Term  5000\n",
      "Term  6000\n",
      "Term  7000\n",
      "Term  8000\n",
      "Term  9000\n",
      "df length (9023, 3)\n",
      "df filt length (997, 3)\n",
      "number of positive trends: 93\n",
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts', 'rtr_keywords']\n",
      "Top 20 trends:\n",
      "{'acute pancreatitis': Counter({'tomography': 66, 'carcinoma': 9, 'ultrasonography': 9, 'pancreatitis': 8, 'rats': 6, 'intubation': 5, 'embolization': 4, 'injections': 2, 'radiography': 1, 'anti-inflammatory agents': 1})}\n",
      "{'performance proposed': Counter()}\n",
      "{'infarction stroke': Counter()}\n",
      "{'systemic therapy': Counter({'tomography': 9, 'anti-inflammatory agents': 6, 'microscopy': 6})}\n",
      "{'images demonstrated': Counter({'tomography': 26, 'microscopy': 3, 'lasers': 3, 'carcinoma': 2})}\n",
      "{'url http unique identifier': Counter()}\n",
      "{'congenital anomalies': Counter({'chromosomes': 12, 'hernia': 9, 'tomography': 6, 'statistics': 6, 'image processing': 5, 'ultrasonography': 4, 'echocardiography': 2})}\n",
      "{'major adverse cardiovascular events': Counter({'ultrasonography': 18, 'ventricular dysfunction': 12, 'tomography': 9, 'echocardiography': 6})}\n",
      "{'man history': Counter()}\n",
      "{'intraventricular hemorrhage': Counter({'tomography': 40, 'embolization': 15, 'infant': 10, 'image processing': 7, 'statistics': 6, 'ultrasonography': 2})}\n",
      "{'pressure hg': Counter()}\n",
      "{'penetration depth': Counter({'image processing': 25, 'phantoms': 15, 'tomography': 10, 'microscopy': 8, 'burns': 6, 'ultrasonography': 3, 'spectroscopy': 1})}\n",
      "{'cardiovascular cv': Counter()}\n",
      "{'presenting symptoms': Counter({'tomography': 16, 'child': 9, 'biopsy': 6, 'ultrasonography': 3, 'endocarditis': 2, 'cardiomyopathy': 1, 'embolization': 1})}\n",
      "{'mg daily': Counter({'tomography': 6, 'diet': 4, 'sheep': 3, 'rats': 2})}\n",
      "{'normal coronary': Counter({'echocardiography': 9, 'ventricular dysfunction': 6, 'tomography': 3})}\n",
      "{'highlight importance': Counter()}\n",
      "{'presented history': Counter()}\n",
      "{'infectious disease': Counter({'tomography': 44, 'microscopy': 21, 'pregnancy complications': 12, 'communicable diseases': 9, 'lung diseases': 9, 'myelitis': 6, 'chromatography': 6, 'virulence factors': 3, 'peritonitis': 1})}\n",
      "{'precision medicine': Counter({'image processing': 8, 'diarrhea': 6, 'liver cirrhosis': 6, 'tomography': 3})}\n",
      "{'serum protein': Counter({'ventricular dysfunction': 6, 'lymphoma': 6, 'biomarkers': 4, 'rats': 1})}\n",
      "MRI trends:\n",
      "[]\n",
      "Hottest keywords:\n",
      "[('tomography', 517), ('image processing', 242), ('ultrasonography', 175), ('microscopy', 58), ('ventricular dysfunction', 54), ('muscle', 52), ('embolization', 43), ('phantoms', 40), ('crystallography', 30), ('biopsy', 29), ('diagnosis', 27), ('carcinoma', 25), ('echocardiography', 23), ('range of motion', 23), ('rna', 22), ('chromosomes', 21), ('spectroscopy', 21), ('statistics', 20), ('infant', 19), ('hernia', 18)]\n",
      "Top degree centrality: [('tomography', 0.27868852459016397), ('ultrasonography', 0.1639344262295082), ('image processing', 0.1639344262295082), ('blood supply', 0.10655737704918034), ('acute pancreatitis', 0.0819672131147541), ('histologically confirmed', 0.0819672131147541), ('infectious disease', 0.07377049180327869), ('conformational changes', 0.06557377049180328), ('muscle', 0.06557377049180328), ('measurement error', 0.06557377049180328)]\n",
      "Top betweenness centrality: [('tomography', 0.4860681405705789), ('image processing', 0.16957439553786893), ('ultrasonography', 0.1326259973598421), ('infectious disease', 0.11566861010230754), ('blood supply', 0.08395729510247837), ('conformational changes', 0.08007515654951347), ('acute pancreatitis', 0.07938469845247609), ('histologically confirmed', 0.07121045789944676), ('measurement error', 0.05732259997544057), ('penetration depth', 0.05636667984914698)]\n",
      "Top eigencentrality: [('tomography', 0.4660335234979402), ('ultrasonography', 0.29670070305666535), ('image processing', 0.2930683844320681), ('histologically confirmed', 0.18810310265528787), ('blood supply', 0.18265393946760966), ('phantom experiments', 0.17082662113467464), ('intraventricular hemorrhage', 0.16182182740444612), ('congenital anomalies', 0.16085954373739722), ('penetration depth', 0.1602826783663374), ('tissue perfusion', 0.14610101556913505)]\n"
     ]
    }
   ],
   "source": [
    "df_filt = filt_df(df)\n",
    "N_KEYWORDS = 10000\n",
    "L = 10\n",
    "num_above_zero = sum(df_filt.score > 0.1)\n",
    "print(\"number of positive trends:\", num_above_zero)\n",
    "trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                   round(num_above_zero*3/4), N_KEYWORDS, from_sql = True)\n",
    "print(\"Top 20 trends:\")\n",
    "print_trend_dict(trends_list, 20)\n",
    "G = populate_graph(trends_list)\n",
    "print(\"MRI trends:\")\n",
    "mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "print(mri_trends)\n",
    "print(\"Hottest keywords:\")\n",
    "hot_keywords = get_hot_keywords(G)\n",
    "print(hot_keywords)\n",
    "dc, bc, ec = centrality_measures(G)\n",
    "print(\"Top degree centrality:\", list(dc)[:L])\n",
    "print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "trends_converted = convert_trends(trends)\n",
    "export_to_app(G, trends_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                   round(num_above_zero*3/4), N_KEYWORDS, from_sql = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_converted = convert_trends(trends)\n",
    "export_to_app(G, trends_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(df_filt.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sort = df_filt.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_filt.score > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
