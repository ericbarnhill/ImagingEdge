{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 1: Scrape PubMed\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 1: (this notebook) of the app scrapes PubMed to build a database of abstracts and search terms from the Radiological literature.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 2: Convert PubMed abstracts to trends\n",
    "\n",
    "Part 3: Build graph connecting search terms and trends\n",
    "\n",
    "Part 4: Graph \"learns\" from unstructured sources\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel:\n",
      "/home/ericbarnhill/anaconda3/envs/ecb/bin/python\n",
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/rtr/scrape.log\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "print(\"Python kernel:\")\n",
    "!which python\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import scipy\n",
    "import altair\n",
    "import logging\n",
    "from dateutil.relativedelta import relativedelta\n",
    "chart_months = mdates.MonthLocator()\n",
    "chart_years = mdates.YearLocator()\n",
    "import time\n",
    "USER_EMAIL = 'ericbarnhill@gmail.com'\n",
    "dbname = 'rtr_db'\n",
    "username = 'ericbarnhill'\n",
    "pswd = 'carter0109'\n",
    "logfile = os.getcwd() + '/scrape.log'\n",
    "print(\"Logfile path: \", logfile)\n",
    "logging.basicConfig(filename=logfile,level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main methods for scraping PubMed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods to scrape PubMed\n",
    "from Bio import Entrez\n",
    "\n",
    "def query_pubmed(query, retmax=1):\n",
    "    \"\"\"Reads query and returns html with summary ot query including count. \n",
    "    \n",
    "    Does not return full abstracts. Used to get count for further queries.\n",
    "    \n",
    "    Source: \n",
    "        Searching PubMed with Biopython, https://gist.github.com/bonzanini/5a4c39e4c02502a8451d\n",
    "    \n",
    "    Args: \n",
    "        url: text string containing url.\n",
    "        \n",
    "    Returns:\n",
    "        String containing html contents of query.\n",
    "    \"\"\"\n",
    "    Entrez.email = USER_EMAIL\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax=retmax,\n",
    "                            retmode='xml', \n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def format_pubmed_query(start_date, end_date, mesh_terms):\n",
    "    \"\"\"Formats radiology subject query for PubMed given dates.\n",
    "        \n",
    "    Args:\n",
    "        start_date, end_date: datetime date objects.\n",
    "        \n",
    "    Returns:\n",
    "        String of URL containing date query.\n",
    "    \"\"\"\n",
    "    DATE_TIME_FORMAT = '%Y/%m/%d'\n",
    "    query_root = '(('\n",
    "    term_ctr = 0\n",
    "    for mesh_term in mesh_terms:\n",
    "        if term_ctr > 0:\n",
    "            query_root += ' OR ('\n",
    "        query_root += mesh_term + '[MeSH Terms])'\n",
    "        term_ctr += 1\n",
    "    query_date_1 = ') AND (' + start_date.strftime(DATE_TIME_FORMAT)\n",
    "    query_date_1_post = '[Date - Publication] : '\n",
    "    query_date_2 = end_date.strftime(DATE_TIME_FORMAT)\n",
    "    query_date_2_post = '[Date - Publication])'\n",
    "    query = query_root + query_date_1 + query_date_1_post + query_date_2 + query_date_2_post\n",
    "    query += ' NOT (history[MeSH Terms])'\n",
    "    return query\n",
    "\n",
    "def triple2date(triple):\n",
    "    \"\"\"Convert (Y,M,D) triple to datetime object.\n",
    "        \n",
    "    Args:\n",
    "        Triple.\n",
    "        \n",
    "    Returns:\n",
    "        Datetime object.\n",
    "    \"\"\"\n",
    "    return datetime.date(triple[0], triple[1], triple[2])\n",
    "\n",
    "def fetch_details(query_res):\n",
    "    \"\"\"Fetches details for Pubmed entry by ID number given a query result\n",
    "    \n",
    "    Source: \n",
    "        Searching PubMed with Biopython, https://gist.github.com/bonzanini/5a4c39e4c02502a8451d\n",
    "    \n",
    "    Args:\n",
    "        Triple.\n",
    "        \n",
    "    Returns:\n",
    "        Datetime object.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    ids = ','.join(query_res['IdList'])\n",
    "    Entrez.email = USER_EMAIL\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    end = time.time()\n",
    "    logging.debug(\"elapsed time \" + str(round(end-start)) +  \" seconds\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test PubMed scraper with a single query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_query_test():\n",
    "    start_date = triple2date((2015, 2, 3))\n",
    "    end_date = triple2date((2015, 5, 3))\n",
    "    mesh_terms = ['radiology', 'diagnostic imaging']\n",
    "    query = format_pubmed_query(start_date, end_date, mesh_terms)\n",
    "    query_res = query_pubmed(query)\n",
    "    logging.info(\"length of query results: \" ,len(query_res))\n",
    "    logging.info(query)\n",
    "    details = fetch_details(query_res)\n",
    "    logging.info(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ballpark estimates on query numbers\n",
    "Next is a method that allowed me to get some ballpark estimates on a few things, by querying PubMed without scraping. First, I just wanted to estimate how many queries there were on a given topic for a particular rolling window. Second, I wanted to see what the relative numbers were for various mesh terms, that is, my original idea of \"radiology\" versus say \"magnetic resonance imaging\". \n",
    "\\[Code is currently commented out and output cleared for readibility\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMATE HOW MANY RELATED QUERIES OCCUR OVER N MONTHS\n",
    "def estimate_queries(start_year, end_year, window=1, months=[], daily=False, mesh_term='radiology', plot=True):\n",
    "    \"\"\"Estimates number of queries for a given MeSH and a given rolling window.\n",
    "    \n",
    "    Args:\n",
    "        start_year, end_year: integer years\n",
    "        window: size of rolling average in months\n",
    "        mesh_term: mesh_term to be queried\n",
    "    \n",
    "    Returns:\n",
    "        None. Displays matplotlib plot across the specified time frame.\n",
    "    \"\"\"\n",
    "    N_MONTHS = 12 # num months in year\n",
    "    FIRST_OF_MONTH = 1 # if not daily, use first day of month\n",
    "    counts = []\n",
    "    start_dates = []\n",
    "    if not months:\n",
    "        months = range(1, 13, window)\n",
    "    logging.info(\"Analyzing year: \")\n",
    "    for start_year in range(start_year, end_year):\n",
    "        logging.info(start_year, \"...\")\n",
    "        end_year = start_year\n",
    "        # FOR ROLLING\n",
    "        # for start_month in range(1,N_MONTHS+1):\n",
    "        # FOR SEPARATE\n",
    "        for start_month in months:\n",
    "            start_date = datetime.date(start_year, start_month, FIRST_OF_MONTH)\n",
    "            end_month = start_month + window\n",
    "            if end_month > N_MONTHS:\n",
    "                end_year += 1\n",
    "                end_month = end_month % N_MONTHS\n",
    "            end_date = datetime.date(end_year, end_month, FIRST_OF_MONTH)\n",
    "            if not daily:\n",
    "                query_res = query_pubmed(format_pubmed_query(start_date, end_date, mesh_term))\n",
    "                count = query_res['Count']\n",
    "                counts.append(count)\n",
    "                start_dates.append(start_date)\n",
    "            else:\n",
    "                date_range = (end_date - start_date).days\n",
    "                for date_index in range(date_range):\n",
    "                    logging.debug(date_index)\n",
    "                    date = start_date + datetime.timedelta(days=date_index)\n",
    "                    query_res = query_pubmed(format_pubmed_query(date, \n",
    "                                date+datetime.timedelta(days=1), mesh_term))\n",
    "                    count = query_res['Count']\n",
    "                    counts.append(count)\n",
    "                    start_dates.append(date)\n",
    "\n",
    "    counts = pd.DataFrame(np.double(np.array(counts)))\n",
    "    counts.columns = ['Counts']\n",
    "    logging.info('Median counts: ', counts.median(axis=0))\n",
    "    counts['Dates'] = pd.to_datetime(pd.Series(start_dates))\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(counts.Dates, counts.Counts)\n",
    "        if not daily:\n",
    "            # format the ticks\n",
    "            ax.xaxis.set_major_locator(chart_years)\n",
    "            ax.xaxis.set_minor_locator(chart_months)\n",
    "        else:\n",
    "            for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "                if n % 2 != 0:\n",
    "                    label.set_visible(False)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for \"Magnetic Resonance Imaging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = estimate_queries(2010, 2017, 1, mesh_term = 'magnetic resonance imaging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for \"Radiology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = estimate_queries(2010, 2017, 1, mesh_term = 'radiology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Radiology had more queries per month than MRI, it was only 50% more or so. This suggests that the Radiology MeSH term is only reaching a small portion of the abstracts that might contain early trends of interest to radiologists.\n",
    "\n",
    "But first there is a funny noise issue. Both methods spiked at the first of the year, and I also needed to account for this behavior. Is it noise, or are there more valid abstracts at this time of year for some reason?\n",
    "\n",
    "The noise is also seen looking at monthly data. Here is an example from three months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = estimate_queries(2011, 2012, months=range(1, 3), daily=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's clear that on a monthly basis, publications spike on the first of the month as well. This suggests that PubMed indexes some publications with year-only dates, and some with month-only dates. These are indexed as the first of the year and month respectively. These are probably not typical abstracts which will come with a full publication date, so I expect them to wash out when the abstracts get filtered later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pubmed scraping method\n",
    "\n",
    "Below is the method that combines the above methods to scrape abstracts and dates from PubMed for a given MeSH term. The retained abstracts (those which have both text abstract and date) are returned in the form of a list of dicts, the dicts having keys \"Abstract\" and \"Date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pubmed_abstracts(start_date, end_date, \n",
    "            mesh_terms=['radiology', 'diagnostic imaging'], debug=False):\n",
    "    \"\"\"Scrape PubMed from a start to a finish date.\n",
    "    \n",
    "    Note that since PubMed only indexes publications by year, all queries for a year are gathered\n",
    "    together, then the dates for each publication are reprocessed.\n",
    "    \n",
    "    Args:\n",
    "        start_date: first date of the scrape\n",
    "        end_date: last date of the scrape\n",
    "        mesh_terms: categorizations from PubMed\n",
    "        \n",
    "    Returns:\n",
    "        HTML of example query.\n",
    "    \"\"\"\n",
    "    query = format_pubmed_query(start_date, end_date, mesh_terms)\n",
    "    logging.info(\"Query text: \" + str(query))\n",
    "    get_count = query_pubmed(query)\n",
    "    count = get_count['Count']\n",
    "    logging.info('Raw Count: ' + count)\n",
    "    query_res = query_pubmed(query, count)\n",
    "    t1 = time.time()\n",
    "    details = fetch_details(query_res)\n",
    "    t2 = time.time()\n",
    "    logging.info(\"Time to fetch: \" +  str(t2-t1))\n",
    "    retained_abstracts = []\n",
    "    num_have_abst = 0\n",
    "    num_have_date = 0\n",
    "    num_have_keys = 0\n",
    "    for i, article in enumerate(details['PubmedArticle']):\n",
    "        has_abst = False\n",
    "        has_date = False\n",
    "        has_keys = False\n",
    "        logging.debug('---- ARTICLE ' + str(i) + ' ----')\n",
    "        citation_article = article['MedlineCitation']['Article']\n",
    "        if 'Abstract' in citation_article:\n",
    "            has_abst = True\n",
    "            num_have_abst += 1\n",
    "            abstract_text = citation_article['Abstract']['AbstractText']\n",
    "            abst = []\n",
    "            for text_el in abstract_text:\n",
    "                abst.append(text_el.lower())\n",
    "            logging.debug('-- ABSTRACT --')\n",
    "            logging.debug(str(abst))\n",
    "        if citation_article['ArticleDate']:\n",
    "            has_date = True\n",
    "            num_have_date += 1\n",
    "            article_date_dict = citation_article['ArticleDate'][0]\n",
    "            date_triple = (\n",
    "                int(article_date_dict['Year']), \n",
    "                int(article_date_dict['Month']), \n",
    "                int(article_date_dict['Day'])\n",
    "            )\n",
    "            logging.debug('-- DATE --')\n",
    "            logging.debug(str(triple2date(date_triple)))\n",
    "        else:\n",
    "            if article['MedlineCitation']['DateCompleted']:\n",
    "                has_date = True\n",
    "                num_have_date += 1\n",
    "                date_completed = article['MedlineCitation']['DateCompleted']\n",
    "                date_triple = (\n",
    "                    int(date_completed['Year']), \n",
    "                    int(date_completed['Month']), \n",
    "                    int(date_completed['Day'])\n",
    "                )\n",
    "        keywords = article['MedlineCitation']['KeywordList']\n",
    "        if keywords:\n",
    "            has_keys = True\n",
    "            num_have_keys += 1\n",
    "            keywords = keywords_to_list(keywords)\n",
    "        if article['MedlineCitation']['MeshHeadingList']:\n",
    "            if has_keys == False:\n",
    "                has_keys = True\n",
    "                num_have_keys += 1\n",
    "            for item in article['MedlineCitation']['MeshHeadingList']:\n",
    "                keywords.append(item['DescriptorName'].lower())\n",
    "            logging.debug('--MESH HEADING LIST--')\n",
    "            logging.debug(str(keywords))\n",
    "        if has_abst and has_date and has_keys:\n",
    "            logging.debug('**ACCEPTED**')\n",
    "            retained_abstracts.append({'Abstract':abst, \n",
    "                                       'Date':triple2date(date_triple),\n",
    "                                       'Keywords':'|'.join(set(keywords))})\n",
    "        logging.info(\"Retained Count: \" + str(len(retained_abstracts)))\n",
    "        logging.info(\"Number With Abstracts: \" + str(num_have_abst))\n",
    "        logging.info(\"Number With Dates: \" + str(num_have_date))\n",
    "        logging.info(\"Number With Keywords: \" + str(num_have_keys))\n",
    "    return retained_abstracts\n",
    "\n",
    "def keywords_to_list(keywords):\n",
    "    keywords_list = []\n",
    "    for keyword in keywords[0]:\n",
    "        # the trick to extract from the StringElement\n",
    "        # is to call lower, which we need anyway\n",
    "        keywords_list.append(keyword.lower())\n",
    "    return keywords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of scraping method\n",
    "Below abstracts are identified for a sample date (February 2nd) in 2016 and counted. While PubMed reports 190 abstracts, only 148 have both an abstract and a date, and are thus suitable for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_example():\n",
    "    \"\"\" Performs one-day test scrape of PubMed database.\n",
    "    \"\"\"\n",
    "    start_date = datetime.date(2016, 2, 2)\n",
    "    end_date = datetime.date(2016, 2, 3)\n",
    "    mesh_terms = ['radiology', 'diagnostic_imaging']\n",
    "    retained_abstracts = scrape_pubmed_abstracts(start_date, end_date, mesh_terms)\n",
    "    print(\"Count retained: \", len(retained_abstracts))\n",
    "    print(\"Example abstract: \")\n",
    "    random_abstract_index = int(np.asscalar(np.round(np.random.rand(1)*len(retained_abstracts))))\n",
    "    print(random_abstract_index)\n",
    "    print(retained_abstracts[random_abstract_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_scrape_test():\n",
    "    \"\"\" Performs two-month test scrape of PubMed database.\n",
    "    \"\"\"\n",
    "    start_date = datetime.date(2017, 3, 1)\n",
    "    end_date = datetime.date(2017, 4, 30)\n",
    "    retained_abstracts = scrape_pubmed_abstracts(start_date, end_date, debug=False)\n",
    "    print(\"Count retained:\", len(retained_abstracts))\n",
    "    return retained_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a postgreSQL database\n",
    "\n",
    "The scraped abstracts need to then be transferred to a local SQL engine. Here SQLAlchemy is used for all commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create engine\n",
    "from sqlalchemy import create_engine, select, insert\n",
    "from sqlalchemy import Table, Column, MetaData, String, Date, Float, Boolean, Integer\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy.engine.reflection import Inspector\n",
    "def create_rtr_sql_engine(dbname, username, pswd):\n",
    "    \"\"\" Checks if postgreSQL database has been created, and if not creates it.\n",
    "    \n",
    "    Args:\n",
    "        dbname: database name\n",
    "        username: database username\n",
    "        pswd: database password\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy metadata, engine and connection objects.\n",
    "    \"\"\"\n",
    "    metadata = MetaData()\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    if not database_exists(engine.url):\n",
    "        logging.info(\"Creating RTR database\")\n",
    "        create_database(engine.url)\n",
    "    connection = engine.connect()\n",
    "    logging.info('creating engine:')\n",
    "    logging.info('- ' + str(engine.url))\n",
    "    logging.info('- ' + str(engine.table_names()))\n",
    "    return metadata, engine, connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_sql(metadata):    \n",
    "    \"\"\" Drops SQL table. Will require additional manual intervention through the SQL command line if there are live connections.\n",
    "    \n",
    "    Args:\n",
    "        metadata: metadata object\n",
    "        \n",
    "    Returns:\n",
    "        void\n",
    "    \"\"\"\n",
    "    metadata.drop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_create_abstracts_table(metadata, engine): \n",
    "    \"\"\" Drops SQL table. Will require additional manual intervention through the SQL command line if there are live connections.\n",
    "    \n",
    "    Args:\n",
    "        metadata: metadata object\n",
    "        \n",
    "    Returns:\n",
    "        void\n",
    "    \"\"\"\n",
    "    rtr_abstracts = Table('rtr_abstracts', metadata,\n",
    "             Column('abstract', String()),\n",
    "             Column('date', Date()),\n",
    "             Column('keywords', String())\n",
    "    )\n",
    "    if 'rtr_abstracts' not in engine.table_names():\n",
    "        logging.info('Creating rtr_abstracts table.')\n",
    "        # create table\n",
    "        # could also have done: rtr_abstracts.create(engine, checkexists=True)\n",
    "        rtr_abstracts.create(engine)\n",
    "    else:\n",
    "        logging.info('rtr_abstracts table already exists. Columns are:')\n",
    "        for col in rtr_abstracts.columns:\n",
    "            logging.info(\"-\" + col)\n",
    "    return rtr_abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move retained abstracts into SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_abstracts_into_sql(retained_abstracts, rtr_abstracts, connection):\n",
    "    \"\"\"Moves a list of retained abstracts into a local SQL connection\n",
    "        \n",
    "    Args:\n",
    "        retained_abstracts: List of retained abstracts \n",
    "            following the expected dictionary form.\n",
    "        connection: SQLAlchemy connection object.\n",
    "        \n",
    "    Returns:\n",
    "        void\n",
    "    \"\"\"\n",
    "    for i, abstract in enumerate(retained_abstracts):\n",
    "        stmt = insert(rtr_abstracts).values(abstract=abstract['Abstract'], \n",
    "                                            date=abstract['Date'],\n",
    "                                            keywords= abstract['Keywords'])\n",
    "        results = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate table just for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup SQL engine\n",
    "def get_all_abstracts():\n",
    "    \"\"\"Moves a list of retained abstracts into a local SQL connection\n",
    "        \n",
    "    Args:\n",
    "        retained_abstracts: List of retained abstracts \n",
    "            following the expected dictionary form.\n",
    "        connection: SQLAlchemy connection object.\n",
    "        \n",
    "    Returns:\n",
    "        void\n",
    "    \"\"\"\n",
    "    dbname = 'rtr_db'\n",
    "    username = 'ericbarnhill'\n",
    "    pswd = 'carter0109'\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    stmt = select([rtr_abstracts])\n",
    "    abstracts = connection.execute(stmt).fetchall()\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def populate_keywords_table(abstracts):\n",
    "    \"\"\"Create SQL table of PubMed keywords.\n",
    "        \n",
    "    Args:\n",
    "        abstracts: List of retained abstracts \n",
    "\n",
    "    Returns:\n",
    "        void\n",
    "    \"\"\"\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_keywords = Table('rtr_keywords', metadata,\n",
    "             Column('keyword', String()),\n",
    "             Column('count', Integer())\n",
    "    )\n",
    "    if 'rtr_keywords' not in engine.table_names():\n",
    "        logging.info('Creating rtr_keywords table.')\n",
    "        rtr_keywords.create(engine)\n",
    "    keyword_counter = Counter()\n",
    "    for abstract in abstracts:\n",
    "        keywords = str.split(abstract[2], '|')\n",
    "        for keyword in keywords:\n",
    "            keyword_counter.update({keyword:1})\n",
    "    keyword_counter_sorted = sorted(keyword_counter.items(), \n",
    "                                   key=lambda i: i[1], reverse=True)\n",
    "    insert_counter = 0\n",
    "    for key, value in keyword_counter_sorted:\n",
    "        stmt = insert(rtr_keywords).values(keyword=key,\n",
    "                                          count=value)\n",
    "        connection.execute(stmt)\n",
    "        insert_counter += 1\n",
    "        if insert_counter % 1000 == 0:\n",
    "            logging.info(\"insert counter: \" + str(insert_counter))\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return keyword_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(cutoff):\n",
    "    \"\"\"Get the most frequent keywords, for inspection and evaluation.\n",
    "        \n",
    "    Args:\n",
    "        cutoff: number of desired keywords \n",
    "\n",
    "    Returns:\n",
    "        array of top keywords.\n",
    "    \"\"\"\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_keywords = Table('rtr_keywords', metadata,\n",
    "                 Column('keyword', String()),\n",
    "                 Column('count', Integer())\n",
    "        )\n",
    "    #rtr_keywords.drop(engine)\n",
    "    stmt = select([rtr_keywords])\n",
    "    results = connection.execute(stmt).fetchmany(cutoff)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_counter = populate_keywords_table(get_all_abstracts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer 2 years of filtered abstracts to database by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(start, n_weeks):\n",
    "    \"\"\"The scrape used for most ImagingEdge testing.\n",
    "        \n",
    "    Args:\n",
    "        cutoff: number of desired keywords \n",
    "\n",
    "    Returns:\n",
    "        array of top keywords.\n",
    "    \"\"\"\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    for week in range(n_weeks): \n",
    "        print(\"Week: \" + str(week)) # print for quick recovery from 503\n",
    "        curr_start = start + relativedelta(weeks=+week)\n",
    "        curr_end = curr_start + relativedelta(weeks=+1)\n",
    "        abstracts = scrape_pubmed_abstracts(curr_start, curr_end)\n",
    "        move_abstracts_into_sql(abstracts, rtr_abstracts, connection)\n",
    "    connection.close()\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week: 0\n",
      "Week: 1\n",
      "Week: 2\n",
      "Week: 3\n",
      "Week: 4\n",
      "Week: 5\n",
      "Week: 6\n",
      "Week: 7\n",
      "Week: 8\n",
      "Week: 9\n",
      "Week: 10\n",
      "Week: 11\n",
      "Week: 12\n",
      "Week: 13\n",
      "Week: 14\n",
      "Week: 15\n",
      "Week: 16\n",
      "Week: 17\n",
      "Week: 18\n",
      "Week: 19\n",
      "Week: 20\n",
      "Week: 21\n",
      "Week: 22\n",
      "Week: 23\n",
      "Week: 24\n",
      "Week: 25\n"
     ]
    }
   ],
   "source": [
    "#scrape(datetime.date(2018,1,1), 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
