{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling Trends In Radiology\n",
    "\n",
    "## Research trend aggregator for radiologists\n",
    "\n",
    "### Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)\n",
    "\n",
    "*Activity / TODO log*\n",
    "\n",
    "1. \\[DONE\\] Data Camp NLP class\n",
    "2. \\[DONE\\] read [Britney Spears Problem paper](https://www.jstor.org/stable/27859169?newaccount=true&read-now=1&seq=1#page_scan_tab_contents)\n",
    "3. write PubMed scraper \n",
    "    - [some R tempates on Gist](https://gist.github.com/briatte/542736520e8b42e6a08e)\n",
    "    - superseded by [using biopython to query PubMed](https://gist.github.com/bonzanini/5a4c39e4c02502a8451d)\n",
    "4. store abstracts in postgreSQL database\n",
    "5. filter abstracts (TODO: sublist of filtering steps)\n",
    "6. draw queries in SQL by date range\n",
    "    - complete SQL datacamp course in preparation\n",
    "7. identify subjects with named entity recognition\n",
    "8. calculate z scores from one month to another (current mentions / leaky integrator of previous mentions)\n",
    "9. create annotated email with highest z score terms and a list of papers that use these terms sorted by (impact factor?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "import altair\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import scipy\n",
    "from dateutil.relativedelta import relativedelta\n",
    "chart_months = mdates.MonthLocator()  # every month\n",
    "chart_years = mdates.YearLocator()  # every month\n",
    "import time\n",
    "os.getcwd()\n",
    "USER_EMAIL = 'ericbarnhill@gmail.com'\n",
    "dbname = 'rtr_db'\n",
    "username = 'ericbarnhill'\n",
    "pswd = 'carter0109'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main methods for scraping PubMed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods to scrape PubMed\n",
    "from Bio import Entrez\n",
    "\n",
    "def query_pubmed(query, retmax=1):\n",
    "    \"\"\"Reads query and returns html with summary ot query including count. \n",
    "    \n",
    "    Does not return full abstracts. Used to get count for further queries.\n",
    "    \n",
    "    Source: \n",
    "        Searching PubMed with Biopython, https://gist.github.com/bonzanini/5a4c39e4c02502a8451d\n",
    "    \n",
    "    Args: \n",
    "        url: text string containing url.\n",
    "        \n",
    "    Returns:\n",
    "        String containing html contents of query.\n",
    "    \"\"\"\n",
    "    Entrez.email = USER_EMAIL\n",
    "    handle = Entrez.esearch(db='pubmed', \n",
    "                            sort='relevance', \n",
    "                            retmax=retmax,\n",
    "                            retmode='xml', \n",
    "                            term=query)\n",
    "    results = Entrez.read(handle)\n",
    "    return results\n",
    "\n",
    "def format_pubmed_query(start_date, end_date, mesh_terms):\n",
    "    \"\"\"Formats radiology subject query for PubMed given dates.\n",
    "        \n",
    "    Args:\n",
    "        start_date, end_date: datetime date objects.\n",
    "        \n",
    "    Returns:\n",
    "        String of URL containing date query.\n",
    "    \"\"\"\n",
    "    DATE_TIME_FORMAT = '%Y/%m/%d'\n",
    "    query_root = '(('\n",
    "    term_ctr = 0\n",
    "    for mesh_term in mesh_terms:\n",
    "        if term_ctr > 0:\n",
    "            query_root += ' OR ('\n",
    "        query_root += mesh_term + '[MeSH Terms])'\n",
    "        term_ctr += 1\n",
    "    query_date_1 = ') AND (' + start_date.strftime(DATE_TIME_FORMAT)\n",
    "    query_date_1_post = '[Date - Publication] : '\n",
    "    query_date_2 = end_date.strftime(DATE_TIME_FORMAT)\n",
    "    query_date_2_post = '[Date - Publication])'\n",
    "    query = query_root + query_date_1 + query_date_1_post + query_date_2 + query_date_2_post\n",
    "    query += ' NOT (history[MeSH Terms])'\n",
    "    return query\n",
    "\n",
    "def triple2date(triple):\n",
    "    \"\"\"Convert (Y,M,D) triple to datetime object.\n",
    "        \n",
    "    Args:\n",
    "        Triple.\n",
    "        \n",
    "    Returns:\n",
    "        Datetime object.\n",
    "    \"\"\"\n",
    "    return datetime.date(triple[0], triple[1], triple[2])\n",
    "\n",
    "def fetch_details(query_res, debug=False):\n",
    "    \"\"\"Fetches details for Pubmed entry by ID number given a query result\n",
    "    \n",
    "    Source: \n",
    "        Searching PubMed with Biopython, https://gist.github.com/bonzanini/5a4c39e4c02502a8451d\n",
    "    \n",
    "    Args:\n",
    "        Triple.\n",
    "        \n",
    "    Returns:\n",
    "        Datetime object.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        start = time.time()\n",
    "    ids = ','.join(query_res['IdList'])\n",
    "    Entrez.email = USER_EMAIL\n",
    "    handle = Entrez.efetch(db='pubmed',\n",
    "                           retmode='xml',\n",
    "                           id=ids)\n",
    "    results = Entrez.read(handle)\n",
    "    if debug:\n",
    "        end = time.time()\n",
    "        print(\"elapsed time \", round(end-start), \" seconds\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test PubMed scraper with a single query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_query_test():\n",
    "    start_date = triple2date((2015, 2, 3))\n",
    "    end_date = triple2date((2015, 5, 3))\n",
    "    mesh_terms = ['radiology', 'diagnostic imaging']\n",
    "    query = format_pubmed_query(start_date, end_date, mesh_terms)\n",
    "    query_res = query_pubmed(query)\n",
    "    print(\"length of query results: \" ,len(query_res))\n",
    "    print(query)\n",
    "    details = fetch_details(query_res)\n",
    "    print(details)\n",
    "#single_query_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ballpark estimates on query numbers\n",
    "Next is a method that allowed me to get some ballpark estimates on a few things, by querying PubMed without scraping. First, I just wanted to estimate how many queries there were on a given topic for a particular rolling window. Second, I wanted to see what the relative numbers were for various mesh terms, that is, my original idea of \"radiology\" versus say \"magnetic resonance imaging\". \n",
    "\\[Code is currently commented out and output cleared for readibility\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMATE HOW MANY RELATED QUERIES OCCUR OVER N MONTHS\n",
    "def estimate_queries(start_year, end_year, window=1, months=[], daily=False, mesh_term='radiology',\n",
    "                    debug=False, plot=True):\n",
    "    \"\"\"Estimates number of queries for a given MeSH and a given rolling window.\n",
    "    \n",
    "    Args:\n",
    "        start_year, end_year: integer years\n",
    "        window: size of rolling average in months\n",
    "        mesh_term: mesh_term to be queried\n",
    "    \n",
    "    Returns:\n",
    "        None. Displays matplotlib plot across the specified time frame.\n",
    "    \"\"\"\n",
    "    N_MONTHS = 12 # num months in year\n",
    "    FIRST_OF_MONTH = 1 # if not daily, use first day of month\n",
    "    counts = []\n",
    "    start_dates = []\n",
    "    if not months:\n",
    "        months = range(1, 13, window)\n",
    "    print(\"Analyzing year: \")\n",
    "    for start_year in range(start_year, end_year):\n",
    "        print(start_year, \"...\")\n",
    "        end_year = start_year\n",
    "        # FOR ROLLING\n",
    "        # for start_month in range(1,N_MONTHS+1):\n",
    "        # FOR SEPARATE\n",
    "        for start_month in months:\n",
    "            start_date = datetime.date(start_year, start_month, FIRST_OF_MONTH)\n",
    "            end_month = start_month + window\n",
    "            if end_month > N_MONTHS:\n",
    "                end_year += 1\n",
    "                end_month = end_month % N_MONTHS\n",
    "            end_date = datetime.date(end_year, end_month, FIRST_OF_MONTH)\n",
    "            if not daily:\n",
    "                query_res = query_pubmed(format_pubmed_query(start_date, end_date, mesh_term))\n",
    "                count = query_res['Count']\n",
    "                counts.append(count)\n",
    "                start_dates.append(start_date)\n",
    "            else:\n",
    "                date_range = (end_date - start_date).days\n",
    "                for date_index in range(date_range):\n",
    "                    if debug:\n",
    "                        print(date_index)\n",
    "                    date = start_date + datetime.timedelta(days=date_index)\n",
    "                    query_res = query_pubmed(format_pubmed_query(date, \n",
    "                                date+datetime.timedelta(days=1), mesh_term))\n",
    "                    count = query_res['Count']\n",
    "                    counts.append(count)\n",
    "                    start_dates.append(date)\n",
    "\n",
    "    counts = pd.DataFrame(np.double(np.array(counts)))\n",
    "    counts.columns = ['Counts']\n",
    "    print('Median counts: ', counts.median(axis=0))\n",
    "    counts['Dates'] = pd.to_datetime(pd.Series(start_dates))\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(counts.Dates, counts.Counts)\n",
    "        if not daily:\n",
    "            # format the ticks\n",
    "            ax.xaxis.set_major_locator(chart_years)\n",
    "            ax.xaxis.set_minor_locator(chart_months)\n",
    "        else:\n",
    "            for n, label in enumerate(ax.xaxis.get_ticklabels()):\n",
    "                if n % 2 != 0:\n",
    "                    label.set_visible(False)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for \"Magnetic Resonance Imaging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = estimate_queries(2010, 2017, 1, mesh_term = 'magnetic resonance imaging')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for \"Radiology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = estimate_queries(2010, 2017, 1, mesh_term = 'radiology')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Radiology had more queries per month than MRI, it was only 50% more or so. This suggests that the Radiology MeSH term is only reaching a small portion of the abstracts that might contain early trends of interest to radiologists.\n",
    "\n",
    "But first there is a funny noise issue. Both methods spiked at the first of the year, and I also needed to account for this behavior. Is it noise, or are there more valid abstracts at this time of year for some reason?\n",
    "\n",
    "The noise is also seen looking at monthly data. Here is an example from three months:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = estimate_queries(2011, 2012, months=range(1, 3), daily=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it's clear that on a monthly basis, publications spike on the first of the month as well. This suggests that PubMed indexes some publications with year-only dates, and some with month-only dates. These are indexed as the first of the year and month respectively. These are probably not typical abstracts which will come with a full publication date, so I expect them to wash out when the abstracts get filtered later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Possible TODO: Relationship between Radiology, MRI, and other MeSH terms: probably should have more terms than just Radiology to be optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pubmed scraping method\n",
    "\n",
    "Below is the method that combines the above methods to scrape abstracts and dates from PubMed for a given MeSH term. The retained abstracts (those which have both text abstract and date) are returned in the form of a list of dicts, the dicts having keys \"Abstract\" and \"Date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pubmed_abstracts(start_date, end_date, mesh_terms=['radiology', 'diagnostic imaging'], debug=False):\n",
    "    \"\"\"Scrape PubMed from a start to a finish date.\n",
    "    \n",
    "    Note that since PubMed only indexes publications by year, all queries for a year are gathered\n",
    "    together, then the dates for each publication are reprocessed.\n",
    "    \n",
    "    Args:\n",
    "        debug: turns on printing to watch processing of all abstracts.\n",
    "        \n",
    "    Returns:\n",
    "        HTML of example query.\n",
    "    \"\"\"\n",
    "    query = format_pubmed_query(start_date, end_date, mesh_terms)\n",
    "    print(\"Query text:\")\n",
    "    print(query)\n",
    "    get_count = query_pubmed(query)\n",
    "    count = get_count['Count']\n",
    "    #print(get_count)\n",
    "    print('Raw Count: ' + count)\n",
    "    query_res = query_pubmed(query, count)\n",
    "    t1 = time.time()\n",
    "    details = fetch_details(query_res, debug)\n",
    "    t2 = time.time()\n",
    "    print(\"Time to fetch: \", t2-t1)\n",
    "    retained_abstracts = []\n",
    "    num_have_abst = 0\n",
    "    num_have_date = 0\n",
    "    num_have_keys = 0\n",
    "    for i, article in enumerate(details['PubmedArticle']):\n",
    "        has_abst = False\n",
    "        has_date = False\n",
    "        has_keys = False\n",
    "        if debug:\n",
    "            print('---- ARTICLE ',i,' ----')\n",
    "            #print(article)\n",
    "        citation_article = article['MedlineCitation']['Article']\n",
    "        if 'Abstract' in citation_article:\n",
    "            has_abst = True\n",
    "            num_have_abst += 1\n",
    "            abstract_text = citation_article['Abstract']['AbstractText']\n",
    "            abst = []\n",
    "            for text_el in abstract_text:\n",
    "                abst.append(text_el.lower())\n",
    "            if debug:\n",
    "                print('-- ABSTRACT --')\n",
    "                print(abst)\n",
    "        if citation_article['ArticleDate']:\n",
    "            has_date = True\n",
    "            num_have_date += 1\n",
    "            article_date_dict = citation_article['ArticleDate'][0]\n",
    "            date_triple = (\n",
    "                int(article_date_dict['Year']), \n",
    "                int(article_date_dict['Month']), \n",
    "                int(article_date_dict['Day'])\n",
    "            )\n",
    "            if debug:\n",
    "                print('-- DATE --')\n",
    "                print(triple2date(date_triple))\n",
    "        else:\n",
    "            if article['MedlineCitation']['DateCompleted']:\n",
    "                has_date = True\n",
    "                num_have_date += 1\n",
    "                #print(\"no ArticleDate, caught DateCompleted\")\n",
    "                date_completed = article['MedlineCitation']['DateCompleted']\n",
    "                date_triple = (\n",
    "                    int(date_completed['Year']), \n",
    "                    int(date_completed['Month']), \n",
    "                    int(date_completed['Day'])\n",
    "                )\n",
    "        keywords = article['MedlineCitation']['KeywordList']\n",
    "        if keywords:\n",
    "            has_keys = True\n",
    "            num_have_keys += 1\n",
    "            keywords = keywords_to_list(keywords)\n",
    "            if debug:\n",
    "                print('--KEYWORDS--')\n",
    "                print(keywords)\n",
    "        if article['MedlineCitation']['MeshHeadingList']:\n",
    "            if has_keys == False:\n",
    "                has_keys = True\n",
    "                num_have_keys += 1\n",
    "            for item in article['MedlineCitation']['MeshHeadingList']:\n",
    "                keywords.append(item['DescriptorName'].lower())\n",
    "            if debug:\n",
    "                print('--MESH HEADING LIST--')\n",
    "                print(keywords)\n",
    "        if has_abst and has_date and has_keys:\n",
    "            if debug:\n",
    "                print('**ACCEPTED**')\n",
    "            retained_abstracts.append({'Abstract':abst, \n",
    "                                       'Date':triple2date(date_triple),\n",
    "                                       'Keywords':' '.join(set(keywords))})\n",
    "        if debug:\n",
    "            input('Press enter to see next abstract...')\n",
    "    print(\"Retained Count: \", len(retained_abstracts))\n",
    "    print(\"Number With Abstracts: \", num_have_abst)\n",
    "    print(\"Number With Dates: \", num_have_date)\n",
    "    print(\"Number With Keywords: \", num_have_keys)\n",
    "\n",
    "    return retained_abstracts\n",
    "\n",
    "def keywords_to_list(keywords):\n",
    "    keywords_list = []\n",
    "    for keyword in keywords[0]:\n",
    "        # the trick to extract from the StringElement\n",
    "        # is to call lower, which we need anyway\n",
    "        keywords_list.append(keyword.lower())\n",
    "    return keywords_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of scraping method\n",
    "Below abstracts are identified for a sample date (February 2nd) in 2016 and counted. While PubMed reports 190 abstracts, only 148 have both an abstract and a date, and are thus suitable for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_example():\n",
    "    start_date = datetime.date(2016, 2, 2)\n",
    "    end_date = datetime.date(2016, 2, 3)\n",
    "    mesh_terms = ['radiology', 'diagnostic_imaging']\n",
    "    retained_abstracts = scrape_pubmed_abstracts(start_date, end_date, mesh_terms)\n",
    "    print(\"Count retained: \", len(retained_abstracts))\n",
    "    print(\"Example abstract: \")\n",
    "    random_abstract_index = int(np.asscalar(np.round(np.random.rand(1)*len(retained_abstracts))))\n",
    "    print(random_abstract_index)\n",
    "    print(retained_abstracts[random_abstract_index])\n",
    "#scraping_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_scrape_test():\n",
    "    start_date = datetime.date(2017, 3, 1)\n",
    "    end_date = datetime.date(2017, 4, 30)\n",
    "    retained_abstracts = scrape_pubmed_abstracts(start_date, end_date, debug=False)\n",
    "    print(\"Count retained:\", len(retained_abstracts))\n",
    "    return retained_abstracts\n",
    "#retained_abstracts = month_scrape_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the abstracts to a postgreSQL database\n",
    "\n",
    "The scraped abstracts need to then be transferred to a local SQL engine. Here SQLAlchemy is used for all commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create engine\n",
    "from sqlalchemy import create_engine, select, insert\n",
    "from sqlalchemy import Table, Column, MetaData, String, Date, Float, Boolean, Integer\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy.engine.reflection import Inspector\n",
    "def create_rtr_sql_engine(dbname, username, pswd):\n",
    "    metadata = MetaData()\n",
    "    engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "    if not database_exists(engine.url):\n",
    "        print(\"Creating RTR database\")\n",
    "        create_database(engine.url)\n",
    "    connection = engine.connect()\n",
    "    print('creating engine:')\n",
    "    print('- ', engine.url)\n",
    "    print('- ', engine.table_names())\n",
    "    return metadata, engine, connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET SQL\n",
    "def reset_sql(metadata):\n",
    "    metadata.drop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if doesn't exist, create table\n",
    "def check_create_abstracts_table(metadata, engine): \n",
    "    rtr_abstracts = Table('rtr_abstracts', metadata,\n",
    "             Column('abstract', String()),\n",
    "             Column('date', Date()),\n",
    "             Column('keywords', String())\n",
    "    )\n",
    "    if 'rtr_abstracts' not in engine.table_names():\n",
    "        print('Creating rtr_abstracts table.')\n",
    "        # create table\n",
    "        # could also have done: rtr_abstracts.create(engine, checkexists=True)\n",
    "        rtr_abstracts.create(engine)\n",
    "    else:\n",
    "        print('rtr_abstracts table already exists. Columns are:')\n",
    "        for col in rtr_abstracts.columns:\n",
    "            print(\"-\", col)\n",
    "    return rtr_abstracts\n",
    "#check_create_abstracts_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move retained abstracts into SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_abstracts_into_sql(retained_abstracts, rtr_abstracts, connection):\n",
    "    \"\"\"Moves a list of retained abstracts into a local SQL connection\n",
    "        \n",
    "    Args:\n",
    "        retained_abstracts: List of retained abstracts \n",
    "            following the expected dictionary form.\n",
    "        connection: SQLAlchemy connection object.\n",
    "        \n",
    "    Returns:\n",
    "        void.\n",
    "    \"\"\"\n",
    "    for i, abstract in enumerate(retained_abstracts):\n",
    "        if i % 1000 == 0:\n",
    "            print('inserting abstract ',i,'...')\n",
    "        stmt = insert(rtr_abstracts).values(abstract=abstract['Abstract'], \n",
    "                                            date=abstract['Date'],\n",
    "                                            keywords= abstract['Keywords'])\n",
    "        results = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate table just for keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def populate_keywords_table(abstracts):\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_keywords = Table('rtr_keywords', metadata,\n",
    "             Column('keyword', String()),\n",
    "             Column('count', Integer())\n",
    "    )\n",
    "    if 'rtr_keywords' not in engine.table_names():\n",
    "        print('Creating rtr_keywords table.')\n",
    "        rtr_keywords.create(engine)\n",
    "    keyword_counter = Counter()\n",
    "    for abstract in abstracts:\n",
    "        keywords = abstract['Keywords']\n",
    "        for keyword in keywords:\n",
    "            keyword_counter.update({keyword:1})\n",
    "    keyword_counter_sorted = sorted(keyword_counter.items(), \n",
    "                                   key=lambda i: i[1], reverse=True)\n",
    "    insert_counter = 0\n",
    "    for key, value in keyword_counter_sorted:\n",
    "        stmt = insert(rtr_keywords).values(keyword=key,\n",
    "                                          count=value)\n",
    "        connection.execute(stmt)\n",
    "        insert_counter += 1\n",
    "        if insert_counter % 1000 == 0:\n",
    "            print(\"insert counter: \", insert_counter)\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return keyword_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_date = datetime.date(2016,1,1)\n",
    "#end_date = start_date + relativedelta(months=+12)\n",
    "#abstracts = scrape_pubmed_abstracts(start_date, end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_counter = populate_keywords_table(abstracts)\n",
    "#keyword_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keywords(cutoff):\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_keywords = Table('rtr_keywords', metadata,\n",
    "                 Column('keyword', String()),\n",
    "                 Column('count', Integer())\n",
    "        )\n",
    "    #rtr_keywords.drop(engine)\n",
    "    stmt = select([rtr_keywords])\n",
    "    results = connection.execute(stmt).fetchmany(cutoff)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer 2 years of filtered abstracts to database by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_test():\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    start = datetime.date(2016,1,2)\n",
    "    for week in range(104): #about 2 years\n",
    "        print(\"Week: \", week)\n",
    "        curr_start = start + relativedelta(weeks=+week)\n",
    "        curr_end = curr_start + relativedelta(weeks=+1)\n",
    "        abstracts = scrape_pubmed_abstracts(curr_start, curr_end)\n",
    "        move_abstracts_into_sql(abstracts, rtr_abstracts, connection)\n",
    "    connection.close()\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#production_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
