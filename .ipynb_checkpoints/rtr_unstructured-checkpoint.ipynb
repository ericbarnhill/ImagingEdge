{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 4: Mine Unstructured Sources\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 4: In (this notebook) of the app, the graphs \"learns\" by adding unstructured sources.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 1: Scrape PubMed\n",
    "\n",
    "Part 2: Convert PubMed abstracts to Bag of Words\n",
    "\n",
    "Part 3: Build graph connecting search terms and trends\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "### Part 1: Scraping Arxiv\n",
    "### Part 2: Scraping Twitter\n",
    "### Part 3: Scraping Custom URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pickle\n",
    "import os\n",
    "import twitterscraper\n",
    "import lxml\n",
    "import logging\n",
    "import contextlib\n",
    "import time\n",
    "YEAR = 2015\n",
    "PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/\" + str(YEAR) + \"/\"\n",
    "os.chdir(PATH)\n",
    "# copypasta from previous notebook -- todo: strategy to set these variables\n",
    "N_WINDOWS = 12\n",
    "N_MONTHS = 1\n",
    "START_DATE = datetime.date(YEAR,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arxiv(start_date, n_windows, n_months, search_term):\n",
    "    \"\"\"Pull abstracts within rolling date windows from the ArXiv\n",
    "        \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        node: radiological search term used to probe the ArXiv\n",
    "        \n",
    "    Returns:\n",
    "        List of arxiv abstract sets, one per rolling window\n",
    "    \"\"\"\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        start_string = start.strftime(\"%Y-%m-%d\")\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        end_string = end.strftime(\"%Y-%m-%d\")\n",
    "        #logging.info(\"Query from \" + str(start) + \" to \" + str(end))\n",
    "        # ArXiv API does not appear to handle combined abstract & date searches.\n",
    "        # Consequently ArXiv is scraped through the 'front door'\n",
    "        # Note that this has a hard limit of 200 per rolling window, however\n",
    "        # this seems to be sufficient in preliminary testing\n",
    "        arxiv_request = 'https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term='+ \\\n",
    "        search_term+ \\\n",
    "        '&terms-0-field=abstract&classification-physics_archives=all&' + \\\n",
    "        'date-year=&date-filter_by=date_range&' \\\n",
    "        'date-from_date=' + start_string + '&date-to_date=' + \\\n",
    "        end_string + '&size=200'\n",
    "        with request.urlopen(arxiv_request) as response:\n",
    "            page = response.read()\n",
    "        soup = BeautifulSoup(page, 'html.parser').get_text()\n",
    "        soup_splits = soup.split('More')\n",
    "        soup_abstracts = []\n",
    "        for n in range(1, len(soup_splits)): # skip first one, it is preparatory text\n",
    "            soup_split = soup_splits[n]\n",
    "            soup_abstracts.append(soup_split.split('Less')[0])\n",
    "        #(\"Acquired\", len(soup_abstracts), \"abstracts across time window\", n)\n",
    "        window_records.append(soup_abstracts)\n",
    "    return window_records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_twitter(start_date, n_windows, n_months, search_term, trending_term):\n",
    "    \"\"\"Scrape Twitter within rolling date windows\n",
    "    \n",
    "    To handle Twitter, the methodology changes a bit. I search the dates\n",
    "    for tweets containing search AND trending terms together.\n",
    "        \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        node: radiological search term used to probe the ArXiv\n",
    "        \n",
    "    Returns:\n",
    "        List of twitter mentions, one per rolling window\n",
    "    \"\"\"\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        query = search_term + ' AND ' + trending_term\n",
    "        logger = logging.getLogger('twitterscraper')\n",
    "        logger.disabled = True\n",
    "        twitter_response = twitterscraper.query_tweets(query, limit=200, \n",
    "                            begindate=start, \n",
    "                            enddate=end, poolsize=20, lang='')\n",
    "        logger.disabled = False\n",
    "        window_records.append(len(twitter_response))\n",
    "    return window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_and_trends(path):\n",
    "    with open(path+'/G.pickle', 'rb') as fp:\n",
    "        G = pickle.load(fp)\n",
    "    with open(path+'/trends_converted.pickle', 'rb') as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    return G, trends\n",
    "\n",
    "def save_graph_and_trends(path, G, trends):\n",
    "    with open(path+'/G_x.pickle', 'wb') as fp:\n",
    "        pickle.dump(G)\n",
    "    with open(path+'/trends_converted_x.pickle', 'wb') as fp:\n",
    "        pickle.dump(trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def develop_graph(path):\n",
    "    G, trends = load_graph_and_trends(path)\n",
    "    ## loop through nodes\n",
    "    for node,data in G.nodes(data=True):\n",
    "        if data['is_key']:\n",
    "            node_neighbors = list(G.neighbors(node))\n",
    "            # scrape the ArXiv\n",
    "            print(\"Scraping ArXiv for:\", node)\n",
    "            print(\"number of neighbors\", len(node_neighbors))\n",
    "            start=time.time()\n",
    "            arxiv_abstracts = scrape_arxiv(START_DATE, N_WINDOWS, N_MONTHS, node)\n",
    "            for i, abstract_set in enumerate(arxiv_abstracts):\n",
    "                for abstract in abstract_set:\n",
    "                    for neighbor in node_neighbors:\n",
    "                        if neighbor in abstract:\n",
    "                            # add a weight to the edge of the graph\n",
    "                            G[node][neighbor]['weight'] = G[node][neighbor]['weight'] + 1\n",
    "                            # add a mention in the trends data\n",
    "                            #print(\"Adding one to\", neighbor, \\\n",
    "                            #      \"new mentions total\", trends[neighbor][i] + 1)\n",
    "                            trends[neighbor][i] = trends[neighbor][i] + 1\n",
    "                            # If found in this time period in the ArXiv, look\n",
    "                            # at Twitter output.\n",
    "                            # This was done to reduce the computational scope.\n",
    "                            #print(\"Searching Twitter for\", node, '+', neighbor)\n",
    "                            #twitter_mentions = scrape_twitter(START_DATE, N_WINDOWS, N_MONTHS, \n",
    "                            #                                  node, neighbor)\n",
    "                            #for i, tweet_count in enumerate(twitter_mentions):\n",
    "                            #    G[node][neighbor]['weight'] = G[node][neighbor]['weight'] + 1\n",
    "                            #    trends[neighbor][i] = trends[neighbor][i] + 1\n",
    "            print(\"Elapsed time \", time.time()-start)\n",
    "    save_graph_and_trends(path, G, trends)\n",
    "    return G, trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "develop_graph(PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
