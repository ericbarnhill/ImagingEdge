{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTR BOW\n",
    "\n",
    "## Converts scraped PubMed to BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/rtr/2015/bow.log\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "%run rtr.ipynb\n",
    "from sqlalchemy import and_\n",
    "import pickle\n",
    "import logging\n",
    "import datetime\n",
    "YEAR = 2015\n",
    "PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/\" + str(YEAR) + \"/\"\n",
    "os.chdir(PATH)\n",
    "logfile = os.getcwd() + '/bow.log'\n",
    "print(\"Logfile path: \", logfile)\n",
    "logging.basicConfig(filename=logfile,level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure ngram trends within abstract rolling windows\n",
    "\n",
    "Find most common BOW terms from Diagnostic Imaging abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "# create radiology stops\n",
    "radiology_stops = ['pci', 'mm', 'auroc', 'roc', 'mri', 'ct', 'pet', 'us', \\\n",
    "                  'p', 'significant', 'significantly', 't', 'ci', 'highly', 'mm', \\\n",
    "                  'considered', 'increase', 'increased', 'decrease', \\\n",
    "                  'decreased', 'measure', 'group', 'standard', 'gold', \\\n",
    "                  'ground', 'truth', 'patient', 'inc', 'cad', \\\n",
    "                  'positive', 'negative', 'year', 'study', \\\n",
    "                  'one', 'two', 'three', 'wiley', 'springer', 'patient', \\\n",
    "                  'diagnosis', 'evaluate', 'statistical', 'statistically', \\\n",
    "                  'ass', 'cohort', 'confidence', 'interval', 'primary', \\\n",
    "                  'outcome', 'increasing', 'decreasing', 'clinical', \\\n",
    "                  'r', 'correlation', 'sample', 'size', 'compared', \\\n",
    "                  'randomly', 'divided', 'baseline', 'case', 'report', \\\n",
    "                  'outcome', 'also', 'half', 'may', 'can', 'powerful', \\\n",
    "                  'tool', 'ass', 'informed', 'consent', 'age', 'sex',\n",
    "                  'rare', 'cause', 'risk', 'likelihood', 'ratio', 'patients', \\\n",
    "                  'whether', 'regarding', 'including', 'results', 'result', 'mi', \\\n",
    "                  'data', 'set', 'ex', 'in', 'pro', 'mr', 'scan', \\\n",
    "                  'showed', 'studies', 'accuracy', 'performed', 'used', \\\n",
    "                  'determine', 'higher', 'lower', 'male', 'female', \\\n",
    "                  'important', 'role', 'remains', 'unclear', \\\n",
    "                  'range', 'years', 'months', 'days', 'young', 'old', \\\n",
    "                  'modality', 'modalities', 'oct', 'hcc', 'among', \\\n",
    "                  'could', 'useful', 'groups', 'respectively', \\\n",
    "                  'ifg', 'octa', 'mra', 'using', 'suggest', \\\n",
    "                  'revealed', 'options', 'treatment', 'findings', \\\n",
    "                  'medical', 'image', 'using', 'often', \\\n",
    "                  'underwent', 'high', 'low', 'assessed', 'method', \\\n",
    "                  'better', 'per', 'case', 'cases']\n",
    "\n",
    "def get_bow(retained_abstracts, ngram_range=range(1), from_sql = False,\n",
    "            dedupe = True, debug=False):\n",
    "    \"\"\"Get bag of words\n",
    "        \n",
    "    Args:\n",
    "        retained_abstracts: List of retained abstracts \n",
    "            following the expected dictionary form.\n",
    "        \n",
    "    Returns:\n",
    "        Counter object containing bag of words.\n",
    "    \"\"\"\n",
    "    BOW_all = Counter()\n",
    "    for i, abst in enumerate(retained_abstracts):\n",
    "        if (i) % 1000 == 0:\n",
    "            logging.info(\"Abstract \" + str(i) + \"...\")\n",
    "        if from_sql:\n",
    "            abst_txt = ''.join(abst[0])\n",
    "        else:\n",
    "            abst_txt = ''.join(abst['Abstract'])\n",
    "        abst_tokens = word_tokenize(abst_txt)\n",
    "        # already lower case from earlier parsing\n",
    "        # Retain alphabetic words: alpha_only\n",
    "        abst_alpha = [t for t in abst_tokens if t.isalpha()]\n",
    "        # Remove all stop words: no_stops\n",
    "        abst_no_eng_stops = [t for t in abst_alpha if t not in english_stops]\n",
    "        abst_no_stops = [t for t in abst_no_eng_stops if t not in radiology_stops]\n",
    "        # get rid of individual letters\n",
    "        abst_no_stops = [t for t in abst_no_stops if len(t) > 1]\n",
    "        # Instantiate the WordNetLemmatizer - decided against\n",
    "        #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # Lemmatize all tokens into a new list: lemmatized\n",
    "        #abst_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in abst_no_stops]\n",
    "        # Create the bag-of-words for each ngram: bow\n",
    "        bow_ngrams = []\n",
    "        for n in ngram_range:\n",
    "            if debug:\n",
    "                print(\"ngram length:\", n)\n",
    "            abst_ngrams = ngrams(abst_no_stops, n)\n",
    "            #abst_ngrams = ngrams(abst_lemmatized, n)\n",
    "            bow_ngrams.append(Counter(abst_ngrams))\n",
    "        # de-dupe by removing smaller ngrams contained within larger\n",
    "        if dedupe:\n",
    "            range_max = max(ngram_range)\n",
    "            range_min = min(ngram_range)\n",
    "            num_ngrams_sets = len(bow_ngrams)\n",
    "            for n in range(1, num_ngrams_sets):\n",
    "                for larger_key in list(bow_ngrams[n]):\n",
    "                    for smaller_key in list(bow_ngrams[n-1]):\n",
    "                        if ' '.join(smaller_key) in ' '.join(larger_key):\n",
    "                            del bow_ngrams[n-1][smaller_key]\n",
    "        for bow_ngram in bow_ngrams:\n",
    "            BOW_all += bow_ngram\n",
    "    return BOW_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join BOWs across rolling windows\n",
    "\n",
    "Next is to compare the normalized frequency of BOW ngrams across two or more time periods. First step is to filter the BOW for each window to only contain common ngrams. The method will produce some summary stats on that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bows(bows_unfilt):\n",
    "    \"\"\"Filter list BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    # allocate filtered bows\n",
    "    n_bows = len(bows_unfilt)\n",
    "    bows_filt = []\n",
    "    for n in range(n_bows):\n",
    "        bows_filt.append({})\n",
    "    # create intersected set of keys\n",
    "    intersected_keys = []\n",
    "    for bow in bows_unfilt:\n",
    "        if not intersected_keys:\n",
    "            logging.info('initializing intersected keys')\n",
    "            intersected_keys = bow.keys()\n",
    "            logging.info('length: ' + str(len(intersected_keys)))\n",
    "        else:\n",
    "            logging.info('updating intersected keys')\n",
    "            logging.info('length next set: ' + str(len(bow.keys())))\n",
    "            intersected_keys = intersected_keys & bow.keys()\n",
    "            logging.info('length intersected:' +  str(len(intersected_keys)))\n",
    "    # filter bows to only contain intersecting keys:\n",
    "    bows_filt = []\n",
    "    for bow in bows_unfilt:\n",
    "        bow_filt = { key: bow[key] for key in intersected_keys }\n",
    "        bows_filt.append(bow_filt)\n",
    "    return bows_filt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw abstracts in rolling windows from the SQL store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_window_abstracts_sql(start_date, n_windows, n_months, connection, rtr_abstracts):\n",
    "    \"\"\"Pull abstracts within rolling date windows from the SQL database\n",
    "        \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        connection: SQLAlchemy connection object\n",
    "        rtr_abstracts = RTR abstracts table object\n",
    "        \n",
    "    Returns:\n",
    "        List of window records, one per rolling window\n",
    "    \"\"\"\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        logging.info(\"Query from \" + str(start) + \" to \" + str(end))\n",
    "        stmt = select([rtr_abstracts])\n",
    "        stmt = stmt.where(\n",
    "            and_(rtr_abstracts.columns.date > start,\n",
    "                 rtr_abstracts.columns.date < end\n",
    "                 )\n",
    "        )\n",
    "        results = connection.execute(stmt).fetchall()\n",
    "        window_records.append(results)\n",
    "        logging.info(\"Length of results: \" + str(len(results)))\n",
    "    return window_records\n",
    "\n",
    "def get_record_counts(records):\n",
    "    \"\"\"Returns record counts for rolling windows. Used to calibrate monthly mentions.\n",
    "        \n",
    "    Args:\n",
    "        List of window records, one per rolling window (from get_rolling_window_abstracts_sql)\n",
    "    \"\"\"\n",
    "    record_counts = []\n",
    "    for record_set in records:\n",
    "        record_counts.append(len(record_set))\n",
    "    return record_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_window_abstracts_direct(start_date, n_windows, n_months, \n",
    "                                        mesh_terms=['radiology', 'diagnostic imaging']):\n",
    "    \"\"\"Pull abstracts within rolling date windows directly from PubMed.\n",
    "    \n",
    "    Used to check the performance of the calls to the SQL database.\n",
    "    \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        mesh_terms: search terms used for PubMed\n",
    "        \n",
    "    Returns:\n",
    "        List of window records, one per rolling window\n",
    "    \"\"\"\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        results = scrape_pubmed_abstracts(start, end, mesh_terms, debug)\n",
    "        window_records.append(results)\n",
    "        print(\"Length of results: \", len(results))\n",
    "    return window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_records_setup_pull(n_windows, n_months, start_date):\n",
    "    \"\"\"Pull abstracts within rolling date windows directly from PubMed.\n",
    "    \n",
    "    Used to check the performance of the calls to the SQL database.\n",
    "    \n",
    "    Args:\n",
    "        start_date: start date for all windows\n",
    "        n_windows: number of rolling windows\n",
    "        n_months: number of months in each window\n",
    "        mesh_terms: search terms used for PubMed\n",
    "        \n",
    "    Returns:\n",
    "        List of window records, one per rolling window\n",
    "    \"\"\"\n",
    "    N_WINDOWS = n_windows\n",
    "    N_MONTHS = n_months\n",
    "    dbname = 'rtr_db'\n",
    "    username = 'ericbarnhill'\n",
    "    pswd = 'carter0109'\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    window_records = get_rolling_window_abstracts_sql(start_date, N_WINDOWS, N_MONTHS,\n",
    "                                                  connection, rtr_abstracts)\n",
    "    # close and dispose connections\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return window_records\n",
    "\n",
    "def records_to_bows(window_records, ngram_range = range(2,5), dedupe = True):\n",
    "    \"\"\"Converts PubMed scrapes to Bag-Of-Words ngrams. These are then used to build the graph.\n",
    "    \n",
    "    Filters BOW lists as it proceeds, retaining only persistent terms, to reduce memory burdens.\n",
    "    \n",
    "    Args:\n",
    "        window_records = list of rolling window records\n",
    "        ngram_range = range of ngrams considered by the algorithm\n",
    "        dedupe = performing deduping at this stage. Thus far, this shows inferior performance\n",
    "            to performing deduping after trends have been measured.\n",
    "        \n",
    "    Returns:\n",
    "        List of window records, one per rolling window\n",
    "    \"\"\"\n",
    "    bows_filt = []\n",
    "    for records in window_records:\n",
    "        bow = get_bow(records, ngram_range, from_sql=True, dedupe=dedupe)\n",
    "        bows_filt.append(bow)\n",
    "        bows_filt = merge_bows(bows_filt)\n",
    "        # save partial results, so that partial runs can be inspected:\n",
    "        with open(\"bows_filt_temp.pickle\", \"wb\") as fp:\n",
    "            pickle.dump(bows_filt, fp)\n",
    "        logging.info(\"bag of words length: \" + str(len(bow)))\n",
    "    return bows_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_bow_trends(bows, record_counts, log_trends=True, debug=False):\n",
    "    \"\"\"Produce Z score gradient of consecutive BOWs\n",
    "    \n",
    "    Methodology:\n",
    "        get slope and residuals of linear fit from previous time periods\n",
    "        slope gives 'trendiness'\n",
    "        norm resid gives 'uncertainty'\n",
    "        trend_score =  divide trendiness by uncertainty\n",
    "        \n",
    "    Args:\n",
    "        bows: filtered BOW dicts.\n",
    "        log: measure log trends (goodness of fit to exponential trend).\n",
    "        debug: print lots of variables.\n",
    "        \n",
    "    Returns:\n",
    "        single BOW dict containing gradients\n",
    "    \"\"\"    \n",
    "    # some adjustable constants\n",
    "    #MIN_TOTAL_MENTIONS = 200\n",
    "    #MIN_PER_PERIOD = 50\n",
    "    MIN_TOTAL_MENTIONS = 50\n",
    "    MIN_PER_PERIOD = 10\n",
    "    bow_trends = {}\n",
    "    # keys from filtered dicts are all identical, so just need any one\n",
    "    keys = bows[0].keys()\n",
    "    n_keys = len(keys)\n",
    "    # preallocate points vector\n",
    "    # dummy date variable for regression\n",
    "    x = np.arange(len(bows))\n",
    "    trends = {}\n",
    "    pct_prev = 0\n",
    "    UPDATE = 5\n",
    "    record_counts = np.array(record_counts)\n",
    "    # normalize record counts\n",
    "    record_counts = record_counts / np.mean(record_counts)\n",
    "    for j, key in enumerate(keys):\n",
    "        pct = round((j/n_keys)*100)\n",
    "        if pct % UPDATE == 0:\n",
    "            if pct != pct_prev:\n",
    "                logging.info(str(pct) + \"% complete...\")\n",
    "                pct_prev = pct\n",
    "        y = np.zeros(len(bows))\n",
    "        for i, bow in enumerate(bows):\n",
    "            # loop through the filtered bows of each year\n",
    "            # assign each # mentions mention to a spot in the array, creating time series\n",
    "            # normalize by total number of scraped publications that month\n",
    "            y[i] = np.round(bow[key] / record_counts[i])\n",
    "        y = np.array(y)\n",
    "        # filter by total mentions,\n",
    "        # and min mentions per period\n",
    "        y_min = np.min(y)\n",
    "        y_tot = np.sum(y)\n",
    "        # TESTING\n",
    "        if y_tot > MIN_TOTAL_MENTIONS and y_min > MIN_PER_PERIOD:\n",
    "            if debug:\n",
    "                logging.debug(key + \" passed as trend\")\n",
    "            # run it\n",
    "            if log_trends:\n",
    "                y = np.log(y)\n",
    "            # normalize\n",
    "            y_mean = np.mean(y)\n",
    "            y_norm = y / y_mean\n",
    "            # get other filtering criteria\n",
    "            # fit\n",
    "            fit = np.polyfit(x, y_norm, 1, full=True)\n",
    "            # nota bene: polyfit returns highest power first\n",
    "            slope = fit[0][0]\n",
    "            intercept = fit[0][1]\n",
    "            resid = fit[1]\n",
    "            # avoid distorted divisions when resid is too low\n",
    "            # disabled for now\n",
    "            if resid > 1e-3:\n",
    "                trend_score = slope / resid\n",
    "            else:\n",
    "                trend_score = 0\n",
    "            fit_stats = {'intercept' : intercept, \n",
    "                         'slope' : slope, \n",
    "                         'resid' : resid, \n",
    "                         'trend_score' : trend_score,\n",
    "                         'vals': y, \n",
    "                        'total_mentions': np.sum(y)}\n",
    "            if debug and sum(y > 0) == 3:\n",
    "                logging.debug(str(fit))\n",
    "                logging.debug(str(fit_stats))\n",
    "            trends.update({key:fit_stats})\n",
    "        else:\n",
    "            if debug:\n",
    "                logging.debug(key + \" failed as trend\")\n",
    "    return trends\n",
    "\n",
    "def bows_to_trends(bows_filt, record_counts, log_trends = False):\n",
    "    \"\"\"Converts BOW ngrams to trend measurements.\n",
    "    \n",
    "    Args:\n",
    "        bows_filt = list of rolling window records\n",
    "        record_counts = vector containin total counts for each window\n",
    "        log_trends = evalute log trends, modelling trend growth as\n",
    "            exponential rather than linear. Not currently used in\n",
    "            production.\n",
    "        \n",
    "    Returns:\n",
    "        List of window records, one per rolling window\n",
    "    \"\"\"\n",
    "    logging.info(\"filtered bow length: \" +  str(len(bows_filt)))\n",
    "    bow_trends = measure_bow_trends(bows_filt, record_counts, log_trends)\n",
    "    return bow_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TEST\n",
    "def run_rolling_window_test_direct(debug=False):\n",
    "    N_WINDOWS = 2\n",
    "    N_MONTHS = 2\n",
    "    start_date = triple2date((2016,1,1))\n",
    "    mesh_terms = ['radiology', 'diagnostic imaging']\n",
    "    window_records, statuses = get_rolling_window_abstracts_direct(start_date, N_WINDOWS, N_MONTHS,\n",
    "                                                        mesh_terms, debug)\n",
    "    # abstracts 2 bows\n",
    "    NGRAM_RANGE = range(2, 4)\n",
    "    bows = []\n",
    "    for records in window_records:\n",
    "        bow = get_bow(records, NGRAM_RANGE)\n",
    "        bows.append(bow)\n",
    "    bows_filt = merge_bows(bows)\n",
    "    bow_trends = measure_bow_trends(bows_filt, log_trends = False)\n",
    "    return bow_trends, window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bow_trends(): \n",
    "    bow_trends_all = []\n",
    "    # need window records for trend matching later\n",
    "    window_records_all = []\n",
    "    START_DATE = datetime.date(2015,1,2)\n",
    "    N_WINDOWS = 6 # amount of rolling windows considered together\n",
    "    N_MONTHS = 3 # this is the window size\n",
    "    for w in range(5):\n",
    "        start_date = START_DATE + relativedelta(months=+w)\n",
    "        print(\"start date: \", start_date)\n",
    "        bow_trends, window_records = rolling_window_trends_sql(N_WINDOWS, N_MONTHS, start_date)\n",
    "        bow_trends_all.append(bow_trends)\n",
    "    with open(\"bow_trends_all.txt\", \"wb\") as fp:\n",
    "        pickle.dump(bow_trends_all, fp)\n",
    "    return bow_trends_all, window_records_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of test trend slope values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_hist(trends):\n",
    "    \"\"\"\n",
    "    Plots a histogram of trend slope values.\n",
    "    \"\"\"\n",
    "    BINS = 128\n",
    "    RANGE = [-0.12, 0.12]\n",
    "    slopes = get_slopes(trends)\n",
    "    plt.hist(slopes, bins=BINS, range=RANGE)\n",
    "    plt.show()\n",
    "    \n",
    "    RANGE = [-0.3, 0.3]\n",
    "    trend_scores = get_trend_scores(trends)\n",
    "    plt.hist(trend_scores, bins=BINS, range=RANGE)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_slopes(trends):\n",
    "    \"\"\"\n",
    "    Pulls a list of slopes out of the trends and converts to numpy array.\n",
    "    \"\"\"\n",
    "    slopes = np.zeros(len(trends.items()))\n",
    "    it = 0\n",
    "    for key, item in trends.items():\n",
    "        trend_score = item['slope']\n",
    "        #print(type(trend_score[0]))\n",
    "        slopes[it] = trend_score\n",
    "        it += 1\n",
    "    return slopes\n",
    "\n",
    "def get_trend_scores(trends):\n",
    "    \"\"\"\n",
    "    Pulls a list of slopes out of the trends and converts to numpy array.\n",
    "    \"\"\"\n",
    "    slopes = np.zeros(len(trends.items()))\n",
    "    it = 0\n",
    "    for key, item in trends.items():\n",
    "        trend_score = item['trend_score']\n",
    "        #print(type(trend_score[0]))\n",
    "        slopes[it] = trend_score\n",
    "        it += 1\n",
    "    return slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behavior of trends looks convincingly symmetric. While the slopes took a normal distribution, the distribution of slope over dist is unsurprisingly more double-exponential looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_n_keys(trends, n=5):\n",
    "    \"\"\"\n",
    "    prints out the first n keys of a trend object for manual exploration.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for i, key in enumerate(trends.keys()):\n",
    "        print(key)\n",
    "        keys.append(key)\n",
    "        if i >= n:\n",
    "            break\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give a look at the first five BOW trends. These are unsorted so they should be a \"random sample\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_bow_trends(bow_trends):\n",
    "    is_one = []\n",
    "    first_keys = first_n_keys(bow_trends, 5)\n",
    "    for key in first_keys:\n",
    "        item = bow_trends[key]\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slopes are all moderate, with a range of total mentions of the term, so the sample seems reasonably random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of slopes and residuals\n",
    "Does trend slope correlate with size of residual (in which case trendier topics might just be noising topics)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_vs_resid(trends, ds = 100):\n",
    "    \"\"\"\n",
    "    Plots trend values versus residual of linear fit for each trend.\n",
    "    \n",
    "    Args:\n",
    "        trends - trends object.\n",
    "        ds - downsampling factor.\n",
    "    \"\"\"\n",
    "    slope_vals = []\n",
    "    resid_vals = []\n",
    "    for key, item in trends.items():\n",
    "        slope_vals.append(item['slope'])\n",
    "        resid_vals.append(item['resid'])\n",
    "    df = pd.DataFrame({'slope_vals': slope_vals, 'resid_vals': resid_vals})\n",
    "    df_ds = df.iloc[::ds, :]\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    _ = ax1.scatter(df_ds.slope_vals, df_ds.resid_vals)\n",
    "    _ = ax1.set_xlabel('slope')\n",
    "    _ = ax1.set_ylabel('resid')\n",
    "    #f2, ax2 = plt.subplots(1, 2, 2)\n",
    "    _ = ax2.scatter(np.abs(df_ds.slope_vals), df_ds.resid_vals)\n",
    "    _ = ax2.set_xlabel('abs(slope)')\n",
    "    _ = ax2.set_ylabel('resid')\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    pearsonr = scipy.stats.pearsonr(df_ds.slope_vals, df_ds.resid_vals)\n",
    "    pearsonr_mag = scipy.stats.pearsonr(np.abs(df_ds.slope_vals), df_ds.resid_vals)\n",
    "    return pearsonr, pearsonr_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearsonr(bow_trends):\n",
    "    pearsonr, pearsonr_mag = hist_vs_resid(bow_trends)\n",
    "    print(\"Pearson's r, resid vs slope: \", pearsonr[0])\n",
    "    print(\"Pearson's r, resid vs abs(slope)\", pearsonr_mag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the top trends\n",
    "\n",
    "Below the top trending values are analyzed for this test scrape. First we look at the full output of the top 5 stats profiles generated by merge_bows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(trends, n):\n",
    "    trends_sorted = sort_by_trend_score(trends)\n",
    "    \n",
    "def sort_by_trend_score(trends):\n",
    "    trends_scores = []\n",
    "    trends_keys = []\n",
    "    trends_y_tot = []\n",
    "    for key, value in trends.items():\n",
    "        trends_keys.append(key)\n",
    "        trends_scores.append(value['trend_score'])\n",
    "        trends_y_tot.append(value['total_mentions'])\n",
    "    df = pd.DataFrame({'key':trends_keys, 'score': trends_scores,\n",
    "                       'total_mentions': trends_y_tot}) \n",
    "    df.sort_values(by='score', ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_results(folder=\"\"):\n",
    "    with open(folder + \"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(folder + \"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(folder + \"bows.pickle\", \"rb\") as fp:\n",
    "        bows = pickle.load(fp)\n",
    "    with open(folder + \"bows_filt.pickle\", \"rb\") as fp:\n",
    "        bows_filt = pickle.load(fp)\n",
    "    return trends, records, bows, bows_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(folder=\"\"):\n",
    "    with open(folder + \"trends.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(trends, fp)\n",
    "    with open(folder + \"records.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(records, fp)\n",
    "    with open(folder + \"df.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(dedupe = False, year=2016):\n",
    "    records = sql_records_setup_pull(12, 3, datetime.date(year,1,1))\n",
    "    record_counts = get_record_counts(records)\n",
    "    with open(\"records.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(records, fp)\n",
    "    bows_filt = records_to_bows(records, dedupe=dedupe)\n",
    "    with open(\"bows_filt.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(bows_filt, fp)\n",
    "    trends = bows_to_trends(bows_filt, record_counts)\n",
    "    with open(\"trends.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(trends, fp)\n",
    "    logging.info(\"Final number of trends: \" + str(len(trends)))\n",
    "    trend_hist(trends)\n",
    "    get_pearsonr(trends)\n",
    "    df = sort_by_trend_score(trends)\n",
    "    with open(\"df.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)\n",
    "    return records, bows, trends, bows_filt\n",
    "\n",
    "def run_nb_post_bows(dedupe = False):\n",
    "    trends, records, bows, bows_filt = inspect_results(PATH)\n",
    "    record_counts = get_record_counts(records)\n",
    "    trends = bows_to_trends(bows_filt, record_counts)\n",
    "    print(\"trends length\", len(trends))\n",
    "    with open(\"trends.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(trends, fp)\n",
    "    logging.info(\"Final number of trends: \" + str(len(trends)))\n",
    "    trend_hist(trends)\n",
    "    get_pearsonr(trends)\n",
    "    df = sort_by_trend_score(trends)\n",
    "    with open(\"df.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)\n",
    "    return records, bows, trends, bows_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_length_test(bows_filt):\n",
    "    bow = bows_filt[2]\n",
    "    c = 0\n",
    "    for b in list(bow):\n",
    "        if len(b) == 3:\n",
    "            c += 1\n",
    "    print(len(bow) - c, \"that are not 3\")\n",
    "    print(\"out of\", len(bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nb(year=YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
