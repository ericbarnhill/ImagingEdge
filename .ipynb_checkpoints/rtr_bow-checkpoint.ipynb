{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTR BOW\n",
    "\n",
    "## Converts scraped PubMed to BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "%run rtr.ipynb\n",
    "from sqlalchemy import and_\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup SQL engine\n",
    "def test_getting_all_abstracts():\n",
    "    dbname = 'rtr_db'\n",
    "    username = 'ericbarnhill'\n",
    "    pswd = 'carter0109'\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    stmt = select([rtr_abstracts])\n",
    "    abstracts = connection.execute(stmt).fetchall()\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure ngram trends within abstract rolling windows\n",
    "\n",
    "Find most common BOW terms from 2017 Radiology abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "# create radiology stops\n",
    "radiology_stops = ['pci', 'mm', 'auroc', 'roc', 'mri', 'ct', 'pet', 'us', \\\n",
    "                  'p', 'significant', 'significantly', 't', 'ci', 'highly', 'mm', \\\n",
    "                  'considered', 'increase', 'increased', 'decrease', \\\n",
    "                  'decreased', 'measure', 'group', 'standard', 'gold', \\\n",
    "                  'ground', 'truth', 'patient', 'inc', 'cad', \\\n",
    "                  'positive', 'negative', 'year', 'study', \\\n",
    "                  'one', 'two', 'three', 'wiley', 'springer', 'patient', \\\n",
    "                  'diagnosis', 'evaluate', 'statistical', 'statistically', \\\n",
    "                  'ass', 'cohort', 'confidence', 'interval', 'primary', \\\n",
    "                  'outcome', 'increasing', 'decreasing', 'clinical']\n",
    "\n",
    "def get_bow(retained_abstracts, ngram_range=range(1), from_sql = False):\n",
    "    \"\"\"Get bag of words\n",
    "        \n",
    "    Args:\n",
    "        retained_abstracts: List of retained abstracts \n",
    "            following the expected dictionary form.\n",
    "        \n",
    "    Returns:\n",
    "        Counter object containing bag of words.\n",
    "    \"\"\"\n",
    "    BOW_all = Counter()\n",
    "    for i, abst in enumerate(retained_abstracts):\n",
    "        if (i) % 1000 == 0:\n",
    "            print(\"Abstract\",i,\"...\")\n",
    "        if from_sql:\n",
    "            abst_txt = ''.join(abst[0])\n",
    "        else:\n",
    "            abst_txt = ''.join(abst['Abstract'])\n",
    "        abst_tokens = word_tokenize(abst_txt)\n",
    "        # already lower case from earlier parsing\n",
    "        # Retain alphabetic words: alpha_only\n",
    "        abst_alpha = [t for t in abst_tokens if t.isalpha()]\n",
    "        # Remove all stop words: no_stops\n",
    "        abst_no_eng_stops = [t for t in abst_alpha if t not in english_stops]\n",
    "        abst_no_stops = [t for t in abst_no_eng_stops if t not in radiology_stops]\n",
    "        # Instantiate the WordNetLemmatizer\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # Lemmatize all tokens into a new list: lemmatized\n",
    "        abst_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in abst_no_stops]\n",
    "        # Create the bag-of-words: bow\n",
    "        BOW_single = Counter()\n",
    "        for n in ngram_range:\n",
    "            abst_ngrams = ngrams(abst_lemmatized, n)\n",
    "            BOW_single += Counter(abst_ngrams)\n",
    "        BOW_all += BOW_single\n",
    "    return BOW_all\n",
    "\n",
    "def reprocess_bow(BOW):\n",
    "    \"\"\"Convert tuple keys of BOW to strings for easier processing\n",
    "        \n",
    "    Args:\n",
    "        BOW: bag of words dict\n",
    "        \n",
    "    Returns:\n",
    "        New bag of words dict with string keys\n",
    "    \"\"\"\n",
    "    BOW_string_keys = {}\n",
    "    for key, value in BOW.items():\n",
    "        new_key = ' '.join(keystr for keystr in key)\n",
    "        BOW_string_keys.update({new_key:value})\n",
    "    return BOW_string_keys\n",
    "\n",
    "def normalize_bow(BOW, n=3):\n",
    "    \"\"\"Normalize each BOW hit number by the total of hits for that rolling window.\n",
    "    \n",
    "    Perform this step AFTER the BOW filtering methods below.\n",
    "    TODO: Guard against potential underflow.\n",
    "    \n",
    "    Args: \n",
    "        BOW\n",
    "    \n",
    "    Returns: \n",
    "        Normalized BOW\n",
    "    \"\"\"\n",
    "    BOW_total_count = sum(BOW.values())\n",
    "    for key, value in BOW.items():\n",
    "        value_adj = value / BOW_total_count\n",
    "        BOW.update({key:value_adj})\n",
    "    return BOW\n",
    "\n",
    "def bow_head(BOW, n=5): \n",
    "    inc = 0\n",
    "    for key, value in BOW.items():\n",
    "        if inc < n:\n",
    "            print(key)\n",
    "            print(value)\n",
    "            \n",
    "        inc += 1\n",
    "            \n",
    "def bow_hist(BOW):\n",
    "    vals = np.array(list(BOW.values()))\n",
    "    plt.hist(np.log(vals), bins=256, range=(0,50))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join BOWs across rolling windows\n",
    "\n",
    "Next is to compare the normalized frequency of BOW ngrams across two or more time periods. First step is to filter the BOW for each window to only contain common ngrams. The method will produce some summary stats on that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bows(bows_unfilt):\n",
    "    \"\"\"Filter lost BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    # allocate filtered bows\n",
    "    n_bows = len(bows_unfilt)\n",
    "    bows_filt = []\n",
    "    for n in range(n_bows):\n",
    "        bows_filt.append({})\n",
    "    # create intersected set of keys\n",
    "    intersected_keys = []\n",
    "    for bow in bows_unfilt:\n",
    "        if not intersected_keys:\n",
    "            print('initializing intersected keys')\n",
    "            intersected_keys = bow.keys()\n",
    "            print('length: ', len(intersected_keys))\n",
    "        else:\n",
    "            print('updating intersected keys')\n",
    "            print('length next set:', len(bow.keys()))\n",
    "            intersected_keys = intersected_keys & bow.keys()\n",
    "            print('length intersected:', len(intersected_keys))\n",
    "    # filter bows to only contain intersecting keys:\n",
    "    bows_filt = []\n",
    "    for bow in bows_unfilt:\n",
    "        bow_filt = { key: bow[key] for key in intersected_keys }\n",
    "        bows_filt.append(bow_filt)\n",
    "    return bows_filt \n",
    "\n",
    "def measure_bow_trends(bows, log_trends=True, debug=False):\n",
    "    \"\"\"Produce Z score gradient of consecutive BOWs\n",
    "    \n",
    "    Methodology:\n",
    "        get slope and residuals of linear fit from previous time periods\n",
    "        slope gives 'trendiness'\n",
    "        norm resid gives 'uncertainty'\n",
    "        composite divide trendiness by uncertainty\n",
    "        \n",
    "    Args:\n",
    "        bows: filtered BOW dicts.\n",
    "        log: measure log trends (goodness of fit to exponential trend).\n",
    "        debug: print lots of variables.\n",
    "        \n",
    "    Returns:\n",
    "        single BOW dict containing gradients\n",
    "    \"\"\"    \n",
    "    # some adjustable constants\n",
    "    MIN_TOTAL_MENTIONS = 100\n",
    "    MIN_PER_PERIOD = 20\n",
    "    bow_trends = {}\n",
    "    # keys from filtered dicts are all identical, so just need any one\n",
    "    keys = bows[0].keys()\n",
    "    n_keys = len(keys)\n",
    "    # preallocate points vector\n",
    "    # dummy date variable for regression\n",
    "    x = np.arange(len(bows))\n",
    "    trends = {}\n",
    "    pct_prev = 0\n",
    "    UPDATE = 5\n",
    "    for j, key in enumerate(keys):\n",
    "        pct = round((j/n_keys)*100)\n",
    "        if pct % UPDATE == 0:\n",
    "            if pct != pct_prev:\n",
    "                print(pct,\"% complete...\")\n",
    "                pct_prev = pct\n",
    "        y = np.zeros(len(bows))\n",
    "        for i, bow in enumerate(bows):\n",
    "            y[i] = bow[key]\n",
    "        if debug and sum(y > 0) == 3:\n",
    "            print(y)\n",
    "        y = np.array(y)\n",
    "        # filter by total mentions,\n",
    "        # and min mentions per period\n",
    "        y_min = np.min(y)\n",
    "        y_tot = np.sum(y)\n",
    "        if y_tot > MIN_TOTAL_MENTIONS and y_min > MIN_PER_PERIOD:\n",
    "            # run it\n",
    "            if log_trends:\n",
    "                y = np.log(y)\n",
    "            # normalize\n",
    "            y_mean = np.mean(y)\n",
    "            y_norm = y / y_mean\n",
    "            # get other filtering criteria\n",
    "            # fit\n",
    "            fit = np.polyfit(x, y_norm, 1, full=True)\n",
    "            # nota bene: polyfit returns highest power first\n",
    "            slope = fit[0][0]\n",
    "            intercept = fit[0][1]\n",
    "            resid = fit[1]\n",
    "            # avoid distorted divisions when resid is too low\n",
    "            # disabled for now\n",
    "            if resid > 1e-3:\n",
    "                trend_score = slope / resid\n",
    "            else:\n",
    "                trend_score = 0\n",
    "            fit_stats = {'intercept' : intercept, \n",
    "                         'slope' : slope, \n",
    "                         'resid' : resid, \n",
    "                         'trend_score' : trend_score,\n",
    "                         'vals': y, \n",
    "                        'total_mentions': np.sum(y)}\n",
    "            if debug and sum(y > 0) == 3:\n",
    "                print(fit)\n",
    "                print(fit_stats)\n",
    "            trends.update({key:fit_stats})\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw abstracts in rolling windows from the SQL store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this just isn't working right for some reason. For now I am just going to scrape directly from PubMed for each rolling window. This is the method get_rolling_window_abstracts_direct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_window_abstracts_sql(start_date, n_windows, n_months, connection, rtr_abstracts):\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        stmt = select([rtr_abstracts])\n",
    "        stmt = stmt.where(\n",
    "            and_(rtr_abstracts.columns.date > start,\n",
    "                 rtr_abstracts.columns.date < end\n",
    "                 )\n",
    "        )\n",
    "        results = connection.execute(stmt).fetchall()\n",
    "        window_records.append(results)\n",
    "        print(\"Length of results: \", len(results))\n",
    "    return window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_window_abstracts_direct(start_date, n_windows, n_months, \n",
    "                                        mesh_terms, debug=False):\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        results, statuses = scrape_pubmed_abstracts(start, end, mesh_terms, debug)\n",
    "        window_records.append(results)\n",
    "        print(\"Length of results: \", len(results))\n",
    "    return window_records, statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on 6 3 month rolling windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TEST\n",
    "def rolling_window_trends_sql(n_windows, n_months, start_date):\n",
    "    N_WINDOWS = n_windows\n",
    "    N_MONTHS = n_months\n",
    "    start_date = triple2date((2016,1,1))\n",
    "    dbname = 'rtr_db'\n",
    "    username = 'ericbarnhill'\n",
    "    pswd = 'carter0109'\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    window_records = get_rolling_window_abstracts_sql(start_date, N_WINDOWS, N_MONTHS,\n",
    "                                                  connection, rtr_abstracts)\n",
    "    # close and dispose connections\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    # abstracts 2 bows\n",
    "    NGRAM_RANGE = range(2, 3)\n",
    "    bows = []\n",
    "    for records in window_records:\n",
    "        bow = get_bow(records, NGRAM_RANGE, from_sql=True)\n",
    "        bows.append(bow)\n",
    "    bows_filt = merge_bows(bows)\n",
    "    bow_trends = measure_bow_trends(bows_filt, log_trends = False)\n",
    "    return bow_trends, bows, bows_filt, window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TEST\n",
    "def run_rolling_window_test_direct(debug=False):\n",
    "    N_WINDOWS = 2\n",
    "    N_MONTHS = 2\n",
    "    start_date = triple2date((2016,1,1))\n",
    "    mesh_terms = ['radiology', 'diagnostic imaging']\n",
    "    window_records, statuses = get_rolling_window_abstracts_direct(start_date, N_WINDOWS, N_MONTHS,\n",
    "                                                        mesh_terms, debug)\n",
    "    # abstracts 2 bows\n",
    "    NGRAM_RANGE = range(2, 4)\n",
    "    bows = []\n",
    "    for records in window_records:\n",
    "        bow = get_bow(records, NGRAM_RANGE)\n",
    "        bows.append(bow)\n",
    "    bows_filt = merge_bows(bows)\n",
    "    bow_trends = measure_bow_trends(bows_filt, log_trends = False)\n",
    "    return bow_trends, bows, bows_filt, window_records, statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts', 'rtr_keywords']\n",
      "rtr_abstracts table already exists. Columns are:\n",
      "- rtr_abstracts.abstract\n",
      "- rtr_abstracts.date\n",
      "- rtr_abstracts.keywords\n",
      "Length of results:  28451\n",
      "Length of results:  29076\n",
      "Length of results:  28793\n",
      "Length of results:  28776\n",
      "Abstract 0 ...\n",
      "Abstract 1000 ...\n",
      "Abstract 2000 ...\n",
      "Abstract 3000 ...\n",
      "Abstract 4000 ...\n",
      "Abstract 5000 ...\n",
      "Abstract 6000 ...\n",
      "Abstract 7000 ...\n",
      "Abstract 8000 ...\n",
      "Abstract 9000 ...\n",
      "Abstract 10000 ...\n",
      "Abstract 11000 ...\n",
      "Abstract 12000 ...\n",
      "Abstract 13000 ...\n",
      "Abstract 14000 ...\n",
      "Abstract 15000 ...\n",
      "Abstract 16000 ...\n",
      "Abstract 17000 ...\n",
      "Abstract 18000 ...\n",
      "Abstract 19000 ...\n",
      "Abstract 20000 ...\n",
      "Abstract 21000 ...\n",
      "Abstract 22000 ...\n",
      "Abstract 23000 ...\n",
      "Abstract 24000 ...\n",
      "Abstract 25000 ...\n",
      "Abstract 26000 ...\n"
     ]
    }
   ],
   "source": [
    "bow_trends_all = []\n",
    "START_DATE = datetime.date(2016,1,2)\n",
    "N_WINDOWS = 4\n",
    "N_MONTHS = 3\n",
    "for w in range(3):\n",
    "    bow_trends, bows, bows_filt, window_records = rolling_window_trends_sql(N_WINDOWS, N_MONTHS, START_DATE + relativedelta(months=+w))\n",
    "    bow_trends_all.append(bow_trends)\n",
    "with open(\"bow_trends_all.txt\", \"wb\") as fp:\n",
    "    pickle.dump(bow_trends_all, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle bow results\n",
    "def pickle_bow_results():\n",
    "    files_to_pickle = {'bow_trends.txt':bow_trends, 'bows.txt':bows, 'bows_filt.txt':bows_filt, \n",
    "                       'window_records.txt':window_records}\n",
    "    for key, value in files_to_pickle.items():\n",
    "        with open(key, \"wb\") as fp:\n",
    "            pickle.dump(value, fp)\n",
    "#pickle_bow_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of test trend slope values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_hist(trends):\n",
    "    \"\"\"\n",
    "    Plots a histogram of trend slope values.\n",
    "    \"\"\"\n",
    "    BINS = 128\n",
    "    RANGE = [-10, 10]\n",
    "    trends = get_slopes(trends)\n",
    "    plt.hist(trends, bins=BINS, range=RANGE)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_slopes(trends):\n",
    "    \"\"\"\n",
    "    Pulls a list of slopes out of the trends and converts to numpy array.\n",
    "    \"\"\"\n",
    "    slopes = np.zeros(len(trends.items()))\n",
    "    it = 0\n",
    "    for key, item in trends.items():\n",
    "        trend_score = item['trend_score']\n",
    "        #print(type(trend_score[0]))\n",
    "        slopes[it] = trend_score\n",
    "        it += 1\n",
    "    return slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41680\n",
      "41680\n",
      "41680\n"
     ]
    }
   ],
   "source": [
    "for bow_trends in bow_trends_all:\n",
    "    #trend_hist(bow_trends)\n",
    "    print(len(bow_trends))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behavior of trends looks convincingly symmetric. While the slopes took a normal distribution, the distribution of slope over dist is unsurprisingly more double-exponential looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_n_keys(trends, n=5):\n",
    "    \"\"\"\n",
    "    prints out the first n keys of a trend object for manual exploration.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for i, key in enumerate(trends.keys()):\n",
    "        print(key)\n",
    "        keys.append(key)\n",
    "        if i >= n:\n",
    "            break\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give a look at the first five BOW trends. These are unsorted so they should be a \"random sample\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_bow_trends(bow_trends):\n",
    "    is_one = []\n",
    "    first_keys = first_n_keys(bow_trends, 5)\n",
    "    for key in first_keys:\n",
    "        item = bow_trends[key]\n",
    "        print(item)\n",
    "#first_bow_trends(bow_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slopes are all moderate, with a range of total mentions of the term, so the sample seems reasonably random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of slopes and residuals\n",
    "Does trend slope correlate with size of residual (in which case trendier topics might just be noising topics)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_vs_resid(trends, ds = 100):\n",
    "    \"\"\"\n",
    "    Plots trend values versus residual of linear fit for each trend.\n",
    "    \n",
    "    Args:\n",
    "        trends - trends object.\n",
    "        ds - downsampling factor.\n",
    "    \"\"\"\n",
    "    slope_vals = []\n",
    "    resid_vals = []\n",
    "    for key, item in trends.items():\n",
    "        slope_vals.append(item['slope'])\n",
    "        resid_vals.append(item['resid'])\n",
    "    df = pd.DataFrame({'slope_vals': slope_vals, 'resid_vals': resid_vals})\n",
    "    df_ds = df.iloc[::ds, :]\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    _ = ax1.scatter(df_ds.slope_vals, df_ds.resid_vals)\n",
    "    _ = ax1.set_xlabel('slope')\n",
    "    _ = ax1.set_ylabel('resid')\n",
    "    #f2, ax2 = plt.subplots(1, 2, 2)\n",
    "    _ = ax2.scatter(np.abs(df_ds.slope_vals), df_ds.resid_vals)\n",
    "    _ = ax2.set_xlabel('abs(slope)')\n",
    "    _ = ax2.set_ylabel('resid')\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    pearsonr = scipy.stats.pearsonr(df_ds.slope_vals, df_ds.resid_vals)\n",
    "    pearsonr_mag = scipy.stats.pearsonr(np.abs(df_ds.slope_vals), df_ds.resid_vals)\n",
    "    return pearsonr, pearsonr_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearsonr(bow_trends):\n",
    "    pearsonr, pearsonr_mag = hist_vs_resid(bow_trends)\n",
    "    print(\"Pearson's r, resid vs slope: \", pearsonr[0])\n",
    "    print(\"Pearson's r, resid vs abs(slope)\", pearsonr_mag[0])\n",
    "#get_pearsonr(bow_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the top trends\n",
    "\n",
    "Below the top trending values are analyzed for this test scrape. First we look at the full output of the top 5 stats profiles generated by merge_bows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(trends, n):\n",
    "    trends_sorted = sort_by_slope(trends)\n",
    "    \n",
    "def sort_by_trend_score(trends):\n",
    "    trends_scores = []\n",
    "    trends_keys = []\n",
    "    trends_y_tot = []\n",
    "    for key, value in trends.items():\n",
    "        trends_keys.append(key)\n",
    "        trends_scores.append(value['trend_score'])\n",
    "        trends_y_tot.append(value['total_mentions'])\n",
    "    df = pd.DataFrame({'key':trends_keys, 'score': trends_scores,\n",
    "                       'total_mentions': trends_y_tot}) \n",
    "    df.sort_values(by='score', ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(bow_trends):\n",
    "    df = sort_by_trend_score(bow_trends).reset_index()\n",
    "    for n in range(5):\n",
    "        trend_name = df['key'][n]\n",
    "        print(trend_name)\n",
    "        print(bow_trends[trend_name])\n",
    "    return df\n",
    "#df = process_df(bow_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retain top N trending items\n",
    "\n",
    "Choosing the methodology to prune the terms is also challenging. Here we start with simply keeping all the abstracts whose trend score was positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_vars():\n",
    "    with open(\"df.txt\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)\n",
    "#pickle_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
