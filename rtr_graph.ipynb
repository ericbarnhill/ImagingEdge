{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 3: Convert Trends To Graph\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 3: (this notebook) of the app creates a graph combining search terms and trending terms, and deploys this graph to the web app.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 1: Scrape PubMed\n",
    "\n",
    "Part 2: Convert PubMed abstracts to Bag of Words\n",
    "\n",
    "Part 4: Spider unstructured sources\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel:\n",
      "/home/ericbarnhill/anaconda3/envs/ecb/bin/python\n",
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/rtr/scrape.log\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "%run rtr.ipynb\n",
    "import operator\n",
    "import pickle\n",
    "PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/12_mo_nodedupe/\"\n",
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_data():\n",
    "    \"\"\"Recover BOW and trends data.\n",
    "    \"\"\" \n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"df.pickle\", \"rb\") as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    \"\"\"Filter list BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = get_top_keywords(n_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    #print(top_keywords_text)\n",
    "    # TRIPLE LOOP - sure to be a bottleneck\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            # direct method only --\n",
    "            # update to include sql compatibility\n",
    "            # when sql is working\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            #print(\"matching trend\", trend.trend, \"and keyword\", keyword)\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ten most heavily weighted edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, keyword, n=10):\n",
    "    node_edges = G.edges(keyword.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find which keywords made the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_keywords(G, N=20):\n",
    "    keywords =  {}\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['is_key']:\n",
    "            total_wt = 0\n",
    "            for edge in G.edges(node[0], data=True):\n",
    "                total_wt += edge[2]['weight']\n",
    "            keywords.update({node[0]: total_wt})\n",
    "    n = 0\n",
    "    hot_keywords = []\n",
    "    for item in sorted(keywords.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            break\n",
    "        else:\n",
    "            hot_keywords.append(item)\n",
    "    return hot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trends(trends):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = ' '.join(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle graph\n",
    "def pickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"wb\") as fp:\n",
    "            pickle.dump(G, fp)\n",
    "    except:\n",
    "        print('error')\n",
    "        \n",
    "def unpickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"rb\") as fp:\n",
    "            G = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return G\n",
    "        \n",
    "def unpickle_bow_trends():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/bow_converted.txt\", \"rb\") as fp:\n",
    "            bow_trends = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return bow_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def centrality_measures(G):\n",
    "    dc = nx.degree_centrality(G)\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    ec = nx.eigenvector_centrality_numpy(G)\n",
    "    dc = sorted(dc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bc = sorted(bc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ec = sorted(ec.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dc, bc, ec\n",
    "#    for key, value in sorted(bc.items(), key=operator.itemgetter(1), reverse=True):\n",
    "#        print(key, value)\n",
    "#        n += 1\n",
    "#        if n > 10:\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_app(G, trends_converted):\n",
    "    GRAPH_PATH = \"/home/ericbarnhill/Documents/code/insight_app/G.pickle\"\n",
    "    TRENDS_PATH = \"/home/ericbarnhill/Documents/code/insight_app/trends_converted.pickle\"\n",
    "    with open(GRAPH_PATH, \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(TRENDS_PATH, \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(set_up=False):\n",
    "    if set_up:\n",
    "        setup()\n",
    "    N_KEYWORDS = 10000\n",
    "    L = 5\n",
    "    trends, records, df = unpickle_data()\n",
    "    trends_list = pair_trends_keywords(df, records,\n",
    "                                       len(trends), N_KEYWORDS, from_sql = True)\n",
    "    print(\"Top 20 trends:\")\n",
    "    print_trend_dict(trends_list, 20)\n",
    "    G = populate_graph(trends_list)\n",
    "    print(\"MRI trends:\")\n",
    "    mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "    print(mri_trends)\n",
    "    print(\"Hottest keywords:\")\n",
    "    hot_keywords = get_hot_keywords(G)\n",
    "    print(hot_keywords)\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    print(\"Top degree centrality:\", list(dc)[:L])\n",
    "    print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "    print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "    trends_converted = convert_trends(trends)\n",
    "    export_to_app(G, trends_converted)\n",
    "    return G, trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, trends_converted = run_nb(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc, bc, ec = centrality_measures(G)\n",
    "L = 10\n",
    "web_list = {'imaging':1, 'tomography':2, 'rats':3, 'ultrasonography':4, \\\n",
    "            'carcinoma':5, 'mice':6, 'echocardiography':7, 'diagnosis':8, \\\n",
    "           'microscopy':9, 'cartilage':10}\n",
    "dc_list = {}\n",
    "bc_list = {}\n",
    "ec_list = {}\n",
    "for n in range(L):\n",
    "    dc_list.update({list(dc)[n][0]:n+1})\n",
    "    bc_list.update({list(bc)[n][0]:n+1})\n",
    "    ec_list.update({list(ec)[n][0]:n+1})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_scatter_plots():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "    dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "    dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "    ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "    ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "    bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "    bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))\n",
    "    import altair as alt\n",
    "    chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "        x = 'listnum:O',\n",
    "        y = 'value:O', \n",
    "        color = 'variable'\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends, records, df = unpickle_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intercept': 0.9043373129817133,\n",
       " 'slope': 0.01739321582150662,\n",
       " 'resid': array([0.89434546]),\n",
       " 'trend_score': array([0.01944798]),\n",
       " 'vals': array([41., 51., 53., 43., 27., 22., 25., 36., 53., 52., 59., 47.]),\n",
       " 'total_mentions': 509.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trends[list(trends)[100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ericbarnhill/Documents/code/insight/rtr/12_mo_nodedupe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = df.iloc[0,0]\n",
    "sum(df['key']==t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_df(df):\n",
    "    df_filt = df.copy(deep=True)\n",
    "    df_filt = df_filt[df_filt.score > 0]\n",
    "    for i in range(df.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Term \",i)\n",
    "        single_let = False\n",
    "        term = df.iloc[i,0]\n",
    "        for element in term:\n",
    "            if len(element) == 1:\n",
    "                print(\"dropping \",term,\" as it contains a single letter term\")\n",
    "                df_filt.drop(df_filt[df_filt['key'] == term].index, inplace=True)\n",
    "                single_let = True\n",
    "        if not single_let:\n",
    "            term_set = set(term)    \n",
    "            for j in range(df.shape[0]):\n",
    "                entry = df.iloc[j,0]\n",
    "                entry_set = set(entry)\n",
    "                if i != j:\n",
    "                    if entry_set.issubset(term_set):\n",
    "                        df_filt.drop(df_filt[df_filt['key'] == entry].index, inplace=True)            \n",
    "    print(\"df length\", df.shape)\n",
    "    print(\"df filt length\", df_filt.shape)\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term  0\n",
      "dropping  ('velocity', 'e')  as it contains a single letter term\n",
      "dropping  ('within', 'h')  as it contains a single letter term\n",
      "dropping  ('type', 'b', 'aortic')  as it contains a single letter term\n",
      "dropping  ('b', 'aortic')  as it contains a single letter term\n",
      "dropping  ('using', 'f')  as it contains a single letter term\n",
      "Term  1000\n",
      "dropping  ('χ', 'test')  as it contains a single letter term\n",
      "dropping  ('b', 'aortic', 'dissection')  as it contains a single letter term\n",
      "dropping  ('κ', 'κ')  as it contains a single letter term\n",
      "dropping  ('κ', 'κ')  as it contains a single letter term\n",
      "dropping  ('using', 'u')  as it contains a single letter term\n",
      "Term  2000\n",
      "dropping  ('u', 'test')  as it contains a single letter term\n",
      "dropping  ('x', 'z')  as it contains a single letter term\n",
      "dropping  ('x', 'z')  as it contains a single letter term\n",
      "Term  3000\n",
      "dropping  ('apolipoprotein', 'e')  as it contains a single letter term\n",
      "dropping  ('hepatitis', 'c')  as it contains a single letter term\n",
      "dropping  ('using', 'u', 'test')  as it contains a single letter term\n",
      "dropping  ('h', 'h')  as it contains a single letter term\n",
      "dropping  ('h', 'h')  as it contains a single letter term\n",
      "dropping  ('pittsburgh', 'compound', 'b')  as it contains a single letter term\n",
      "dropping  ('c', 'virus')  as it contains a single letter term\n",
      "dropping  ('f', 'f')  as it contains a single letter term\n",
      "dropping  ('f', 'f')  as it contains a single letter term\n",
      "Term  4000\n",
      "dropping  ('hepatitis', 'c', 'virus')  as it contains a single letter term\n",
      "dropping  ('type', 'c')  as it contains a single letter term\n",
      "dropping  ('e', 'wave')  as it contains a single letter term\n",
      "dropping  ('b', 'c')  as it contains a single letter term\n",
      "dropping  ('b', 'c')  as it contains a single letter term\n",
      "dropping  ('time', 'h')  as it contains a single letter term\n",
      "dropping  ('h', 'nmr')  as it contains a single letter term\n",
      "Term  5000\n",
      "dropping  ('h', 'e')  as it contains a single letter term\n",
      "dropping  ('h', 'e')  as it contains a single letter term\n",
      "dropping  ('chronic', 'hepatitis', 'b')  as it contains a single letter term\n",
      "dropping  ('f', 'fdg')  as it contains a single letter term\n",
      "dropping  ('b', 'values')  as it contains a single letter term\n",
      "Term  6000\n",
      "dropping  ('β', 'β')  as it contains a single letter term\n",
      "dropping  ('β', 'β')  as it contains a single letter term\n",
      "dropping  ('compound', 'b')  as it contains a single letter term\n",
      "dropping  ('first', 'h')  as it contains a single letter term\n",
      "dropping  ('type', 'b')  as it contains a single letter term\n",
      "dropping  ('å', 'resolution')  as it contains a single letter term\n",
      "dropping  ('agreement', 'κ')  as it contains a single letter term\n",
      "Term  7000\n",
      "dropping  ('z', 'score')  as it contains a single letter term\n",
      "dropping  ('weight', 'g')  as it contains a single letter term\n",
      "Term  8000\n",
      "dropping  ('vitamin', 'e')  as it contains a single letter term\n",
      "dropping  ('using', 'c')  as it contains a single letter term\n",
      "dropping  ('e', 'staining')  as it contains a single letter term\n",
      "dropping  ('h', 'e', 'staining')  as it contains a single letter term\n",
      "dropping  ('h', 'e', 'staining')  as it contains a single letter term\n",
      "dropping  ('f', 'mean')  as it contains a single letter term\n",
      "dropping  ('b', 'virus')  as it contains a single letter term\n",
      "dropping  ('hepatitis', 'b')  as it contains a single letter term\n",
      "Term  9000\n",
      "dropping  ('hepatitis', 'b', 'virus')  as it contains a single letter term\n",
      "dropping  ('mg', 'l')  as it contains a single letter term\n",
      "dropping  ('b', 'cells')  as it contains a single letter term\n",
      "Term  10000\n",
      "dropping  ('b', 'cell')  as it contains a single letter term\n",
      "dropping  ('cohen', 'κ')  as it contains a single letter term\n",
      "dropping  ('cytochrome', 'c')  as it contains a single letter term\n",
      "dropping  ('k', 'trans')  as it contains a single letter term\n",
      "dropping  ('g', 'g')  as it contains a single letter term\n",
      "dropping  ('g', 'g')  as it contains a single letter term\n",
      "Term  11000\n",
      "dropping  ('h', 'injection')  as it contains a single letter term\n",
      "df length (11729, 3)\n",
      "df filt length (4798, 3)\n",
      "number of positive trends: 1249\n",
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts', 'rtr_keywords']\n",
      "Top 20 trends:\n",
      "{'cardiac computed tomography': Counter({'tomography': 36, 'phantoms': 6})}\n",
      "{'medline embase': Counter()}\n",
      "{'postoperative day': Counter({'tomography': 32, 'imaging': 12, 'anti-infective agents': 9, 'anesthesia': 6, 'fractures': 6, 'anastomosis': 6, 'thoracic surgery': 3, 'liver failure': 3, 'ultrasonography': 2})}\n",
      "{'pathological findings': Counter({'tomography': 43, 'infertility': 9, 'chemotherapy': 9, 'animals': 6, 'microscopy': 4})}\n",
      "{'high level': Counter({'tomography': 33, 'imaging': 21, 'microscopy': 19, 'mice': 15, 'animals': 12, 'phantoms': 12, 'immunity': 9, 'macrophages': 9, 'antineoplastic agents': 6, 'receptor': 6, 'fluorescent antibody technique': 6, 'rna': 6, '2': 4, '3': 4, 'carcinoma': 3, 'muscle': 3, 'africa': 3, 'image processing': 3, 'statistics': 3, 'stress': 3, 'embryo': 2})}\n",
      "{'review current literature': Counter()}\n",
      "{'right heart': Counter({'tomography': 43, 'stress': 9, 'heart septal defects': 9, 'heart failure': 3, 'ventricular function': 2, 'chemotherapy': 1})}\n",
      "{'systolic diastolic blood': Counter()}\n",
      "{'color doppler': Counter({'ultrasonography': 36, 'phantoms': 18, 'tomography': 15, 'ventricular function': 9, 'heart septal defects': 8, 'carcinoma': 6, 'animals': 6, 'ossification': 6, 'chorionic gonadotropin': 6, 'hip dislocation': 4, 'breast neoplasms': 3})}\n",
      "{'using spatial': Counter({'phantoms': 6})}\n",
      "{'right ventricular rv': Counter()}\n",
      "{'prefrontal cortex dlpfc': Counter()}\n",
      "{'image acquisition': Counter({'phantoms': 64, 'tomography': 38, 'imaging': 36, 'antineoplastic agents': 12, 'image processing': 12, 'ventricular function': 9, 'microscopy': 9, 'fluorescent antibody technique': 9, 'in situ hybridization': 6, 'glaucoma': 6, 'heart septal defects': 3})}\n",
      "{'newly formed': Counter({'calcification': 20, 'microscopy': 9, 'tomography': 9, 'animals': 6})}\n",
      "{'period january': Counter({'tomography': 4})}\n",
      "{'aimed compare': Counter()}\n",
      "{'survival analysis': Counter({'tomography': 9, 'heart septal defects': 3})}\n",
      "{'femoral component': Counter()}\n",
      "{'mass spectrometry': Counter({'chromatography': 37, 'microscopy': 34, '4': 12, 'tomography': 6, 'sequence homology': 6, 'transcription factors': 6, 'spectroscopy': 6, 'animals': 4, 'phantoms': 3, 'fluorescent antibody technique': 3, 'heart septal defects': 3, 'metals': 3})}\n",
      "{'low incidence': Counter({'tomography': 9})}\n",
      "{'sensitivity specificity predictive': Counter()}\n",
      "MRI trends:\n",
      "['cardiovascular diseases 9', 'mouse model 9', 'beam computed tomography 6', 'cone beam computed 6', 'mean change 6', 'complete resolution 6', 'radiological evaluation 6', 'central nervous system 3']\n",
      "Hottest keywords:\n",
      "[('tomography', 6463), ('phantoms', 2566), ('imaging', 1692), ('microscopy', 1584), ('animals', 794), ('ventricular function', 727), ('stress', 475), ('mice', 468), ('antineoplastic agents', 453), ('ultrasonography', 343), ('heart septal defects', 335), ('image processing', 322), ('calcification', 218), ('fractures', 217), ('infertility', 183), ('chromatography', 182), ('fluorescent antibody technique', 152), ('embryo', 142), ('implants', 117), ('endothelium', 115)]\n",
      "Top degree centrality: [('tomography', 0.5283505154639175), ('imaging', 0.2384020618556701), ('phantoms', 0.22551546391752578), ('microscopy', 0.20103092783505155), ('animals', 0.13788659793814434), ('ventricular function', 0.11855670103092783), ('stress', 0.08376288659793814), ('antineoplastic agents', 0.07731958762886598), ('mice', 0.07216494845360824), ('ultrasonography', 0.06056701030927835)]\n",
      "Top betweenness centrality: [('tomography', 0.5543858910628938), ('imaging', 0.13251630823093946), ('phantoms', 0.12107295608207222), ('microscopy', 0.11993720251557492), ('animals', 0.06075880388707858), ('ventricular function', 0.044977528106941216), ('antineoplastic agents', 0.0249639608839398), ('stress', 0.021539509793429192), ('image processing', 0.017933810406783157), ('mouse model', 0.01715760163513286)]\n",
      "Top eigencentrality: [('tomography', 0.49567565967217175), ('imaging', 0.2578199067528791), ('phantoms', 0.24470574947187634), ('microscopy', 0.20018574344435489), ('animals', 0.1421640482649924), ('ventricular function', 0.11142937993326144), ('image processing', 0.10137129933450499), ('stress', 0.09141455643632115), ('mice', 0.08092697841845091), ('antineoplastic agents', 0.07815507633083225)]\n"
     ]
    }
   ],
   "source": [
    "df_filt = filt_df(df)\n",
    "N_KEYWORDS = 10000\n",
    "L = 10\n",
    "num_above_zero = sum(df_filt.score > 0.1)\n",
    "print(\"number of positive trends:\", num_above_zero)\n",
    "trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                   round(num_above_zero*3/4), N_KEYWORDS, from_sql = True)\n",
    "print(\"Top 20 trends:\")\n",
    "print_trend_dict(trends_list, 20)\n",
    "G = populate_graph(trends_list)\n",
    "print(\"MRI trends:\")\n",
    "mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "print(mri_trends)\n",
    "print(\"Hottest keywords:\")\n",
    "hot_keywords = get_hot_keywords(G)\n",
    "print(hot_keywords)\n",
    "dc, bc, ec = centrality_measures(G)\n",
    "print(\"Top degree centrality:\", list(dc)[:L])\n",
    "print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "trends_converted = convert_trends(trends)\n",
    "export_to_app(G, trends_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts', 'rtr_keywords']\n"
     ]
    }
   ],
   "source": [
    "trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                   round(num_above_zero*3/4), N_KEYWORDS, from_sql = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trends_converted = convert_trends(trends)\n",
    "export_to_app(G, trends_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.62328853e-17])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(df_filt.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sort = df_filt.sort_values(by='score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_filt.score > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
