{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTR Connectivity Graph\n",
    "\n",
    "- Index pruned trends to pruned keywords\n",
    "- Build graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:\n",
    "\n",
    "- Make Network With Pruned Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "import operator\n",
    "import pickle\n",
    "def setup():\n",
    "    %run rtr_bow.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_data():\n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"df.pickle\", \"rb\") as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = df.iloc[:n_keywords,0]\n",
    "    print(top_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    #print(top_keywords_text)\n",
    "    # TRIPLE LOOP - sure to be a bottleneck\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            # direct method only --\n",
    "            # update to include sql compatibility\n",
    "            # when sql is working\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            #print(\"matching trend\", trend.trend, \"and keyword\", keyword)\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ten most heavily weighted edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, keyword, n=10):\n",
    "    node_edges = G.edges(keyword.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find which keywords made the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_keywords(G, N=20):\n",
    "    keywords =  {}\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['is_key']:\n",
    "            total_wt = 0\n",
    "            for edge in G.edges(node[0], data=True):\n",
    "                total_wt += edge[2]['weight']\n",
    "            keywords.update({node[0]: total_wt})\n",
    "    n = 0\n",
    "    hot_keywords = []\n",
    "    for item in sorted(keywords.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            break\n",
    "        else:\n",
    "            hot_keywords.append(item)\n",
    "    return hot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trends(trends_list):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = ' '.join(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return bow_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle graph\n",
    "def pickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"wb\") as fp:\n",
    "            pickle.dump(G, fp)\n",
    "    except:\n",
    "        print('error')\n",
    "        \n",
    "def unpickle_graph():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/G.txt\", \"rb\") as fp:\n",
    "            G = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return G\n",
    "        \n",
    "def unpickle_bow_trends():\n",
    "    try: \n",
    "        with open(\"/home/ericbarnhill/Documents/code/insight/rtr/bow_converted.txt\", \"rb\") as fp:\n",
    "            bow_trends = pickle.load(fp)\n",
    "    except:\n",
    "        print('error')\n",
    "    return bow_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def centrality_measures(G):\n",
    "    dc = nx.degree_centrality(G)\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    ec = nx.eigenvector_centrality_numpy(G)\n",
    "    dc = sorted(dc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bc = sorted(bc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ec = sorted(ec.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dc, bc, ec\n",
    "#    for key, value in sorted(bc.items(), key=operator.itemgetter(1), reverse=True):\n",
    "#        print(key, value)\n",
    "#        n += 1\n",
    "#        if n > 10:\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(set_up=False):\n",
    "    if set_up:\n",
    "        setup()\n",
    "    N_TRENDS = 1000\n",
    "    N_KEYWORDS = 3000\n",
    "    L = 5\n",
    "    trends, records, df = unpickle_data()\n",
    "    trends_list = pair_trends_keywords(df, records,\n",
    "                                       N_TRENDS, N_KEYWORDS, from_sql = True)\n",
    "    print(\"Top 20 trends:\")\n",
    "    print_trend_dict(trends_list, 20)\n",
    "    G = populate_graph(trends_list)\n",
    "    print(\"MRI trends:\")\n",
    "    mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "    print(mri_trends)\n",
    "    print(\"Hottest keywords:\")\n",
    "    hot_keywords = get_hot_keywords(G)\n",
    "    print(hot_keywords)\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    print(\"Top degree centrality:\", list(dc)[:L])\n",
    "    print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "    print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "    trends_converted = convert_trends(trends_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62               (resonance, imaging, contrast)\n",
      "291       (prospective, randomized, controlled)\n",
      "332             (tomography, angiography, octa)\n",
      "422                    (drug, delivery, system)\n",
      "426                       (frontal, gyrus, ifg)\n",
      "638           (adverse, cardiovascular, events)\n",
      "496                  (serious, adverse, events)\n",
      "676                (nonalcoholic, fatty, liver)\n",
      "282          (independent, component, analysis)\n",
      "577            (systemic, lupus, erythematosus)\n",
      "649    (fluorodeoxyglucose, positron, emission)\n",
      "470                 (using, scanning, electron)\n",
      "398               (resonance, angiography, mra)\n",
      "543                      (grey, matter, volume)\n",
      "537                  (using, cox, proportional)\n",
      "588                (anterior, cerebral, artery)\n",
      "568        (functional, connectivity, analysis)\n",
      "125             (death, myocardial, infarction)\n",
      "61                 (cardiac, death, myocardial)\n",
      "678         (adolescent, idiopathic, scoliosis)\n",
      "155        (retrospectively, reviewed, medical)\n",
      "475                    (bone, mineral, content)\n",
      "346                         (stem, cells, mscs)\n",
      "328        (computed, tomographic, angiography)\n",
      "692                         (mean, adc, values)\n",
      "153                   (lymph, node, metastases)\n",
      "622                  (stable, coronary, artery)\n",
      "317          (magnetic, resonance, angiography)\n",
      "183                (using, receiver, operating)\n",
      "427              (descending, coronary, artery)\n",
      "                         ...                   \n",
      "205                      (tensor, imaging, dti)\n",
      "465                  (pluripotent, stem, cells)\n",
      "531             (cardiac, computed, tomography)\n",
      "415                 (reactive, oxygen, species)\n",
      "378          (apparent, diffusion, coefficient)\n",
      "102          (multivariate, analysis, revealed)\n",
      "493                 (area, receiver, operating)\n",
      "495        (transmission, electron, microscopy)\n",
      "322            (scanning, electron, microscopy)\n",
      "466                 (using, analysis, variance)\n",
      "506                   (left, inferior, frontal)\n",
      "30                     (hip, arthroplasty, tha)\n",
      "344                (brain, magnetic, resonance)\n",
      "433                 (atomic, force, microscopy)\n",
      "133                 (cox, regression, analysis)\n",
      "487                (diffusion, tensor, imaging)\n",
      "503                    (force, microscopy, afm)\n",
      "305         (transform, infrared, spectroscopy)\n",
      "525         (operating, characteristic, curves)\n",
      "615                  (traumatic, brain, injury)\n",
      "79                 (linear, regression, models)\n",
      "402            (underwent, magnetic, resonance)\n",
      "345                    (however, little, known)\n",
      "594              (magnetic, resonance, imaging)\n",
      "67             (myocardial, perfusion, imaging)\n",
      "551               (aortic, valve, implantation)\n",
      "308           (functional, magnetic, resonance)\n",
      "520               (chest, computed, tomography)\n",
      "96                  (valve, implantation, tavi)\n",
      "300                    (deep, vein, thrombosis)\n",
      "Name: key, Length: 696, dtype: object\n",
      "Top 20 trends:\n",
      "{'resonance imaging contrast': Counter()}\n",
      "{'prospective randomized controlled': Counter()}\n",
      "{'tomography angiography octa': Counter()}\n",
      "{'drug delivery system': Counter({'tomography': 4})}\n",
      "{'frontal gyrus ifg': Counter()}\n",
      "{'adverse cardiovascular events': Counter()}\n",
      "{'serious adverse events': Counter({'tomography': 2})}\n",
      "{'nonalcoholic fatty liver': Counter({'tomography': 4})}\n",
      "{'independent component analysis': Counter()}\n",
      "{'systemic lupus erythematosus': Counter({'tomography': 4})}\n",
      "{'fluorodeoxyglucose positron emission': Counter({'tomography': 7, 'imaging': 3})}\n",
      "{'using scanning electron': Counter({'imaging': 2, 'microscopy': 2})}\n",
      "{'resonance angiography mra': Counter()}\n",
      "{'grey matter volume': Counter({'tomography': 2})}\n",
      "{'using cox proportional': Counter({'tomography': 6})}\n",
      "{'anterior cerebral artery': Counter({'imaging': 16, 'aneurysm': 4, 'tomography': 4})}\n",
      "{'functional connectivity analysis': Counter({'tomography': 2})}\n",
      "{'death myocardial infarction': Counter()}\n",
      "{'cardiac death myocardial': Counter()}\n",
      "{'adolescent idiopathic scoliosis': Counter({'imaging': 13, 'tomography': 2})}\n",
      "{'retrospectively reviewed medical': Counter()}\n",
      "MRI trends:\n",
      "['imaging 155', 'tomography 138', 'microscopy 3']\n",
      "Hottest keywords:\n",
      "[('tomography', 2476), ('imaging', 1686), ('microscopy', 100), ('aneurysm', 68), ('stroke', 4)]\n",
      "Top degree centrality: [('tomography', 0.7843137254901961), ('imaging', 0.6190476190476191), ('microscopy', 0.07563025210084033), ('aneurysm', 0.06442577030812326), ('anterior cerebral artery', 0.008403361344537815)]\n",
      "Top betweenness centrality: [('tomography', 0.678672704697707), ('imaging', 0.43926122510776716), ('microscopy', 0.04177494551667053), ('met inclusion criteria', 0.006442244988229892), ('transmission electron microscope', 0.004065940774417518)]\n",
      "Top eigencentrality: [('tomography', 0.5423467634469459), ('imaging', 0.44954293031277764), ('modified rankin scale', 0.05134233231580666), ('anterior cerebral artery', 0.05134233231580665), ('magnetic resonance angiography', 0.05134233231580665)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'trends' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-253-9c90033dab2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_nb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-252-20675157f8fb>\u001b[0m in \u001b[0;36mrun_nb\u001b[0;34m(set_up)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top betweenness centrality:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Top eigencentrality:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrends_converted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-248-6d5111894691>\u001b[0m in \u001b[0;36mconvert_trends\u001b[0;34m(trends_list)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconvert_trends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrends_converted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mkey_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mitem_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trends' is not defined"
     ]
    }
   ],
   "source": [
    "run_nb(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc, bc, ec = centrality_measures(G)\n",
    "L = 10\n",
    "web_list = {'imaging':1, 'tomography':2, 'rats':3, 'ultrasonography':4, \\\n",
    "            'carcinoma':5, 'mice':6, 'echocardiography':7, 'diagnosis':8, \\\n",
    "           'microscopy':9, 'cartilage':10}\n",
    "dc_list = {}\n",
    "bc_list = {}\n",
    "ec_list = {}\n",
    "for n in range(L):\n",
    "    dc_list.update({list(dc)[n][0]:n+1})\n",
    "    bc_list.update({list(bc)[n][0]:n+1})\n",
    "    ec_list.update({list(ec)[n][0]:n+1})\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([web_df, dc_df, ec_df, bc_df])\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "    x = 'listnum:O',\n",
    "    y = 'value:O', \n",
    "    color = 'variable'\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
