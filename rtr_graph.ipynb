{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Edge Notebook 3: Convert Trends To Graph\n",
    "\n",
    "ImagingEdge detects trends in the radiological research literature before they become mainstream publications, patents and products.\n",
    "\n",
    "*Part 3: (this notebook) of the app creates a graph combining search terms and trending terms, and deploys this graph to the web app.*\n",
    "\n",
    "Other parts:\n",
    "\n",
    "Part 1: Scrape PubMed\n",
    "\n",
    "Part 2: Convert PubMed abstracts to Bag of Words\n",
    "\n",
    "Part 4: Graph \"learns\" from unstructured sources\n",
    "\n",
    "### Created by Eric Barnhill for Insight Health Data Science\n",
    "#### 2018 No License\n",
    "\n",
    "Documentation follows the [Google Python Style Guide](http://google.github.io/styleguide/pyguide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new structure with: \n",
    "- keywords from given abstract\n",
    "- trending words from given abstract\n",
    "\n",
    "TODO: include pubmed link to original abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python kernel:\n",
      "/home/ericbarnhill/anaconda3/envs/ecb/bin/python\n",
      "Logfile path:  /home/ericbarnhill/Documents/code/insight/rtr/scrape.log\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "import os\n",
    "ROOT_PATH = \"/home/ericbarnhill/Documents/code/insight/rtr/\" \n",
    "os.chdir(ROOT_PATH)\n",
    "%run rtr.ipynb\n",
    "import operator\n",
    "import pickle\n",
    "YEAR = 2015\n",
    "DATA_PATH = ROOT_PATH + str(YEAR) + \"/\"\n",
    "os.chdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_data():\n",
    "    \"\"\"Recover BOW and trends data.\n",
    "    \"\"\" \n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"df.pickle\", \"rb\") as fp:\n",
    "        df = pickle.load(fp)\n",
    "    return trends, records, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data structure to hold trend and associated keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class Trend:\n",
    "    def __init__(self, trend):\n",
    "        self.trend = trend\n",
    "        #using a set doesn't allow duplicates\n",
    "        #self.keywords = set()\n",
    "        self.keywords = Counter()\n",
    "    def add_keyword(self, keyword):\n",
    "        #self.keywords.add(keyword)\n",
    "        self.keywords.update({keyword:1})\n",
    "    def as_dict(self):\n",
    "        return {self.trend:self.keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_trends_keywords(df, window_records, n_trends, n_keywords, from_sql = False):\n",
    "    \"\"\"Filter list BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    df_pruned = df.iloc[:n_trends,:]\n",
    "    trends_list = []\n",
    "    for entry in df_pruned.key:\n",
    "        trend_str = ' '.join(entry)\n",
    "        trend = Trend(trend_str)\n",
    "        trends_list.append(trend)\n",
    "    # get filtered keywords\n",
    "    top_keywords = get_top_keywords(n_keywords)\n",
    "    top_keywords_text = [keyword[0] for keyword in top_keywords]\n",
    "    #print(top_keywords_text)\n",
    "    # TRIPLE LOOP - sure to be a bottleneck\n",
    "    for trend in trends_list:\n",
    "        for window in window_records:\n",
    "            # direct method only --\n",
    "            # update to include sql compatibility\n",
    "            # when sql is working\n",
    "            for abstract_record in window:\n",
    "                if from_sql:\n",
    "                    abstract = abstract_record[0]\n",
    "                else:\n",
    "                    abstract = abstract_record['Abstract'][0]\n",
    "                if trend.trend in abstract:\n",
    "                    if from_sql:\n",
    "                        keywords = str.split(abstract_record[2], ',')\n",
    "                    else:\n",
    "                        keywords = abstract_record['Keywords']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in top_keywords_text:\n",
    "                            #print(\"matching trend\", trend.trend, \"and keyword\", keyword)\n",
    "                            trend.add_keyword(keyword)\n",
    "                #else:\n",
    "                    #print(\"not in abstract\")\n",
    "    return trends_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trend_dict(trends_list, N=50):\n",
    "    n = 0\n",
    "    for trend in trends_list:\n",
    "        print(trend.as_dict())\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_graph(trends_list):\n",
    "    G = nx.Graph()\n",
    "    for trend in trends_list:\n",
    "        #node1 = convert_trend(trend.trend)\n",
    "        node1 = trend.trend\n",
    "        for key, item in trend.keywords.items():\n",
    "            node2 = key\n",
    "            if G.has_edge(node1, node2):\n",
    "                G[node1][node2]['weight'] += item\n",
    "            else:\n",
    "                # new edge. add with weight=1\n",
    "                G.add_node(node1, is_key = False)\n",
    "                G.add_node(node2, is_key = True)\n",
    "                G.add_edge(node1, node2, weight=item)\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        weight = d['weight']\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find ten most heavily weighted edges of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N_trends(G, keyword, n=10):\n",
    "    node_edges = G.edges(keyword.lower())\n",
    "    edges_dict = {}\n",
    "    for edge in node_edges:\n",
    "        key = edge[1]\n",
    "        value = G[edge[0]][edge[1]]['weight']\n",
    "        edges_dict.update({key:value})\n",
    "    index = 1\n",
    "    trends = []\n",
    "    for key, value in sorted(edges_dict.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        trends.append(' '.join([key, str(value)]))\n",
    "        index += 1\n",
    "        if index > n:\n",
    "            break\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find which keywords made the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hot_keywords(G, N=20):\n",
    "    keywords =  {}\n",
    "    for node in G.nodes(data=True):\n",
    "        if node[1]['is_key']:\n",
    "            total_wt = 0\n",
    "            for edge in G.edges(node[0], data=True):\n",
    "                total_wt += edge[2]['weight']\n",
    "            keywords.update({node[0]: total_wt})\n",
    "    n = 0\n",
    "    hot_keywords = []\n",
    "    for item in sorted(keywords.items(), key=operator.itemgetter(1), reverse=True):\n",
    "        n += 1\n",
    "        if n > N:\n",
    "            break\n",
    "        else:\n",
    "            hot_keywords.append(item)\n",
    "    return hot_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_trend(trend):\n",
    "    if len(trend) > 1:\n",
    "         trend = ' '.join(trend)\n",
    "    return trend\n",
    "\n",
    "def convert_trends(trends):\n",
    "    trends_converted = {}\n",
    "    for key, item in trends.items():\n",
    "        key_new = convert_trend(key)\n",
    "        item_new = item['vals']\n",
    "        trends_converted.update({key_new:item_new})\n",
    "    return trends_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_figure(G):\n",
    "    nx.draw_networkx_nodes(G[2], nx.spring_layout(G[2]), node_size=10)\n",
    "    nx.draw_networkx_edges(G[2], nx.spring_layout(G[2]), alpha=0.4)\n",
    "    plt.xlim((-0.1, 0.1))\n",
    "    plt.ylim((-0.1, 0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def centrality_measures(G):\n",
    "    dc = nx.degree_centrality(G)\n",
    "    bc = nx.betweenness_centrality(G)\n",
    "    ec = nx.eigenvector_centrality_numpy(G)\n",
    "    dc = sorted(dc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    bc = sorted(bc.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    ec = sorted(ec.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return dc, bc, ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_df(df):\n",
    "    df_filt = df.copy(deep=True)\n",
    "    df_filt = df_filt[df_filt.score > 0]\n",
    "    for i in range(df.shape[0]):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Term \",i)\n",
    "        single_let = False\n",
    "        term = df.iloc[i,0]\n",
    "        for element in term:\n",
    "            if len(element) == 1:\n",
    "                print(\"dropping \",term,\" as it contains a single letter term\")\n",
    "                df_filt.drop(df_filt[df_filt['key'] == term].index, inplace=True)\n",
    "                single_let = True\n",
    "        if not single_let:\n",
    "            term_set = set(term)    \n",
    "            for j in range(df.shape[0]):\n",
    "                entry = df.iloc[j,0]\n",
    "                entry_set = set(entry)\n",
    "                if i != j:\n",
    "                    if entry_set.issubset(term_set):\n",
    "                        df_filt.drop(df_filt[df_filt['key'] == entry].index, inplace=True)            \n",
    "    print(\"df length\", df.shape)\n",
    "    print(\"df filt length\", df_filt.shape)\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_app(G, trends_converted):\n",
    "    GRAPH_PATH = \"/home/ericbarnhill/Documents/code/insight_app/G.pickle\"\n",
    "    TRENDS_PATH = \"/home/ericbarnhill/Documents/code/insight_app/trends_converted.pickle\"\n",
    "    with open(GRAPH_PATH, \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open(TRENDS_PATH, \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_presentation_graphic(): \n",
    "    # todo: clean up\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    L = 10\n",
    "    web_list = {'imaging':1, 'tomography':2, 'rats':3, 'ultrasonography':4, \\\n",
    "                'carcinoma':5, 'mice':6, 'echocardiography':7, 'diagnosis':8, \\\n",
    "               'microscopy':9, 'cartilage':10}\n",
    "    dc_list = {}\n",
    "    bc_list = {}\n",
    "    ec_list = {}\n",
    "    for n in range(L):\n",
    "        dc_list.update({list(dc)[n][0]:n+1})\n",
    "        bc_list.update({list(bc)[n][0]:n+1})\n",
    "        ec_list.update({list(ec)[n][0]:n+1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_scatter_plots():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    web_df['listnum'] = np.tile(1, (web_df.shape[0], 1))\n",
    "    dc_df = pd.DataFrame(dc_list, index=[1]).melt()\n",
    "    dc_df['listnum'] = np.tile(2, (web_df.shape[0], 1))\n",
    "    ec_df = pd.DataFrame(ec_list, index=[2]).melt()\n",
    "    ec_df['listnum'] = np.tile(3, (web_df.shape[0], 1))\n",
    "    bc_df = pd.DataFrame(bc_list, index=[3]).melt()\n",
    "    bc_df['listnum'] = np.tile(4, (web_df.shape[0], 1))\n",
    "    import altair as alt\n",
    "    chart = alt.Chart(df, width=400).mark_line().encode(\n",
    "        x = 'listnum:O',\n",
    "        y = 'value:O', \n",
    "        color = 'variable'\n",
    "    )\n",
    "    chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(): \n",
    "    trends, records, df = unpickle_data()\n",
    "    df_filt = filt_df(df)\n",
    "    N_KEYWORDS = 10000\n",
    "    L = 10\n",
    "    num_above_zero = sum(df_filt.score > 0.01)\n",
    "    print(\"number of positive trends:\", num_above_zero)\n",
    "    trends_list = pair_trends_keywords(df_filt, records,\n",
    "                                       round(num_above_zero*3/4), N_KEYWORDS, from_sql = True)\n",
    "    print(\"Top 20 trends:\")\n",
    "    print_trend_dict(trends_list, 20)\n",
    "    G = populate_graph(trends_list)\n",
    "    print(\"MRI trends:\")\n",
    "    mri_trends = top_N_trends(G, 'magnetic resonance imaging')\n",
    "    print(mri_trends)\n",
    "    print(\"Hottest keywords:\")\n",
    "    hot_keywords = get_hot_keywords(G)\n",
    "    print(hot_keywords)\n",
    "    dc, bc, ec = centrality_measures(G)\n",
    "    print(\"Top degree centrality:\", list(dc)[:L])\n",
    "    print(\"Top betweenness centrality:\", list(bc)[:L])\n",
    "    print(\"Top eigencentrality:\", list(ec)[:L])\n",
    "    trends_converted = convert_trends(trends)\n",
    "    with open('G.pickle', \"wb\") as graph_path:\n",
    "        pickle.dump(G, graph_path)\n",
    "    with open('trends_converted.pickle', \"wb\") as trends_path:\n",
    "        pickle.dump(trends_converted, trends_path)\n",
    "    export_to_app(G, trends_converted)\n",
    "    return df_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term  0\n",
      "Term  1000\n",
      "Term  2000\n",
      "df length (2180, 3)\n",
      "df filt length (805, 3)\n",
      "number of positive trends: 663\n",
      "Top 20 trends:\n",
      "{'carotid artery ica': Counter()}\n",
      "{'median interquartile iqr': Counter()}\n",
      "{'chronic obstructive pulmonary': Counter({'tomography': 123, 'aorta': 15, 'image processing': 11, 'ventricular dysfunction': 9, 'endothelium': 6, 'receptor': 6, 'statistics': 3, 'echocardiography': 3, 'fractures': 1})}\n",
      "{'prospective randomized controlled': Counter({'range of motion': 12, 'ultrasonography': 9, 'tomography': 3})}\n",
      "{'radiofrequency ablation rfa': Counter()}\n",
      "{'chronic total occlusion': Counter({'ultrasonography': 47, 'tomography': 7, 'spectroscopy': 3, 'radiography': 3})}\n",
      "{'squamous cell carcinoma': Counter({'tomography': 93, 'carcinoma': 31, 'radiotherapy': 12, 'muscle': 9, 'statistics': 6, 'biopsy': 6, 'remission': 6, 'spectrum analysis': 6, 'printing': 6, 'injections': 6, 'radiography': 6, 'image processing': 5, 'neoplasms': 3, 'colon': 3})}\n",
      "{'obstructive pulmonary disease': Counter({'tomography': 120, 'aorta': 15, 'image processing': 11, 'ventricular dysfunction': 9, 'endothelium': 6, 'receptor': 6, 'statistics': 3, 'echocardiography': 3, 'fractures': 1})}\n",
      "{'glasgow coma scale': Counter({'tomography': 99, 'brain injuries': 9, 'infarction': 6, 'puberty': 6, 'suicide': 6, 'child': 3, 'hemostasis': 1})}\n",
      "{'myocardial infarction stroke': Counter()}\n",
      "{'major adverse cardiovascular': Counter({'ultrasonography': 18, 'tomography': 13, 'ventricular dysfunction': 12, 'echocardiography': 6, 'spectroscopy': 2})}\n",
      "{'human immunodeficiency virus': Counter({'tomography': 21, 'catheters': 6, 'lymphoma': 4, 'tuberculosis': 3})}\n",
      "{'mode network dmn': Counter()}\n",
      "{'renal cell carcinoma': Counter({'tomography': 104, 'carcinoma': 45, 'embolization': 15, 'ultrasonography': 15, 'microscopy': 9, 'spectroscopy': 6, 'kidney diseases': 6, 'adenoma': 5, 'mice': 3, 'image processing': 3, 'neoplasms': 2, 'biopsy': 2})}\n",
      "{'endoscopic ultrasound eus': Counter()}\n",
      "{'preserved ejection fraction': Counter({'ventricular dysfunction': 67, 'echocardiography': 27, 'muscle': 12, 'endothelium': 6, 'diagnosis': 4})}\n",
      "{'total shoulder arthroplasty': Counter({'range of motion': 51, 'tomography': 5, 'orientation': 3})}\n",
      "{'superficial femoral artery': Counter({'ultrasonography': 9, 'endothelium': 6, 'image processing': 1})}\n",
      "{'url http unique': Counter()}\n",
      "{'chronic total occlusions': Counter({'ultrasonography': 13, 'tomography': 4, 'spectroscopy': 3, 'radiography': 3})}\n",
      "{'lupus erythematosus sle': Counter()}\n",
      "MRI trends:\n",
      "[]\n",
      "Hottest keywords:\n",
      "[('tomography', 3865), ('image processing', 1652), ('ultrasonography', 1017), ('ventricular dysfunction', 645), ('carcinoma', 592), ('embolization', 530), ('aorta', 371), ('muscle', 338), ('range of motion', 277), ('statistics', 265), ('biopsy', 196), ('echocardiography', 186), ('spectroscopy', 186), ('child', 172), ('mice', 159), ('diagnosis', 145), ('radiography', 128), ('rats', 124), ('brain injuries', 123), ('radiotherapy', 119)]\n",
      "Top degree centrality: [('tomography', 0.40145985401459855), ('image processing', 0.30900243309002434), ('ultrasonography', 0.18004866180048662), ('ventricular dysfunction', 0.11192214111922141), ('muscle', 0.09002433090024331), ('statistics', 0.08515815085158152), ('embolization', 0.07542579075425791), ('aorta', 0.0705596107055961), ('spectroscopy', 0.0705596107055961), ('echocardiography', 0.0681265206812652)]\n",
      "Top betweenness centrality: [('tomography', 0.44197508835168753), ('image processing', 0.28468736116155574), ('ultrasonography', 0.10661626844054879), ('central nervous system', 0.05178298624607653), ('polymerase chain reaction', 0.05075303659294762), ('ventricular dysfunction', 0.04397039203769314), ('intensive care unit', 0.04165375420977599), ('muscle', 0.028630125374128235), ('statistics', 0.026720705541575414), ('child', 0.026218121906511394)]\n",
      "Top eigencentrality: [('tomography', 0.4596740010102081), ('image processing', 0.3296529820327509), ('ultrasonography', 0.22034182421493376), ('ventricular dysfunction', 0.1352957472521772), ('muscle', 0.11382442653292939), ('intensive care unit', 0.11228455495593119), ('statistics', 0.111876495255223), ('central nervous system', 0.10012977676795562), ('cell lung cancer', 0.09761398422056829), ('spectroscopy', 0.09625956136439537)]\n"
     ]
    }
   ],
   "source": [
    "df_filt = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-64adf31e0f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_filt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "df_filt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ericbarnhill/Documents/code/insight/rtr/2015\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
