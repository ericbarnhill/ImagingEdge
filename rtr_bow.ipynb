{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTR BOW\n",
    "\n",
    "## Converts scraped PubMed to BOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP\n",
    "#%run rtr.ipynb\n",
    "from sqlalchemy import and_\n",
    "import pickle\n",
    "import logging\n",
    "import datetime\n",
    "logging.basicConfig(filename='bow_errors.log',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup SQL engine\n",
    "def test_getting_all_abstracts():\n",
    "    dbname = 'rtr_db'\n",
    "    username = 'ericbarnhill'\n",
    "    pswd = 'carter0109'\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    stmt = select([rtr_abstracts])\n",
    "    abstracts = connection.execute(stmt).fetchall()\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure ngram trends within abstract rolling windows\n",
    "\n",
    "Find most common BOW terms from 2017 Radiology abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "english_stops = set(stopwords.words('english'))\n",
    "# create radiology stops\n",
    "radiology_stops = ['pci', 'mm', 'auroc', 'roc', 'mri', 'ct', 'pet', 'us', \\\n",
    "                  'p', 'significant', 'significantly', 't', 'ci', 'highly', 'mm', \\\n",
    "                  'considered', 'increase', 'increased', 'decrease', \\\n",
    "                  'decreased', 'measure', 'group', 'standard', 'gold', \\\n",
    "                  'ground', 'truth', 'patient', 'inc', 'cad', \\\n",
    "                  'positive', 'negative', 'year', 'study', \\\n",
    "                  'one', 'two', 'three', 'wiley', 'springer', 'patient', \\\n",
    "                  'diagnosis', 'evaluate', 'statistical', 'statistically', \\\n",
    "                  'ass', 'cohort', 'confidence', 'interval', 'primary', \\\n",
    "                  'outcome', 'increasing', 'decreasing', 'clinical', \\\n",
    "                  'r', 'correlation', 'sample', 'size', 'compared', \\\n",
    "                  'randomly', 'divided', 'baseline', 'case', 'report', \\\n",
    "                  'outcome', 'also', 'half', 'may', 'can', 'powerful', \\\n",
    "                  'tool', 'ass', 'n', 'j', 'informed', 'consent', 'age', 'sex',\n",
    "                  'rare', 'cause', 'risk', 'likelihood', 'ratio', 'patients', \\\n",
    "                  'whether', 'regarding', 'including', 'results', 'result', 'mi', \\\n",
    "                  'data', 'set', 'ex', 'in', 'pro', 'mr', 'scan', \\\n",
    "                  'showed', 'studies', 'accuracy', 'performed', 'used', \\\n",
    "                  'determine', 'higher', 'lower', 'male', 'female', \\\n",
    "                  'important', 'role', 'remains', 'unclear', \\\n",
    "                  'range', 'years', 'months', 'days', 'young', 'old', \\\n",
    "                  'modality', 'modalities', 'oct', 'hcc', 'among', \\\n",
    "                  'could', 'useful', 'groups', 'respectively']\n",
    "\n",
    "def get_bow(retained_abstracts, ngram_range=range(1), from_sql = False,\n",
    "            dedupe = True, debug=False):\n",
    "    \"\"\"Get bag of words\n",
    "        \n",
    "    Args:\n",
    "        retained_abstracts: List of retained abstracts \n",
    "            following the expected dictionary form.\n",
    "        \n",
    "    Returns:\n",
    "        Counter object containing bag of words.\n",
    "    \"\"\"\n",
    "    BOW_all = Counter()\n",
    "    for i, abst in enumerate(retained_abstracts):\n",
    "        if (i) % 1000 == 0:\n",
    "            logging.info(\"Abstract \" + str(i) + \"...\")\n",
    "        if from_sql:\n",
    "            abst_txt = ''.join(abst[0])\n",
    "        else:\n",
    "            abst_txt = ''.join(abst['Abstract'])\n",
    "        abst_tokens = word_tokenize(abst_txt)\n",
    "        # already lower case from earlier parsing\n",
    "        # Retain alphabetic words: alpha_only\n",
    "        abst_alpha = [t for t in abst_tokens if t.isalpha()]\n",
    "        # Remove all stop words: no_stops\n",
    "        abst_no_eng_stops = [t for t in abst_alpha if t not in english_stops]\n",
    "        abst_no_stops = [t for t in abst_no_eng_stops if t not in radiology_stops]\n",
    "        # Instantiate the WordNetLemmatizer - decided against\n",
    "        #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        # Lemmatize all tokens into a new list: lemmatized\n",
    "        #abst_lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in abst_no_stops]\n",
    "        # Create the bag-of-words for each ngram: bow\n",
    "        bow_ngrams = []\n",
    "        for n in ngram_range:\n",
    "            if debug:\n",
    "                print(\"ngram length:\", n)\n",
    "            abst_ngrams = ngrams(abst_no_stops, n)\n",
    "            #abst_ngrams = ngrams(abst_lemmatized, n)\n",
    "            bow_ngrams.append(Counter(abst_ngrams))\n",
    "        # de-dupe by removing smaller ngrams contained within larger\n",
    "        if dedupe:\n",
    "            range_max = max(ngram_range)\n",
    "            range_min = min(ngram_range)\n",
    "            num_ngrams_sets = len(bow_ngrams)\n",
    "            for n in range(1, num_ngrams_sets):\n",
    "                for larger_key in list(bow_ngrams[n]):\n",
    "                    for smaller_key in list(bow_ngrams[n-1]):\n",
    "                        if ' '.join(smaller_key) in ' '.join(larger_key):\n",
    "                            del bow_ngrams[n-1][smaller_key]\n",
    "        for bow_ngram in bow_ngrams:\n",
    "            BOW_all += bow_ngram\n",
    "    return BOW_all\n",
    "\n",
    "def reprocess_bow(BOW):\n",
    "    \"\"\"Convert tuple keys of BOW to strings for easier processing\n",
    "        \n",
    "    Args:\n",
    "        BOW: bag of words dict\n",
    "        \n",
    "    Returns:\n",
    "        New bag of words dict with string keys\n",
    "    \"\"\"\n",
    "    BOW_string_keys = {}\n",
    "    for key, value in BOW.items():\n",
    "        new_key = ' '.join(keystr for keystr in key)\n",
    "        BOW_string_keys.update({new_key:value})\n",
    "    return BOW_string_keys\n",
    "\n",
    "def normalize_bow(BOW, n=3):\n",
    "    \"\"\"Normalize each BOW hit number by the total of hits for that rolling window.\n",
    "    \n",
    "    Perform this step AFTER the BOW filtering methods below.\n",
    "    TODO: Guard against potential underflow.\n",
    "    \n",
    "    Args: \n",
    "        BOW\n",
    "    \n",
    "    Returns: \n",
    "        Normalized BOW\n",
    "    \"\"\"\n",
    "    BOW_total_count = sum(BOW.values())\n",
    "    for key, value in BOW.items():\n",
    "        value_adj = value / BOW_total_count\n",
    "        BOW.update({key:value_adj})\n",
    "    return BOW\n",
    "\n",
    "def bow_head(BOW, n=5): \n",
    "    inc = 0\n",
    "    for key, value in BOW.items():\n",
    "        if inc < n:\n",
    "            print(key)\n",
    "            print(value)\n",
    "            \n",
    "        inc += 1\n",
    "            \n",
    "def bow_hist(BOW):\n",
    "    vals = np.array(list(BOW.values()))\n",
    "    plt.hist(np.log(vals), bins=256, range=(0,50))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join BOWs across rolling windows\n",
    "\n",
    "Next is to compare the normalized frequency of BOW ngrams across two or more time periods. First step is to filter the BOW for each window to only contain common ngrams. The method will produce some summary stats on that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bows(bows_unfilt):\n",
    "    \"\"\"Filter list BOWs so that only common terms are contained\n",
    "        \n",
    "    Args:\n",
    "        list of unfiltered BOW dicts\n",
    "        \n",
    "    Returns:\n",
    "        list of filtered BOW dicts\n",
    "    \"\"\" \n",
    "    # allocate filtered bows\n",
    "    n_bows = len(bows_unfilt)\n",
    "    bows_filt = []\n",
    "    for n in range(n_bows):\n",
    "        bows_filt.append({})\n",
    "    # create intersected set of keys\n",
    "    intersected_keys = []\n",
    "    for bow in bows_unfilt:\n",
    "        if not intersected_keys:\n",
    "            logging.info('initializing intersected keys')\n",
    "            intersected_keys = bow.keys()\n",
    "            logging.info('length: ' + str(len(intersected_keys)))\n",
    "        else:\n",
    "            logging.info('updating intersected keys')\n",
    "            logging.info('length next set: ' + str(len(bow.keys())))\n",
    "            intersected_keys = intersected_keys & bow.keys()\n",
    "            logging.info('length intersected:' +  str(len(intersected_keys)))\n",
    "    # filter bows to only contain intersecting keys:\n",
    "    bows_filt = []\n",
    "    for bow in bows_unfilt:\n",
    "        bow_filt = { key: bow[key] for key in intersected_keys }\n",
    "        bows_filt.append(bow_filt)\n",
    "    return bows_filt \n",
    "\n",
    "def measure_bow_trends(bows, log_trends=True, debug=False):\n",
    "    \"\"\"Produce Z score gradient of consecutive BOWs\n",
    "    \n",
    "    Methodology:\n",
    "        get slope and residuals of linear fit from previous time periods\n",
    "        slope gives 'trendiness'\n",
    "        norm resid gives 'uncertainty'\n",
    "        composite divide trendiness by uncertainty\n",
    "        \n",
    "    Args:\n",
    "        bows: filtered BOW dicts.\n",
    "        log: measure log trends (goodness of fit to exponential trend).\n",
    "        debug: print lots of variables.\n",
    "        \n",
    "    Returns:\n",
    "        single BOW dict containing gradients\n",
    "    \"\"\"    \n",
    "    # some adjustable constants\n",
    "    #MIN_TOTAL_MENTIONS = 200\n",
    "    #MIN_PER_PERIOD = 50\n",
    "    MIN_TOTAL_MENTIONS = 50\n",
    "    MIN_PER_PERIOD = 10\n",
    "    bow_trends = {}\n",
    "    # keys from filtered dicts are all identical, so just need any one\n",
    "    keys = bows[0].keys()\n",
    "    n_keys = len(keys)\n",
    "    # preallocate points vector\n",
    "    # dummy date variable for regression\n",
    "    x = np.arange(len(bows))\n",
    "    trends = {}\n",
    "    pct_prev = 0\n",
    "    UPDATE = 5\n",
    "    for j, key in enumerate(keys):\n",
    "        pct = round((j/n_keys)*100)\n",
    "        if pct % UPDATE == 0:\n",
    "            if pct != pct_prev:\n",
    "                logging.info(str(pct) + \"% complete...\")\n",
    "                pct_prev = pct\n",
    "        y = np.zeros(len(bows))\n",
    "        for i, bow in enumerate(bows):\n",
    "            y[i] = bow[key]\n",
    "        y = np.array(y)\n",
    "        # filter by total mentions,\n",
    "        # and min mentions per period\n",
    "        y_min = np.min(y)\n",
    "        y_tot = np.sum(y)\n",
    "        if y_tot > MIN_TOTAL_MENTIONS and y_min > MIN_PER_PERIOD:\n",
    "            if debug:\n",
    "                logging.debug(key + \" passed as trend\")\n",
    "            # run it\n",
    "            if log_trends:\n",
    "                y = np.log(y)\n",
    "            # normalize\n",
    "            y_mean = np.mean(y)\n",
    "            y_norm = y / y_mean\n",
    "            # get other filtering criteria\n",
    "            # fit\n",
    "            fit = np.polyfit(x, y_norm, 1, full=True)\n",
    "            # nota bene: polyfit returns highest power first\n",
    "            slope = fit[0][0]\n",
    "            intercept = fit[0][1]\n",
    "            resid = fit[1]\n",
    "            # avoid distorted divisions when resid is too low\n",
    "            # disabled for now\n",
    "            if resid > 1e-3:\n",
    "                trend_score = slope / resid\n",
    "            else:\n",
    "                trend_score = 0\n",
    "            fit_stats = {'intercept' : intercept, \n",
    "                         'slope' : slope, \n",
    "                         'resid' : resid, \n",
    "                         'trend_score' : trend_score,\n",
    "                         'vals': y, \n",
    "                        'total_mentions': np.sum(y)}\n",
    "            if debug and sum(y > 0) == 3:\n",
    "                logging.debug(str(fit))\n",
    "                logging.debug(str(fit_stats))\n",
    "            trends.update({key:fit_stats})\n",
    "        else:\n",
    "            if debug:\n",
    "                logging.debug(key + \" failed as trend\")\n",
    "    return trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw abstracts in rolling windows from the SQL store:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: this just isn't working right for some reason. For now I am just going to scrape directly from PubMed for each rolling window. This is the method get_rolling_window_abstracts_direct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_window_abstracts_sql(start_date, n_windows, n_months, connection, rtr_abstracts):\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        logging.info(\"Query from \" + str(start) + \" to \" + str(end))\n",
    "        stmt = select([rtr_abstracts])\n",
    "        stmt = stmt.where(\n",
    "            and_(rtr_abstracts.columns.date > start,\n",
    "                 rtr_abstracts.columns.date < end\n",
    "                 )\n",
    "        )\n",
    "        results = connection.execute(stmt).fetchall()\n",
    "        window_records.append(results)\n",
    "        logging.info(\"Length of results: \" + str(len(results)))\n",
    "    return window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_window_abstracts_direct(start_date, n_windows, n_months, \n",
    "                                        mesh_terms, debug=False):\n",
    "    window_records = []\n",
    "    for n in range(n_windows):\n",
    "        start = start_date + n*relativedelta(months=+1)\n",
    "        end = start + relativedelta(months=+n_months)\n",
    "        results, statuses = scrape_pubmed_abstracts(start, end, mesh_terms, debug)\n",
    "        window_records.append(results)\n",
    "        print(\"Length of results: \", len(results))\n",
    "    return window_records, statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TEST\n",
    "def rolling_window_records_sql(n_windows, n_months, start_date):\n",
    "    N_WINDOWS = n_windows\n",
    "    N_MONTHS = n_months\n",
    "    dbname = 'rtr_db'\n",
    "    username = 'ericbarnhill'\n",
    "    pswd = 'carter0109'\n",
    "    metadata, engine, connection = create_rtr_sql_engine(dbname, username, pswd)\n",
    "    rtr_abstracts = check_create_abstracts_table(metadata, engine)\n",
    "    window_records = get_rolling_window_abstracts_sql(start_date, N_WINDOWS, N_MONTHS,\n",
    "                                                  connection, rtr_abstracts)\n",
    "    # close and dispose connections\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "    return window_records\n",
    "\n",
    "def records_to_bows(window_records, ngram_range = range(2,4), dedupe = True):\n",
    "    # abstracts 2 bows\n",
    "    bows = []\n",
    "    for records in window_records:\n",
    "        bow = get_bow(records, ngram_range, from_sql=True, dedupe=dedupe)\n",
    "        bows.append(bow)\n",
    "        logging.info(\"bag of words length: \" + str(len(bow)))\n",
    "    bows_filt = merge_bows(bows)\n",
    "    return bows_filt, bows\n",
    "\n",
    "def bows_to_trends(bows_filt, log_trends = False):\n",
    "    logging.info(\"filtered bow length: \" +  str(len(bows_filt)))\n",
    "    bow_trends = measure_bow_trends(bows_filt, log_trends)\n",
    "    return bow_trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN TEST\n",
    "def run_rolling_window_test_direct(debug=False):\n",
    "    N_WINDOWS = 2\n",
    "    N_MONTHS = 2\n",
    "    start_date = triple2date((2016,1,1))\n",
    "    mesh_terms = ['radiology', 'diagnostic imaging']\n",
    "    window_records, statuses = get_rolling_window_abstracts_direct(start_date, N_WINDOWS, N_MONTHS,\n",
    "                                                        mesh_terms, debug)\n",
    "    # abstracts 2 bows\n",
    "    NGRAM_RANGE = range(2, 4)\n",
    "    bows = []\n",
    "    for records in window_records:\n",
    "        bow = get_bow(records, NGRAM_RANGE)\n",
    "        bows.append(bow)\n",
    "    bows_filt = merge_bows(bows)\n",
    "    bow_trends = measure_bow_trends(bows_filt, log_trends = False)\n",
    "    return bow_trends, window_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bow_trends(): \n",
    "    bow_trends_all = []\n",
    "    # need window records for trend matching later\n",
    "    window_records_all = []\n",
    "    START_DATE = datetime.date(2015,1,2)\n",
    "    N_WINDOWS = 6 # amount of rolling windows considered together\n",
    "    N_MONTHS = 3 # this is the window size\n",
    "    for w in range(5):\n",
    "        start_date = START_DATE + relativedelta(months=+w)\n",
    "        print(\"start date: \", start_date)\n",
    "        bow_trends, window_records = rolling_window_trends_sql(N_WINDOWS, N_MONTHS, start_date)\n",
    "        bow_trends_all.append(bow_trends)\n",
    "    with open(\"bow_trends_all.txt\", \"wb\") as fp:\n",
    "        pickle.dump(bow_trends_all, fp)\n",
    "    return bow_trends_all, window_records_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of test trend slope values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_hist(trends):\n",
    "    \"\"\"\n",
    "    Plots a histogram of trend slope values.\n",
    "    \"\"\"\n",
    "    BINS = 128\n",
    "    RANGE = [-10, 10]\n",
    "    trends = get_slopes(trends)\n",
    "    plt.hist(trends, bins=BINS, range=RANGE)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_slopes(trends):\n",
    "    \"\"\"\n",
    "    Pulls a list of slopes out of the trends and converts to numpy array.\n",
    "    \"\"\"\n",
    "    slopes = np.zeros(len(trends.items()))\n",
    "    it = 0\n",
    "    for key, item in trends.items():\n",
    "        trend_score = item['trend_score']\n",
    "        #print(type(trend_score[0]))\n",
    "        slopes[it] = trend_score\n",
    "        it += 1\n",
    "    return slopes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behavior of trends looks convincingly symmetric. While the slopes took a normal distribution, the distribution of slope over dist is unsurprisingly more double-exponential looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_n_keys(trends, n=5):\n",
    "    \"\"\"\n",
    "    prints out the first n keys of a trend object for manual exploration.\n",
    "    \"\"\"\n",
    "    keys = []\n",
    "    for i, key in enumerate(trends.keys()):\n",
    "        print(key)\n",
    "        keys.append(key)\n",
    "        if i >= n:\n",
    "            break\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give a look at the first five BOW trends. These are unsorted so they should be a \"random sample\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_bow_trends(bow_trends):\n",
    "    is_one = []\n",
    "    first_keys = first_n_keys(bow_trends, 5)\n",
    "    for key in first_keys:\n",
    "        item = bow_trends[key]\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slopes are all moderate, with a range of total mentions of the term, so the sample seems reasonably random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation of slopes and residuals\n",
    "Does trend slope correlate with size of residual (in which case trendier topics might just be noising topics)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_vs_resid(trends, ds = 100):\n",
    "    \"\"\"\n",
    "    Plots trend values versus residual of linear fit for each trend.\n",
    "    \n",
    "    Args:\n",
    "        trends - trends object.\n",
    "        ds - downsampling factor.\n",
    "    \"\"\"\n",
    "    slope_vals = []\n",
    "    resid_vals = []\n",
    "    for key, item in trends.items():\n",
    "        slope_vals.append(item['slope'])\n",
    "        resid_vals.append(item['resid'])\n",
    "    df = pd.DataFrame({'slope_vals': slope_vals, 'resid_vals': resid_vals})\n",
    "    df_ds = df.iloc[::ds, :]\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    _ = ax1.scatter(df_ds.slope_vals, df_ds.resid_vals)\n",
    "    _ = ax1.set_xlabel('slope')\n",
    "    _ = ax1.set_ylabel('resid')\n",
    "    #f2, ax2 = plt.subplots(1, 2, 2)\n",
    "    _ = ax2.scatter(np.abs(df_ds.slope_vals), df_ds.resid_vals)\n",
    "    _ = ax2.set_xlabel('abs(slope)')\n",
    "    _ = ax2.set_ylabel('resid')\n",
    "    f.tight_layout()\n",
    "    plt.show()\n",
    "    pearsonr = scipy.stats.pearsonr(df_ds.slope_vals, df_ds.resid_vals)\n",
    "    pearsonr_mag = scipy.stats.pearsonr(np.abs(df_ds.slope_vals), df_ds.resid_vals)\n",
    "    return pearsonr, pearsonr_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pearsonr(bow_trends):\n",
    "    pearsonr, pearsonr_mag = hist_vs_resid(bow_trends)\n",
    "    print(\"Pearson's r, resid vs slope: \", pearsonr[0])\n",
    "    print(\"Pearson's r, resid vs abs(slope)\", pearsonr_mag[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing the top trends\n",
    "\n",
    "Below the top trending values are analyzed for this test scrape. First we look at the full output of the top 5 stats profiles generated by merge_bows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n(trends, n):\n",
    "    trends_sorted = sort_by_slope(trends)\n",
    "    \n",
    "def sort_by_trend_score(trends):\n",
    "    trends_scores = []\n",
    "    trends_keys = []\n",
    "    trends_y_tot = []\n",
    "    for key, value in trends.items():\n",
    "        trends_keys.append(key)\n",
    "        trends_scores.append(value['trend_score'])\n",
    "        trends_y_tot.append(value['total_mentions'])\n",
    "    df = pd.DataFrame({'key':trends_keys, 'score': trends_scores,\n",
    "                       'total_mentions': trends_y_tot}) \n",
    "    df.sort_values(by='score', ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(bow_trends, N=5):\n",
    "    df = sort_by_trend_score(bow_trends).reset_index()\n",
    "    for n in range(N):\n",
    "        trend_name = df['key'][n]\n",
    "        print(trend_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retain top N trending items\n",
    "\n",
    "Choosing the methodology to prune the terms is also challenging. Here we start with simply keeping all the abstracts whose trend score was positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_vars():\n",
    "    with open(\"df.txt\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)\n",
    "    with open(\"window_records.txt\", \"wb\") as fp:\n",
    "        pickle.dump(window_records, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb(dedupe = True):\n",
    "    records = rolling_window_records_sql(12, 3, datetime.date(2015,6,1))\n",
    "    with open(\"12_mo/records.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(records, fp)\n",
    "    bows_filt, bows = records_to_bows(records, dedupe=dedupe)\n",
    "    with open(\"12_mo/bows.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(bows, fp)\n",
    "    with open(\"12_mo/bows_filt.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(bows_filt, fp)\n",
    "    trends = bows_to_trends(bows_filt)\n",
    "    with open(\"12_mo/trends.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(trends, fp)\n",
    "    trend_hist(trends)\n",
    "    get_pearsonr(trends)\n",
    "    df = sort_by_trend_score(trends)\n",
    "    with open(\"df.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)\n",
    "    return records, bows, trends, bows_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nb_2(dedupe = False):\n",
    "    records = rolling_window_records_sql(12, 3, datetime.date(2015,6,1))\n",
    "    with open(\"12_mo_nodedupe/records.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(records, fp)\n",
    "    bows_filt, bows = records_to_bows(records, dedupe=dedupe)\n",
    "    with open(\"12_mo_nodedupe/bows.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(bows, fp)\n",
    "    with open(\"12_mo_nodedupe/bows_filt.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(bows_filt, fp)\n",
    "    trends = bows_to_trends(bows_filt)\n",
    "    with open(\"12_mo_nodedupe/trends.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(trends, fp)\n",
    "    print(\"Final number of trends: \", len(trends))\n",
    "    trend_hist(trends)\n",
    "    get_pearsonr(trends)\n",
    "    df = sort_by_trend_score(trends)\n",
    "    with open(\"12_mo_nodedupe/df.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)\n",
    "    return records, bows, trends, bows_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating engine:\n",
      "-  postgresql://ericbarnhill:carter0109@localhost/rtr_db\n",
      "-  ['rtr_abstracts']\n",
      "rtr_abstracts table already exists. Columns are:\n",
      "- rtr_abstracts.abstract\n",
      "- rtr_abstracts.date\n",
      "- rtr_abstracts.keywords\n"
     ]
    }
   ],
   "source": [
    "#records, bows, trends, bows_filt = run_nb()\n",
    "run_nb_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_results():\n",
    "    with open(\"trends.pickle\", \"rb\") as fp:\n",
    "        trends = pickle.load(fp)\n",
    "    with open(\"records.pickle\", \"rb\") as fp:\n",
    "        records = pickle.load(fp)\n",
    "    with open(\"bows.pickle\", \"rb\") as fp:\n",
    "        bows = pickle.load(fp)\n",
    "    return trends, records, bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results():\n",
    "    with open(\"trends.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(trends, fp)\n",
    "    with open(\"records.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(records, fp)\n",
    "    with open(\"df.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_test(bows):\n",
    "    for n in range(1,3):\n",
    "        print('---')\n",
    "        print('n:', n)\n",
    "        print(len(bows[0]))\n",
    "        print(len(bows[n]))\n",
    "        bows_filt_temp = merge_bows([bows[0], bows[n]])\n",
    "        print(len(bows_filt_temp))\n",
    "        print(len(bows_filt_temp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_length_test(bows_filt):\n",
    "    bow = bows_filt[2]\n",
    "    c = 0\n",
    "    for b in list(bow):\n",
    "        if len(b) == 3:\n",
    "            c += 1\n",
    "    print(len(bow) - c, \"that are not 3\")\n",
    "    print(\"out of\", len(bow))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
